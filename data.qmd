# Data

## Description

Our data is collected by a non-profit organization called [Epoch AI](https://epoch.ai/). From their (website)[https://epoch.ai/about], Epoch AI's mission is:

> Epoch AI is a multidisciplinary non-profit research institute investigating the future of artificial intelligence. We examine the driving forces behind AI and forecast its economic and societal impact. We emphasize making our research accessible through our reports, models and visualizations to help ground the discussion of AI on a solid empirical footing. Our goal is to create a healthy scientific environment, where claims about AI are discussed with the rigor they merit.

In the past, Epoch AI's work has been cited in [Our World in Data](https://ourworldindata.org/brief-history-of-ai), [Time Magazine](https://time.com/6300942/ai-progress-charts/), by [Google CEO Sundar Pichai](https://blog.google/technology/ai/bard-google-ai-search-updates/), and other reputable sources.

Generally, we frame our work in terms of input factors and output results and further subdivide these into input factors for AI models, input factors for AI companies, output results for AI models, and output results for AI companies:

- AI Models:
    - Inputs:
        - GPU Clusters
        - Compute Spend
    - Outputs:
        - Performance on model benchmarks
- AI Companies:
    - Inputs:
        - Staffing
        - Funding
    - Outputs:
        - Usage of AI tools
        - Revenue

### Collection Methods

Epoch AI collects data on variety of components related to AI progress including hardware advancements, GPU cluster advancements, model releases, AI company funding, etc. For the most part, Epoch uses a mix of automated collection methods (for example, automatic updates of AI benchmark scores) and manual data collection from public sources of information (company blog posts, news posts, public satellite imagery). Here we link to the full set of data collection methodologies:

- [GPU Cluster Data](https://epoch.ai/data/gpu-clusters-documentation#inclusion)
- [AI Company Data](https://epoch.ai/data/ai-companies-documentation#inclusion)
- [AI Model Data](https://epoch.ai/data/ai-models-documentation#inclusion)
- [AI Benchmark Data](https://epoch.ai/benchmarks/about)

### Datasets Overview

| Title | Description | Frequency of Updates | Dimensions | Notes |
|-------|-------------|---------------------|------------|-------|
|  GPU Clusters     |    Dataset listing planned, existing, and previous GPU clusters         |         Daily            |      786 rows,  55 columns     |   Daily refreshes, but data may not necessarily be updated daily.    |
|  AI Companies     |      Various different datasets pertaining to AI company revenue, compute spend, staffing, funding rounds, user usage, and general information       |         Daily            |     Various datasets       |       |
|   AI Models    |     Dataset listing AI models over the years (including primitive models)        |         Daily            |     3000+ rows, 56 columns       |  Divided into "Frontier", "Notable", and "Large Scale" models   |
| AI Benchmark Data      |   A collection of different datasets of ML model performance on different benchmarks          |      not found               |     Various datasets       |       |

Please see links above for finer details on the datasets.

## Missing value analysis

### Compute Spend
```{r}
library(redav)
library(ggplot2)
library(dplyr)
library(readr)

compute_spend <- read_csv("./Data/ai_companies_dataset/ai_companies_compute_spend.csv")

usage_rates <- read_csv("./Data/ai_companies_dataset/ai_companies_usage_reports.csv")

ai_models <- read_csv("./Data/ai_models_dataset/all_ai_models.csv")

fronteir_math <- read_csv("./Data/benchmark_dataset/frontiermath.csv")

gpu_cluster_ds <- read_csv("./Data/gpu_clusters_dataset/gpu_clusters.csv")


p1 <- plot_missing(compute_spend, max_cols = 10, num_char = 7, percent = FALSE)
p2 <- plot_missing(compute_spend, num_char = 2, percent = TRUE)
p1
p2
```

The above graph describes the missing values of the compute spend dataset. Fortunately, most of the missing data in this dataset is meta-data like "Source 2" and "Source 3" (sources of the information) or "Exclude from graph view". However, there are a few columns with high missing values that would be nice to have:

- Inference compute spend (63% missing)
- R&D compute spend (72% missing)
- Total compute spend (63% missing)

### Usage Rates

```{r}
p3 <- plot_missing(usage_rates, max_cols = 10, num_char = 7, percent = FALSE)
p4 <- plot_missing(usage_rates, num_char = 2, percent = TRUE)
p3
p4
```

The usage reports datasets follows a similar missing values pattern as the compute spend dataset. Columns with the most missing values are meta-data ("Source 2", "Graph Note", "Exclude from graph view").

Likewise there are some important columns that are missing in most entries that would be lovely to have and present a limitation of the current data:

- Active Users (30% missing)
- Active Users time period (30% missing)
- Daily Tokens (82% missing)
- Daily Messages (76% missing)

### Frontier Math Benchmark

```{r}
p7 <- plot_missing(fronteir_math, max_cols = 10, num_char = 7, percent = FALSE)
p8 <- plot_missing(fronteir_math, num_char = 2, percent = TRUE)
p7
p8
```

In the benchmark dataset we used, the FrontierMath benchmark, there are a few notable columns with large percentages of missing values. Notably "Log viewer" has 100% missing data. We expect that Epoch AI added this column to be consistent with other benchmarks but is not applicable to this dataset. 

Otherwise, the only two columns with large percentages of missing values are "Training compute (FLOP)" (75% missing) and "Training compute notes" (42%)


### AI Models

Our GPU Clusters and AI Models dataset did not render properly for the missing values plot, so we will manually describe the missing values below.

The AI models dataset is likely Epoch's most comprehensive dataset. However, Epoch has included many columns that are only applicable to a small subset of the models. Some of these columns are meta-data:

- Notability criteria (72% missing)
- Notability criteria notes (76% missing)
- Archived Links (99% missing)
- Utilization Notes (97% missing)
- Frontier Model (95% missing)
- Hugging Face developer id (81% missing)

and others. We expect not to use any meta-data columns so we are not concerned with the high level of missing values.

There are many training-related columns with high percentages of missing values, for example:

- Hardware utilization (99% missing)
- Post-training compute (FLOP) (99% missing)
- Training compute cost (92% missing)
- Training chip-hours (93% missing)
- Training data center (97% missing)
- Training time (hours) (82% missing)
- Parameters (35% missing)

and others. While this provides a significant limitation to our exploration, we expect that the missing value levels are high since the dataset contains models dating back to the 1960s. We expect these percentages to be lower amongst newer models which we care more about.

### GPU Clusters

Like previous datasets, the columns in the GPU clusters dataset are mostly meta-data (Sources 1 through 5, notes of various kinds).

Notable columns useful for analysis with high missing values include: 

- Reported Power Capacity (MW) (83% missing)
- Chip type (primary) (41% missing)
- Owner (36% missing)
- Hardware cost (31% missing)
- Location (22% missing)
- Users (40% missing)
- First operational date (32% missing) 

Though some of these columns have higher than ideal levels of missing values, we expect to have enough data to provide useful analysis.


## Other problems and limitations of the data

Other than the missing values mentioned above. The data is limited in the fact that many of the columns across the datasets have inconsistent value formats. For example, the location column in the GPU clusters dataset may contain coordinates, addresses, or general regions where the cluster is located.