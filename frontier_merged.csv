Model,Domain,Task,Authors,Notability criteria,Notability criteria notes,Model accessibility,Link,Citations,Reference,Publication date,Organization_x,Parameters,Parameters notes,Training compute (FLOP)_x,Training compute notes_x,Training dataset,Training dataset notes,(DEPRECATED) Training dataset size (datapoints),Dataset size notes,Epochs,Inference compute (FLOP),Inference compute notes,Training time (hours),Training time notes,Training hardware,Approach,Compute cost notes,Compute sponsor categorization,Confidence,Abstract,Last modified,Created By,WikiText and Penn Treebank data,Exclude,Country (of organization),Base model,Finetune compute (FLOP),Finetune compute notes,Hardware quantity,Hardware utilization (MFU),Training cloud compute vendor,Training data center,Archived links,Batch size,Batch size notes,Organization categorization,Foundation model,Training compute lower bound,Training compute upper bound,Training chip-hours,Training code accessibility,Accessibility notes,Possibly over 1e23 FLOP,Training compute cost (2023 USD),Training dataset size,Sparsity,Utilization notes,Estimated over 1e25 FLOP,Power per GPU,Base model compute,Total compute - (base + finetune),API prices,Created,Inference code accessibility,Numerical format,Model versions,Frontier model,Training power draw (W),FLOP/$,Hardware release date,Hardware age,Hardware FP32,Hardware TF32,Hardware count,Hardware TF16,Hardware FP16,Assumed precision,Assumed hardware performance (FLOP/s),Training compute estimation method,Hugging Face developer id,Post-training compute (FLOP),Post-training compute notes,Hardware maker,benchmarks/models,Maybe over 1e25 FLOP,Updated dataset size,Distillation or synthetic data,Distillation or synthetic data compute,Distillation or synthetic data compute notes,Knowledge cutoff,Context window,Documents,Hardware utilization (HFU),Training compute cost (cloud),Training compute cost (upfront),Model_key,Model version,Best score (across scorers),Release date,Organization_y,Country,Training compute (FLOP)_y,Training compute notes_y,stderr,Log viewer,Logs,Started at,id,Model_version_key,model_and_version
iGPT-XL,"Vision,Image generation",Image completion,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",,,Open weights (unrestricted),https://openai.com/research/image-gpt,1620.0,Generative Pretraining from Pixels,2020-06-17,OpenAI,6801000000.0,source: https://openai.com/blog/image-gpt/#rfref53,3.3e+22,"Taken from here
https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening (""There's no compute data for the largest model, iGPT-XL. But based on the FLOP/s increase from GPT-3 XL (same num of params as iGPT-L) to GPT-3 6.7B (same num of params as iGPT-XL), I think it required 5 times more compute: 3.3 * 10^22 FLOP."")",ILSVRC 2012 subset of ImageNet,,1229920.32,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,,Industry,Likely,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",2025-10-16 15:12:27+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,Open source,"Modified MIT, code and weights:

https://github.com/openai/image-gpt?tab=License-1-ov-file#readme

train code: https://github.com/openai/image-gpt/blob/master/src/run.py ",,108390.73774775192,iGPT-XL,,,,250,,,,2023-05-29T21:06:54.000Z,Open source,,,True,,3.0445405839747994e+17,2018-03-27,2.2258726899383983,15670000000000,,1,125000000000000.0,31330000000000,TF16,125000000000000,Third-party estimation,,,,NVIDIA,,,True,,,,,,,,,,igpt-xl,gpt-4o-2024-11-20,0.0034482758620689,2024-11-20,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores.,0.0034482758620689,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/Z7HU95T8eema56EpLtECvG.eval,2025-03-06T18:03:58.731Z,Z7HU95T8eema56EpLtECvG,gpt-4o-2024-11-20,iGPT-XL gpt-4o-2024-11-20
iGPT-XL,"Vision,Image generation",Image completion,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",,,Open weights (unrestricted),https://openai.com/research/image-gpt,1620.0,Generative Pretraining from Pixels,2020-06-17,OpenAI,6801000000.0,source: https://openai.com/blog/image-gpt/#rfref53,3.3e+22,"Taken from here
https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening (""There's no compute data for the largest model, iGPT-XL. But based on the FLOP/s increase from GPT-3 XL (same num of params as iGPT-L) to GPT-3 6.7B (same num of params as iGPT-XL), I think it required 5 times more compute: 3.3 * 10^22 FLOP."")",ILSVRC 2012 subset of ImageNet,,1229920.32,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,,Industry,Likely,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",2025-10-16 15:12:27+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,Open source,"Modified MIT, code and weights:

https://github.com/openai/image-gpt?tab=License-1-ov-file#readme

train code: https://github.com/openai/image-gpt/blob/master/src/run.py ",,108390.73774775192,iGPT-XL,,,,250,,,,2023-05-29T21:06:54.000Z,Open source,,,True,,3.0445405839747994e+17,2018-03-27,2.2258726899383983,15670000000000,,1,125000000000000.0,31330000000000,TF16,125000000000000,Third-party estimation,,,,NVIDIA,,,True,,,,,,,,,,igpt-xl,gpt-4o-2024-08-06,0.0034482758620689,2024-08-06,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores.,0.0034482758620689,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/SmT4ukjkxsXZvFExMMVFgW.eval,2025-03-07T18:56:32.370Z,SmT4ukjkxsXZvFExMMVFgW,gpt-4o-2024-08-06,iGPT-XL gpt-4o-2024-08-06
iGPT-XL,"Vision,Image generation",Image completion,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",,,Open weights (unrestricted),https://openai.com/research/image-gpt,1620.0,Generative Pretraining from Pixels,2020-06-17,OpenAI,6801000000.0,source: https://openai.com/blog/image-gpt/#rfref53,3.3e+22,"Taken from here
https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening (""There's no compute data for the largest model, iGPT-XL. But based on the FLOP/s increase from GPT-3 XL (same num of params as iGPT-L) to GPT-3 6.7B (same num of params as iGPT-XL), I think it required 5 times more compute: 3.3 * 10^22 FLOP."")",ILSVRC 2012 subset of ImageNet,,1229920.32,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,,Industry,Likely,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",2025-10-16 15:12:27+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,Open source,"Modified MIT, code and weights:

https://github.com/openai/image-gpt?tab=License-1-ov-file#readme

train code: https://github.com/openai/image-gpt/blob/master/src/run.py ",,108390.73774775192,iGPT-XL,,,,250,,,,2023-05-29T21:06:54.000Z,Open source,,,True,,3.0445405839747994e+17,2018-03-27,2.2258726899383983,15670000000000,,1,125000000000000.0,31330000000000,TF16,125000000000000,Third-party estimation,,,,NVIDIA,,,True,,,,,,,,,,igpt-xl,gpt-4.1-2025-04-14,0.0551724137931034,2025-04-14,OpenAI,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.0134303816285978,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/NPeiYpjbscreYnFkHPvcNB.eval,2025-04-14T22:40:01.283Z,NPeiYpjbscreYnFkHPvcNB,gpt-4.1-2025-04-14,iGPT-XL gpt-4.1-2025-04-14
iGPT-XL,"Vision,Image generation",Image completion,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",,,Open weights (unrestricted),https://openai.com/research/image-gpt,1620.0,Generative Pretraining from Pixels,2020-06-17,OpenAI,6801000000.0,source: https://openai.com/blog/image-gpt/#rfref53,3.3e+22,"Taken from here
https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening (""There's no compute data for the largest model, iGPT-XL. But based on the FLOP/s increase from GPT-3 XL (same num of params as iGPT-L) to GPT-3 6.7B (same num of params as iGPT-XL), I think it required 5 times more compute: 3.3 * 10^22 FLOP."")",ILSVRC 2012 subset of ImageNet,,1229920.32,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,,Industry,Likely,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",2025-10-16 15:12:27+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,Open source,"Modified MIT, code and weights:

https://github.com/openai/image-gpt?tab=License-1-ov-file#readme

train code: https://github.com/openai/image-gpt/blob/master/src/run.py ",,108390.73774775192,iGPT-XL,,,,250,,,,2023-05-29T21:06:54.000Z,Open source,,,True,,3.0445405839747994e+17,2018-03-27,2.2258726899383983,15670000000000,,1,125000000000000.0,31330000000000,TF16,125000000000000,Third-party estimation,,,,NVIDIA,,,True,,,,,,,,,,igpt-xl,gpt-4.1-mini-2025-04-14,0.0448275862068965,2025-04-14,OpenAI,United States of America,,,0.0121720756094746,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/ephMMUGDANc9iVTNNuuoZx.eval,2025-04-14T22:49:44.216Z,ephMMUGDANc9iVTNNuuoZx,gpt-4.1-mini-2025-04-14,iGPT-XL gpt-4.1-mini-2025-04-14
iGPT-XL,"Vision,Image generation",Image completion,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",,,Open weights (unrestricted),https://openai.com/research/image-gpt,1620.0,Generative Pretraining from Pixels,2020-06-17,OpenAI,6801000000.0,source: https://openai.com/blog/image-gpt/#rfref53,3.3e+22,"Taken from here
https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening (""There's no compute data for the largest model, iGPT-XL. But based on the FLOP/s increase from GPT-3 XL (same num of params as iGPT-L) to GPT-3 6.7B (same num of params as iGPT-XL), I think it required 5 times more compute: 3.3 * 10^22 FLOP."")",ILSVRC 2012 subset of ImageNet,,1229920.32,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,,Industry,Likely,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",2025-10-16 15:12:27+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,Open source,"Modified MIT, code and weights:

https://github.com/openai/image-gpt?tab=License-1-ov-file#readme

train code: https://github.com/openai/image-gpt/blob/master/src/run.py ",,108390.73774775192,iGPT-XL,,,,250,,,,2023-05-29T21:06:54.000Z,Open source,,,True,,3.0445405839747994e+17,2018-03-27,2.2258726899383983,15670000000000,,1,125000000000000.0,31330000000000,TF16,125000000000000,Third-party estimation,,,,NVIDIA,,,True,,,,,,,,,,igpt-xl,gpt-4.1-nano-2025-04-14,0.0103448275862068,2025-04-14,OpenAI,United States of America,,,0.0059518867144507,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/chtQvebcoaE33KkhAjbCnw.eval,2025-04-14T22:51:44.342Z,chtQvebcoaE33KkhAjbCnw,gpt-4.1-nano-2025-04-14,iGPT-XL gpt-4.1-nano-2025-04-14
iGPT-XL,"Vision,Image generation",Image completion,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",,,Open weights (unrestricted),https://openai.com/research/image-gpt,1620.0,Generative Pretraining from Pixels,2020-06-17,OpenAI,6801000000.0,source: https://openai.com/blog/image-gpt/#rfref53,3.3e+22,"Taken from here
https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening (""There's no compute data for the largest model, iGPT-XL. But based on the FLOP/s increase from GPT-3 XL (same num of params as iGPT-L) to GPT-3 6.7B (same num of params as iGPT-XL), I think it required 5 times more compute: 3.3 * 10^22 FLOP."")",ILSVRC 2012 subset of ImageNet,,1229920.32,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,,Industry,Likely,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",2025-10-16 15:12:27+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,Open source,"Modified MIT, code and weights:

https://github.com/openai/image-gpt?tab=License-1-ov-file#readme

train code: https://github.com/openai/image-gpt/blob/master/src/run.py ",,108390.73774775192,iGPT-XL,,,,250,,,,2023-05-29T21:06:54.000Z,Open source,,,True,,3.0445405839747994e+17,2018-03-27,2.2258726899383983,15670000000000,,1,125000000000000.0,31330000000000,TF16,125000000000000,Third-party estimation,,,,NVIDIA,,,True,,,,,,,,,,igpt-xl,gpt-5-2025-08-07_medium,0.2482758620689655,2025-08-07,OpenAI,United States of America,,,0.025412510772196,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/J4g9pcSnmeKzZYfhZmXjVY.eval,2025-08-07T18:43:29.196Z,J4g9pcSnmeKzZYfhZmXjVY,gpt-5-2025-08-07_medium,iGPT-XL gpt-5-2025-08-07_medium
iGPT-XL,"Vision,Image generation",Image completion,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",,,Open weights (unrestricted),https://openai.com/research/image-gpt,1620.0,Generative Pretraining from Pixels,2020-06-17,OpenAI,6801000000.0,source: https://openai.com/blog/image-gpt/#rfref53,3.3e+22,"Taken from here
https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening (""There's no compute data for the largest model, iGPT-XL. But based on the FLOP/s increase from GPT-3 XL (same num of params as iGPT-L) to GPT-3 6.7B (same num of params as iGPT-XL), I think it required 5 times more compute: 3.3 * 10^22 FLOP."")",ILSVRC 2012 subset of ImageNet,,1229920.32,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,,Industry,Likely,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",2025-10-16 15:12:27+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,Open source,"Modified MIT, code and weights:

https://github.com/openai/image-gpt?tab=License-1-ov-file#readme

train code: https://github.com/openai/image-gpt/blob/master/src/run.py ",,108390.73774775192,iGPT-XL,,,,250,,,,2023-05-29T21:06:54.000Z,Open source,,,True,,3.0445405839747994e+17,2018-03-27,2.2258726899383983,15670000000000,,1,125000000000000.0,31330000000000,TF16,125000000000000,Third-party estimation,,,,NVIDIA,,,True,,,,,,,,,,igpt-xl,gpt-5-2025-08-07_high,0.2482758620689655,2025-08-07,OpenAI,United States of America,,,0.025412510772196,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/hKQvuTN85y5g4K3QakKRYb.eval,2025-08-07T18:43:29.626Z,hKQvuTN85y5g4K3QakKRYb,gpt-5-2025-08-07_high,iGPT-XL gpt-5-2025-08-07_high
iGPT-XL,"Vision,Image generation",Image completion,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",,,Open weights (unrestricted),https://openai.com/research/image-gpt,1620.0,Generative Pretraining from Pixels,2020-06-17,OpenAI,6801000000.0,source: https://openai.com/blog/image-gpt/#rfref53,3.3e+22,"Taken from here
https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening (""There's no compute data for the largest model, iGPT-XL. But based on the FLOP/s increase from GPT-3 XL (same num of params as iGPT-L) to GPT-3 6.7B (same num of params as iGPT-XL), I think it required 5 times more compute: 3.3 * 10^22 FLOP."")",ILSVRC 2012 subset of ImageNet,,1229920.32,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,,Industry,Likely,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",2025-10-16 15:12:27+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,Open source,"Modified MIT, code and weights:

https://github.com/openai/image-gpt?tab=License-1-ov-file#readme

train code: https://github.com/openai/image-gpt/blob/master/src/run.py ",,108390.73774775192,iGPT-XL,,,,250,,,,2023-05-29T21:06:54.000Z,Open source,,,True,,3.0445405839747994e+17,2018-03-27,2.2258726899383983,15670000000000,,1,125000000000000.0,31330000000000,TF16,125000000000000,Third-party estimation,,,,NVIDIA,,,True,,,,,,,,,,igpt-xl,gpt-5-nano-2025-08-07_medium,0.0724137931034482,2025-08-07,OpenAI,United States of America,,,0.0152454015611468,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/dHQzrNGjuR5kiHKtNxG62X.eval,2025-08-07T18:43:21.321Z,dHQzrNGjuR5kiHKtNxG62X,gpt-5-nano-2025-08-07_medium,iGPT-XL gpt-5-nano-2025-08-07_medium
iGPT-XL,"Vision,Image generation",Image completion,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",,,Open weights (unrestricted),https://openai.com/research/image-gpt,1620.0,Generative Pretraining from Pixels,2020-06-17,OpenAI,6801000000.0,source: https://openai.com/blog/image-gpt/#rfref53,3.3e+22,"Taken from here
https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening (""There's no compute data for the largest model, iGPT-XL. But based on the FLOP/s increase from GPT-3 XL (same num of params as iGPT-L) to GPT-3 6.7B (same num of params as iGPT-XL), I think it required 5 times more compute: 3.3 * 10^22 FLOP."")",ILSVRC 2012 subset of ImageNet,,1229920.32,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,,Industry,Likely,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",2025-10-16 15:12:27+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,Open source,"Modified MIT, code and weights:

https://github.com/openai/image-gpt?tab=License-1-ov-file#readme

train code: https://github.com/openai/image-gpt/blob/master/src/run.py ",,108390.73774775192,iGPT-XL,,,,250,,,,2023-05-29T21:06:54.000Z,Open source,,,True,,3.0445405839747994e+17,2018-03-27,2.2258726899383983,15670000000000,,1,125000000000000.0,31330000000000,TF16,125000000000000,Third-party estimation,,,,NVIDIA,,,True,,,,,,,,,,igpt-xl,gpt-5-mini-2025-08-07_high,0.1896551724137931,2025-08-07,OpenAI,United States of America,,,0.0230604807320528,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/BBUxVJxYbiHUbrFwv2uGz5.eval,2025-08-07T18:43:36.653Z,BBUxVJxYbiHUbrFwv2uGz5,gpt-5-mini-2025-08-07_high,iGPT-XL gpt-5-mini-2025-08-07_high
iGPT-XL,"Vision,Image generation",Image completion,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",,,Open weights (unrestricted),https://openai.com/research/image-gpt,1620.0,Generative Pretraining from Pixels,2020-06-17,OpenAI,6801000000.0,source: https://openai.com/blog/image-gpt/#rfref53,3.3e+22,"Taken from here
https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening (""There's no compute data for the largest model, iGPT-XL. But based on the FLOP/s increase from GPT-3 XL (same num of params as iGPT-L) to GPT-3 6.7B (same num of params as iGPT-XL), I think it required 5 times more compute: 3.3 * 10^22 FLOP."")",ILSVRC 2012 subset of ImageNet,,1229920.32,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,,Industry,Likely,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",2025-10-16 15:12:27+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,Open source,"Modified MIT, code and weights:

https://github.com/openai/image-gpt?tab=License-1-ov-file#readme

train code: https://github.com/openai/image-gpt/blob/master/src/run.py ",,108390.73774775192,iGPT-XL,,,,250,,,,2023-05-29T21:06:54.000Z,Open source,,,True,,3.0445405839747994e+17,2018-03-27,2.2258726899383983,15670000000000,,1,125000000000000.0,31330000000000,TF16,125000000000000,Third-party estimation,,,,NVIDIA,,,True,,,,,,,,,,igpt-xl,gpt-5-nano-2025-08-07_high,0.0827586206896551,2025-08-07,OpenAI,United States of America,,,0.0162068838583676,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/TAkuyuW78npTzV5PLWAHEy.eval,2025-08-07T18:43:20.166Z,TAkuyuW78npTzV5PLWAHEy,gpt-5-nano-2025-08-07_high,iGPT-XL gpt-5-nano-2025-08-07_high
iGPT-XL,"Vision,Image generation",Image completion,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",,,Open weights (unrestricted),https://openai.com/research/image-gpt,1620.0,Generative Pretraining from Pixels,2020-06-17,OpenAI,6801000000.0,source: https://openai.com/blog/image-gpt/#rfref53,3.3e+22,"Taken from here
https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening (""There's no compute data for the largest model, iGPT-XL. But based on the FLOP/s increase from GPT-3 XL (same num of params as iGPT-L) to GPT-3 6.7B (same num of params as iGPT-XL), I think it required 5 times more compute: 3.3 * 10^22 FLOP."")",ILSVRC 2012 subset of ImageNet,,1229920.32,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,,Industry,Likely,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",2025-10-16 15:12:27+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,Open source,"Modified MIT, code and weights:

https://github.com/openai/image-gpt?tab=License-1-ov-file#readme

train code: https://github.com/openai/image-gpt/blob/master/src/run.py ",,108390.73774775192,iGPT-XL,,,,250,,,,2023-05-29T21:06:54.000Z,Open source,,,True,,3.0445405839747994e+17,2018-03-27,2.2258726899383983,15670000000000,,1,125000000000000.0,31330000000000,TF16,125000000000000,Third-party estimation,,,,NVIDIA,,,True,,,,,,,,,,igpt-xl,gpt-5-mini-2025-08-07_medium,0.193103448275862,2025-08-07,OpenAI,United States of America,,,0.023219615450311,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/9osUw9BJpUhScjwjhk7b4d.eval,2025-08-07T18:43:36.879Z,9osUw9BJpUhScjwjhk7b4d,gpt-5-mini-2025-08-07_medium,iGPT-XL gpt-5-mini-2025-08-07_medium
Gemini 1.5 Pro,"Language,Multimodal","Language modeling,Visual question answering",Gemini Team,Significant use,"Google DeepMind's current best public model, being used for their products.",API access,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,2024-02-15,Google DeepMind,,MoE architecture,,Training compute imputed to be 1.58e25 FLOP from benchmark scores.,Unspecified unreleased,,,,,,,,,Google TPU v4,,,,Speculative,,2025-10-27 19:58:26+00:00,Epoch AI,,0.0,"United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,,,,,,Industry,,,,,Unreleased,API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models,True,8034059.174468736,Gemini 1.5 Pro,,,True,170,,,"$3.5 / 1M input tokens (for <= 128k tokens),$7 / 1M input tokens (for > 128k tokens),$10.5 / 1M output tokens (for <= 128k tokens),$21 / 1M output tokens (for > 128k tokens),$0.875 / 1M cached tokens (for <= 128k tokens),$1.75 / 1M cached tokens (for > 128k tokens),$4.5 / 1M tokens per hour (cached storage),$1.25 / 1M input tokens (for <= 128k tokens),$5 / 1M output tokens (for <= 128k tokens),$0.3125 / 1M cached tokens (for <= 128k tokens),$2.5 / 1M input tokens (for > 128k tokens),$10 / 1M output tokens (for > 128k tokens),$0.625 / 1M cached tokens (for > 128k tokens)",2024-04-03T17:52:18.000Z,,,,True,,0.0,2021-05-20,2.740588637919233,,,1,275000000000000.0,,TF16,275000000000000,Benchmarks,,,,Google,"gemini-1.5-pro-001,gemini-1.5-pro-002",,True,,,,,,,,,,gemini 1.5 pro,gemini-1.5-flash-002,0.0,2024-09-24,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""",0.0,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/8FtfWCm8DdAqF9rnMVwv8h.eval,2025-03-07T18:40:43.945Z,8FtfWCm8DdAqF9rnMVwv8h,gemini-1.5-flash-002,Gemini 1.5 Pro gemini-1.5-flash-002
Claude 3 Opus,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modeling/generation",,"SOTA improvement,Training cost","Based on leaks, Claude 3 Opus and Sonnet probably cost >$1M to train.

""Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. ""

Table 1, Table 3",API access,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,,"The Claude 3 Model Family: Opus, Sonnet, Haiku",2024-03-04,Anthropic,,,,Training compute estimated to be 1.64e25 FLOP from benchmark scores.,Unspecified unreleased,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.
This has a knowledge cutoff date of August 2023, according to https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, which I assume means August 1, 2023.",,,,,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,,"Per https://time.com/6980000/anthropic/
""Claude 3 cost somewhere between $30 million and $300 million to train""
This would seem to include all three versions.

Ballpark estimate, based on relative API costs:
sqrt($30M * $300M) * (15 / (0.25 + 3 + 15)) = $78.0M
(cost) * (Sonnet share of API cost)

Convert to 2020 dollars: $64.7M",,Speculative,"We introduce Claude 3, a new family of large multimodal models – Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].
",2025-10-27 19:36:31+00:00,Robi Rahman,,0.0,United States of America,,,,,,,,,,,Industry,,,,,Unreleased,,True,17106058.0886546,Claude 3 Opus,,,True,,,,"$15 / 1M input tokens,$75 / 1M output tokens",2024-03-06T18:37:28.000Z,,,,True,,0.0,,,,,0,,,FP32,,Benchmarks,,,,,claude-3-opus-20240229,,True,,,,2023-08-01,,,,,,claude 3 opus,claude-3-7-sonnet-20250219,0.0310344827586206,2025-02-24,Anthropic,United States of America,3.3499999999999998e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.0102006417530873,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/m7X2ppkiEkweCtFppiRW8S.eval,2025-03-06T18:05:58.538Z,m7X2ppkiEkweCtFppiRW8S,claude-3-7-sonnet-20250219,Claude 3 Opus claude-3-7-sonnet-20250219
Claude 3 Opus,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modeling/generation",,"SOTA improvement,Training cost","Based on leaks, Claude 3 Opus and Sonnet probably cost >$1M to train.

""Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. ""

Table 1, Table 3",API access,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,,"The Claude 3 Model Family: Opus, Sonnet, Haiku",2024-03-04,Anthropic,,,,Training compute estimated to be 1.64e25 FLOP from benchmark scores.,Unspecified unreleased,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.
This has a knowledge cutoff date of August 2023, according to https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, which I assume means August 1, 2023.",,,,,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,,"Per https://time.com/6980000/anthropic/
""Claude 3 cost somewhere between $30 million and $300 million to train""
This would seem to include all three versions.

Ballpark estimate, based on relative API costs:
sqrt($30M * $300M) * (15 / (0.25 + 3 + 15)) = $78.0M
(cost) * (Sonnet share of API cost)

Convert to 2020 dollars: $64.7M",,Speculative,"We introduce Claude 3, a new family of large multimodal models – Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].
",2025-10-27 19:36:31+00:00,Robi Rahman,,0.0,United States of America,,,,,,,,,,,Industry,,,,,Unreleased,,True,17106058.0886546,Claude 3 Opus,,,True,,,,"$15 / 1M input tokens,$75 / 1M output tokens",2024-03-06T18:37:28.000Z,,,,True,,0.0,,,,,0,,,FP32,,Benchmarks,,,,,claude-3-opus-20240229,,True,,,,2023-08-01,,,,,,claude 3 opus,claude-3-7-sonnet-20250219_16K,0.0413793103448275,2025-02-24,Anthropic,United States of America,3.3499999999999998e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.0117156422541124,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/PU7LXbdNB8Unfvu4qCeJZF.eval,2025-03-06T19:16:31.497Z,PU7LXbdNB8Unfvu4qCeJZF,claude-3-7-sonnet-20250219_16k,Claude 3 Opus claude-3-7-sonnet-20250219_16K
Claude 3 Opus,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modeling/generation",,"SOTA improvement,Training cost","Based on leaks, Claude 3 Opus and Sonnet probably cost >$1M to train.

""Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. ""

Table 1, Table 3",API access,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,,"The Claude 3 Model Family: Opus, Sonnet, Haiku",2024-03-04,Anthropic,,,,Training compute estimated to be 1.64e25 FLOP from benchmark scores.,Unspecified unreleased,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.
This has a knowledge cutoff date of August 2023, according to https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, which I assume means August 1, 2023.",,,,,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,,"Per https://time.com/6980000/anthropic/
""Claude 3 cost somewhere between $30 million and $300 million to train""
This would seem to include all three versions.

Ballpark estimate, based on relative API costs:
sqrt($30M * $300M) * (15 / (0.25 + 3 + 15)) = $78.0M
(cost) * (Sonnet share of API cost)

Convert to 2020 dollars: $64.7M",,Speculative,"We introduce Claude 3, a new family of large multimodal models – Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].
",2025-10-27 19:36:31+00:00,Robi Rahman,,0.0,United States of America,,,,,,,,,,,Industry,,,,,Unreleased,,True,17106058.0886546,Claude 3 Opus,,,True,,,,"$15 / 1M input tokens,$75 / 1M output tokens",2024-03-06T18:37:28.000Z,,,,True,,0.0,,,,,0,,,FP32,,Benchmarks,,,,,claude-3-opus-20240229,,True,,,,2023-08-01,,,,,,claude 3 opus,claude-3-5-sonnet-20241022,0.0206896551724137,2024-10-22,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",0.0083731308075254,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/MVnDrt94wyokzx3a7ZJkph.eval,2025-03-06T23:54:46.118Z,MVnDrt94wyokzx3a7ZJkph,claude-3-5-sonnet-20241022,Claude 3 Opus claude-3-5-sonnet-20241022
Claude 3 Opus,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modeling/generation",,"SOTA improvement,Training cost","Based on leaks, Claude 3 Opus and Sonnet probably cost >$1M to train.

""Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. ""

Table 1, Table 3",API access,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,,"The Claude 3 Model Family: Opus, Sonnet, Haiku",2024-03-04,Anthropic,,,,Training compute estimated to be 1.64e25 FLOP from benchmark scores.,Unspecified unreleased,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.
This has a knowledge cutoff date of August 2023, according to https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, which I assume means August 1, 2023.",,,,,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,,"Per https://time.com/6980000/anthropic/
""Claude 3 cost somewhere between $30 million and $300 million to train""
This would seem to include all three versions.

Ballpark estimate, based on relative API costs:
sqrt($30M * $300M) * (15 / (0.25 + 3 + 15)) = $78.0M
(cost) * (Sonnet share of API cost)

Convert to 2020 dollars: $64.7M",,Speculative,"We introduce Claude 3, a new family of large multimodal models – Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].
",2025-10-27 19:36:31+00:00,Robi Rahman,,0.0,United States of America,,,,,,,,,,,Industry,,,,,Unreleased,,True,17106058.0886546,Claude 3 Opus,,,True,,,,"$15 / 1M input tokens,$75 / 1M output tokens",2024-03-06T18:37:28.000Z,,,,True,,0.0,,,,,0,,,FP32,,Benchmarks,,,,,claude-3-opus-20240229,,True,,,,2023-08-01,,,,,,claude 3 opus,claude-3-5-haiku-20241022,0.0034482758620689,2024-10-22,Anthropic,United States of America,,,0.0034482758620689,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/JQcqpW9xPfJRKJkHne6DpK.eval,2025-03-07T18:11:31.647Z,JQcqpW9xPfJRKJkHne6DpK,claude-3-5-haiku-20241022,Claude 3 Opus claude-3-5-haiku-20241022
Claude 3 Opus,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modeling/generation",,"SOTA improvement,Training cost","Based on leaks, Claude 3 Opus and Sonnet probably cost >$1M to train.

""Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. ""

Table 1, Table 3",API access,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,,"The Claude 3 Model Family: Opus, Sonnet, Haiku",2024-03-04,Anthropic,,,,Training compute estimated to be 1.64e25 FLOP from benchmark scores.,Unspecified unreleased,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.
This has a knowledge cutoff date of August 2023, according to https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, which I assume means August 1, 2023.",,,,,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,,"Per https://time.com/6980000/anthropic/
""Claude 3 cost somewhere between $30 million and $300 million to train""
This would seem to include all three versions.

Ballpark estimate, based on relative API costs:
sqrt($30M * $300M) * (15 / (0.25 + 3 + 15)) = $78.0M
(cost) * (Sonnet share of API cost)

Convert to 2020 dollars: $64.7M",,Speculative,"We introduce Claude 3, a new family of large multimodal models – Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].
",2025-10-27 19:36:31+00:00,Robi Rahman,,0.0,United States of America,,,,,,,,,,,Industry,,,,,Unreleased,,True,17106058.0886546,Claude 3 Opus,,,True,,,,"$15 / 1M input tokens,$75 / 1M output tokens",2024-03-06T18:37:28.000Z,,,,True,,0.0,,,,,0,,,FP32,,Benchmarks,,,,,claude-3-opus-20240229,,True,,,,2023-08-01,,,,,,claude 3 opus,claude-3-5-sonnet-20240620,0.0103448275862068,2024-06-20,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",0.0059518867144507,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/h7f9jZVWzZNDHNxP5eewNH.eval,2025-03-07T18:54:11.499Z,h7f9jZVWzZNDHNxP5eewNH,claude-3-5-sonnet-20240620,Claude 3 Opus claude-3-5-sonnet-20240620
Claude 3 Opus,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modeling/generation",,"SOTA improvement,Training cost","Based on leaks, Claude 3 Opus and Sonnet probably cost >$1M to train.

""Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. ""

Table 1, Table 3",API access,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,,"The Claude 3 Model Family: Opus, Sonnet, Haiku",2024-03-04,Anthropic,,,,Training compute estimated to be 1.64e25 FLOP from benchmark scores.,Unspecified unreleased,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.
This has a knowledge cutoff date of August 2023, according to https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, which I assume means August 1, 2023.",,,,,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,,"Per https://time.com/6980000/anthropic/
""Claude 3 cost somewhere between $30 million and $300 million to train""
This would seem to include all three versions.

Ballpark estimate, based on relative API costs:
sqrt($30M * $300M) * (15 / (0.25 + 3 + 15)) = $78.0M
(cost) * (Sonnet share of API cost)

Convert to 2020 dollars: $64.7M",,Speculative,"We introduce Claude 3, a new family of large multimodal models – Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].
",2025-10-27 19:36:31+00:00,Robi Rahman,,0.0,United States of America,,,,,,,,,,,Industry,,,,,Unreleased,,True,17106058.0886546,Claude 3 Opus,,,True,,,,"$15 / 1M input tokens,$75 / 1M output tokens",2024-03-06T18:37:28.000Z,,,,True,,0.0,,,,,0,,,FP32,,Benchmarks,,,,,claude-3-opus-20240229,,True,,,,2023-08-01,,,,,,claude 3 opus,claude-3-7-sonnet-20250219_32K,0.0344827586206896,2025-02-24,Anthropic,United States of America,3.3499999999999998e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.0107332710388015,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/aZqVpPa4SWHapN9QHEoEm3.eval,2025-03-13T09:34:11.446Z,aZqVpPa4SWHapN9QHEoEm3,claude-3-7-sonnet-20250219_32k,Claude 3 Opus claude-3-7-sonnet-20250219_32K
Claude 3 Opus,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modeling/generation",,"SOTA improvement,Training cost","Based on leaks, Claude 3 Opus and Sonnet probably cost >$1M to train.

""Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. ""

Table 1, Table 3",API access,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,,"The Claude 3 Model Family: Opus, Sonnet, Haiku",2024-03-04,Anthropic,,,,Training compute estimated to be 1.64e25 FLOP from benchmark scores.,Unspecified unreleased,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.
This has a knowledge cutoff date of August 2023, according to https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, which I assume means August 1, 2023.",,,,,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,,"Per https://time.com/6980000/anthropic/
""Claude 3 cost somewhere between $30 million and $300 million to train""
This would seem to include all three versions.

Ballpark estimate, based on relative API costs:
sqrt($30M * $300M) * (15 / (0.25 + 3 + 15)) = $78.0M
(cost) * (Sonnet share of API cost)

Convert to 2020 dollars: $64.7M",,Speculative,"We introduce Claude 3, a new family of large multimodal models – Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].
",2025-10-27 19:36:31+00:00,Robi Rahman,,0.0,United States of America,,,,,,,,,,,Industry,,,,,Unreleased,,True,17106058.0886546,Claude 3 Opus,,,True,,,,"$15 / 1M input tokens,$75 / 1M output tokens",2024-03-06T18:37:28.000Z,,,,True,,0.0,,,,,0,,,FP32,,Benchmarks,,,,,claude-3-opus-20240229,,True,,,,2023-08-01,,,,,,claude 3 opus,claude-3-7-sonnet-20250219_64K,0.0310344827586206,2025-02-24,Anthropic,United States of America,3.3499999999999998e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.0102006417530873,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/DoU54Xy93J3MxztC4uxwYy.eval,2025-03-13T16:49:59.754Z,DoU54Xy93J3MxztC4uxwYy,claude-3-7-sonnet-20250219_64k,Claude 3 Opus claude-3-7-sonnet-20250219_64K
Grok-2,"Language,Vision,Multimodal","Chat,Language modeling/generation,Question answering,Code generation,Visual question answering",,Training cost,,API access,https://x.ai/blog/grok-2,,Grok-2 Beta Release,2024-08-13,xAI,,,2.9599999999999996e+25,Estimate based on xAI statements comparing Grok-2 compute to GPT-4 and Grok-3. Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing,Unspecified unreleased,"Knowledge cutoff date is August 2024, according to https://llm-stats.com/models/grok-2.",,,,,,,,NVIDIA H100 SXM5 80GB,,,,Confident,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.,2025-10-17 18:39:51+00:00,Natalia Martemianova,,,United States of America,,,,,,,,,,,Industry,,2.1e+25,,,Unreleased,,True,31602310.53083552,Grok-2,,,True,700,,,,2024-08-17T03:34:15.000Z,,,,True,,9.366403754281892e+17,2022-09-20,1.897330595482546,66910000000000,494700000000000.0,1,989400000000000.0,133800000000000,TF16,989400000000000,"Comparison with other models,Reported",,,,NVIDIA,"grok-2-1212,grok-2-vision-1212,grok-2-0813",True,True,,,,2024-08-01,,,,,,grok-2,grok-2-1212,0.0068965517241379,2024-12-12,xAI,United States of America,2.9599999999999996e+25,Estimate based on xAI statements comparing Grok-2 compute to GPT-4 and Grok-3. Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing,0.004868154158215,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/kinyvFjqSqUiY5FyYv7tsA.eval,2025-03-06T18:06:22.963Z,kinyvFjqSqUiY5FyYv7tsA,grok-2-1212,Grok-2 grok-2-1212
Grok 3,"Language,Vision,Multimodal","Chat,Language modeling/generation,Question answering,Code generation,Visual question answering",,Training cost,,API access,https://x.ai/blog/grok-3,,Grok 3 Beta — The Age of Reasoning Agents,2025-02-17,xAI,,,3.5000000000000006e+26,"Estimate based on a cluster of 80,000 H100s per the xai website and an estimated training time of approximately three months.

Full estimate here: https://docs.google.com/document/d/1MIUFviULJ3YI_XjyzL8cwG0cBRANKNxVEB4DrUcFiNs/edit?usp=sharing",Unspecified unreleased,"Knowledge cutoff date is November 17, 2024, according to https://docs.x.ai/docs/models#models-and-pricing. ",,,,,,2160.0,"Estimated to be approximately 3 months. See compute estimate notes for more details.
",NVIDIA H100 SXM5 80GB,,,,Confident,"We are pleased to introduce Grok 3, our most advanced model yet: blending strong reasoning with extensive pretraining knowledge. Trained on our Colossus supercluster with 10x the compute of previous state-of-the-art models, Grok 3 displays significant improvements in reasoning, mathematics, coding, world knowledge, and instruction-following tasks. Grok 3's reasoning capabilities, refined through large scale reinforcement learning, allow it to think for seconds to minutes, correcting errors, exploring alternatives, and delivering accurate answers. Grok 3 has leading performance across both academic benchmarks and real-world user preferences, achieving an Elo score of 1402 in the Chatbot Arena. Alongside it, we’re unveiling Grok 3 mini, which represents a new frontier in cost-efficient reasoning. Both models are still in training and will evolve rapidly with your feedback. We are rolling out Grok 3 to users in the coming days, along with an early preview of its reasoning capabilities.",2025-10-17 18:39:06+00:00,James Sanders,,,United States of America,,,,80000.0,,,xAI Memphis Colossus,,,,Industry,,2.1e+26,1.1300000000000001e+27,,Unreleased,,True,217835545.5267339,Grok-3,,,True,700,,,,2025-02-19T20:18:12.000Z,,,,True,109948656.20196916,1.606716659366532e+18,2022-09-20,2.412046543463381,66910000000000,494700000000000.0,1,989400000000000.0,133800000000000,TF16,989400000000000,"Hardware,Comparison with other models",,,,NVIDIA,"grok-3-beta,grok-3",,True,,,,2024-11-17,,,,849737163.9529365,4446013316.520631,grok 3,grok-3-beta,0.0379310344827586,2025-04-09,xAI,United States of America,3.5000000000000006e+26,"Estimate based on a cluster of 80,000 H100s per the xai website and an estimated training time of approximately three months.

Full estimate here: https://docs.google.com/document/d/1MIUFviULJ3YI_XjyzL8cwG0cBRANKNxVEB4DrUcFiNs/edit?usp=sharing",0.0112370296019996,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/DzjFQArKFTZHioQVwKYVtX.eval,2025-04-10T17:57:03.446Z,DzjFQArKFTZHioQVwKYVtX,grok-3-beta,Grok 3 grok-3-beta
Grok 3,"Language,Vision,Multimodal","Chat,Language modeling/generation,Question answering,Code generation,Visual question answering",,Training cost,,API access,https://x.ai/blog/grok-3,,Grok 3 Beta — The Age of Reasoning Agents,2025-02-17,xAI,,,3.5000000000000006e+26,"Estimate based on a cluster of 80,000 H100s per the xai website and an estimated training time of approximately three months.

Full estimate here: https://docs.google.com/document/d/1MIUFviULJ3YI_XjyzL8cwG0cBRANKNxVEB4DrUcFiNs/edit?usp=sharing",Unspecified unreleased,"Knowledge cutoff date is November 17, 2024, according to https://docs.x.ai/docs/models#models-and-pricing. ",,,,,,2160.0,"Estimated to be approximately 3 months. See compute estimate notes for more details.
",NVIDIA H100 SXM5 80GB,,,,Confident,"We are pleased to introduce Grok 3, our most advanced model yet: blending strong reasoning with extensive pretraining knowledge. Trained on our Colossus supercluster with 10x the compute of previous state-of-the-art models, Grok 3 displays significant improvements in reasoning, mathematics, coding, world knowledge, and instruction-following tasks. Grok 3's reasoning capabilities, refined through large scale reinforcement learning, allow it to think for seconds to minutes, correcting errors, exploring alternatives, and delivering accurate answers. Grok 3 has leading performance across both academic benchmarks and real-world user preferences, achieving an Elo score of 1402 in the Chatbot Arena. Alongside it, we’re unveiling Grok 3 mini, which represents a new frontier in cost-efficient reasoning. Both models are still in training and will evolve rapidly with your feedback. We are rolling out Grok 3 to users in the coming days, along with an early preview of its reasoning capabilities.",2025-10-17 18:39:06+00:00,James Sanders,,,United States of America,,,,80000.0,,,xAI Memphis Colossus,,,,Industry,,2.1e+26,1.1300000000000001e+27,,Unreleased,,True,217835545.5267339,Grok-3,,,True,700,,,,2025-02-19T20:18:12.000Z,,,,True,109948656.20196916,1.606716659366532e+18,2022-09-20,2.412046543463381,66910000000000,494700000000000.0,1,989400000000000.0,133800000000000,TF16,989400000000000,"Hardware,Comparison with other models",,,,NVIDIA,"grok-3-beta,grok-3",,True,,,,2024-11-17,,,,849737163.9529365,4446013316.520631,grok 3,grok-3-mini-beta_high,0.0586206896551724,2025-04-09,xAI,United States of America,,,0.0138184351563903,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/Z83F6bJ7Ne5rXaXwgR2xWT.eval,2025-04-10T18:02:17.992Z,Z83F6bJ7Ne5rXaXwgR2xWT,grok-3-mini-beta_high,Grok 3 grok-3-mini-beta_high
Grok 3,"Language,Vision,Multimodal","Chat,Language modeling/generation,Question answering,Code generation,Visual question answering",,Training cost,,API access,https://x.ai/blog/grok-3,,Grok 3 Beta — The Age of Reasoning Agents,2025-02-17,xAI,,,3.5000000000000006e+26,"Estimate based on a cluster of 80,000 H100s per the xai website and an estimated training time of approximately three months.

Full estimate here: https://docs.google.com/document/d/1MIUFviULJ3YI_XjyzL8cwG0cBRANKNxVEB4DrUcFiNs/edit?usp=sharing",Unspecified unreleased,"Knowledge cutoff date is November 17, 2024, according to https://docs.x.ai/docs/models#models-and-pricing. ",,,,,,2160.0,"Estimated to be approximately 3 months. See compute estimate notes for more details.
",NVIDIA H100 SXM5 80GB,,,,Confident,"We are pleased to introduce Grok 3, our most advanced model yet: blending strong reasoning with extensive pretraining knowledge. Trained on our Colossus supercluster with 10x the compute of previous state-of-the-art models, Grok 3 displays significant improvements in reasoning, mathematics, coding, world knowledge, and instruction-following tasks. Grok 3's reasoning capabilities, refined through large scale reinforcement learning, allow it to think for seconds to minutes, correcting errors, exploring alternatives, and delivering accurate answers. Grok 3 has leading performance across both academic benchmarks and real-world user preferences, achieving an Elo score of 1402 in the Chatbot Arena. Alongside it, we’re unveiling Grok 3 mini, which represents a new frontier in cost-efficient reasoning. Both models are still in training and will evolve rapidly with your feedback. We are rolling out Grok 3 to users in the coming days, along with an early preview of its reasoning capabilities.",2025-10-17 18:39:06+00:00,James Sanders,,,United States of America,,,,80000.0,,,xAI Memphis Colossus,,,,Industry,,2.1e+26,1.1300000000000001e+27,,Unreleased,,True,217835545.5267339,Grok-3,,,True,700,,,,2025-02-19T20:18:12.000Z,,,,True,109948656.20196916,1.606716659366532e+18,2022-09-20,2.412046543463381,66910000000000,494700000000000.0,1,989400000000000.0,133800000000000,TF16,989400000000000,"Hardware,Comparison with other models",,,,NVIDIA,"grok-3-beta,grok-3",,True,,,,2024-11-17,,,,849737163.9529365,4446013316.520631,grok 3,grok-3-mini-beta_low,0.0275862068965517,2025-04-09,xAI,United States of America,,,0.0096343546345135,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/9FTMS2t6Ksm43CduHeqtJr.eval,2025-04-10T20:30:21.697Z,9FTMS2t6Ksm43CduHeqtJr,grok-3-mini-beta_low,Grok 3 grok-3-mini-beta_low
Llama Nemotron Ultra 253B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Neural Architecture Search - NAS","Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam et al. (32 additional authors not shown)",,,Open weights (restricted use),https://arxiv.org/abs/2505.00949,,Ultra is 253B distilled from Llama 3.1 405B for maximum agentic accuracy on multi-GPU data center servers.,2025-03-18,NVIDIA,253000000000.0,"253B
""Dense decoder-only Transformer model Network Architecture: Llama-3.1-405B-Instruct, customized through Neural Architecture Search (NAS)

**This model was developed based on Llama-3.1-405B-Instruct
** This model has 253B model parameters.""",3.911001e+25,"Total training compute: 3.8e+25 FLOP (base model) + 1.11e+24 FLOP (fine-tuning) = 3.9e25 FLOP
See calculation in the finetune compute notes.","Unspecified unreleased,Llama Nemotron Post Training Dataset",,603000000000.0,"KD + Continued Training: 
""LN-Ultra is first trained with knowledge distillation for 65B tokens using the same distillation dataset, followed by 88B tokens of continued training on the Nemotron-H phase 4 pretraining dataset (NVIDIA et al., 2025)."" (from the paper)

Reasoning training data (SFT): 
for Super model (from the blog) ""60B tokens of synthetic data (representing 4M of the 30M generated samples)"" -> the entire dataset is ~450B tokens (Ultra model is likely to be trained on the entire dataset (""Likely"" confidence}

65b+88b+450b = 603b tokens

RL for Scientific Reasoning:140k h100 hours (240k samples)

RL for instruction following: 30k prompts

RL for chat: 50k prompts",,,,,,,,,,Likely,"Llama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-405B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling. The model supports a context length of 128K tokens. This model fits on a single 8xH100 node for inference.

Llama-3.1-Nemotron-Ultra-253B-v1 is a model which offers a great tradeoff between model accuracy and efficiency. Efficiency (throughput) directly translates to savings. Using a novel Neural Architecture Search (NAS) approach, we greatly reduce the model’s memory footprint, enabling larger workloads, as well as reducing the number of GPUs required to run the model in a data center environment. This NAS approach enables the selection of a desired point in the accuracy-efficiency tradeoff. Furthermore, by using a novel method to vertically compress the model (see details here), it also offers a significant improvement in latency.

The model underwent a multi-phase post-training process to enhance both its reasoning and non-reasoning capabilities. This includes a supervised fine-tuning stage for Math, Code, Reasoning, Chat, and Tool Calling as well as multiple reinforcement learning (RL) stages using Group Relative Policy Optimization (GRPO) algorithms for reasoning, chat, and instruction-following.",2025-09-28 01:00:10+00:00,Natalia Martemianova,,,United States of America,Llama 3.1-405B,1.114817000000001e+24,"Knowledge Distillation + Continued pre-training + SFT: 

6 FLOP / parameter / token * 253000000000 parameters * 6033000000000 tokens [see dataset size notes] = 9.15354e+23 FLOP

RL: ""the whole training takes approximately 140k H100 hours""

989400000000000 FLOP / sec / GPU [bf16] * 140000 GPU-hours * 3600 sec / hour * 0.4 [assumed utilization] = 1.9946304e+23 FLOP

Total: 9.15354e+23 FLOP + 1.9946304e+23 FLOP = 1.114817e+24 FLOP
",,,,,,,,Industry,,,,,Unreleased,"GOVERNING TERMS: Your use of this model is governed by the NVIDIA Open Model License. Additional Information: Llama 3.3 Community License Agreement. Built with Llama.

https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1",True,,Llama Nemotron Ultra,,,,,3.8e+25,-4.80699999999935e+21,,2025-04-21T19:30:46.000Z,,,,True,,inf,,,,,0,,,FP32,,Operation counting,nvidia,,,,,,True,,,,,,,,,,llama nemotron ultra 253b,Llama-4-Scout-17B-16E-Instruct,0.0,2025-04-05,Meta AI,United States of America,4.08e+24,"40T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md  

Estimating training compute from parameters and tokens:
6 FLOP per token per parameter * 17B active parameters * 40T tokens = 4.08e24 FLOP
(Implying mean throughput was 227 TFLOPS/GPU, or 11.5% MFU in FP8)


The model card also states that Llama 4 Scout used 5.0M H100-hours.
The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:
Compute = 390 TFLOP/s * 5 million hours = 7.02e24 FLOP
(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Scout than for Behemoth.)",0.0,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/bhjLDA5A7gddsX5cAYKma8.eval,2025-04-08T10:14:56.486Z,bhjLDA5A7gddsX5cAYKma8,llama-4-scout-17b-16e-instruct,Llama Nemotron Ultra 253B Llama-4-Scout-17B-16E-Instruct
Llama Nemotron Ultra 253B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Neural Architecture Search - NAS","Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam et al. (32 additional authors not shown)",,,Open weights (restricted use),https://arxiv.org/abs/2505.00949,,Ultra is 253B distilled from Llama 3.1 405B for maximum agentic accuracy on multi-GPU data center servers.,2025-03-18,NVIDIA,253000000000.0,"253B
""Dense decoder-only Transformer model Network Architecture: Llama-3.1-405B-Instruct, customized through Neural Architecture Search (NAS)

**This model was developed based on Llama-3.1-405B-Instruct
** This model has 253B model parameters.""",3.911001e+25,"Total training compute: 3.8e+25 FLOP (base model) + 1.11e+24 FLOP (fine-tuning) = 3.9e25 FLOP
See calculation in the finetune compute notes.","Unspecified unreleased,Llama Nemotron Post Training Dataset",,603000000000.0,"KD + Continued Training: 
""LN-Ultra is first trained with knowledge distillation for 65B tokens using the same distillation dataset, followed by 88B tokens of continued training on the Nemotron-H phase 4 pretraining dataset (NVIDIA et al., 2025)."" (from the paper)

Reasoning training data (SFT): 
for Super model (from the blog) ""60B tokens of synthetic data (representing 4M of the 30M generated samples)"" -> the entire dataset is ~450B tokens (Ultra model is likely to be trained on the entire dataset (""Likely"" confidence}

65b+88b+450b = 603b tokens

RL for Scientific Reasoning:140k h100 hours (240k samples)

RL for instruction following: 30k prompts

RL for chat: 50k prompts",,,,,,,,,,Likely,"Llama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-405B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling. The model supports a context length of 128K tokens. This model fits on a single 8xH100 node for inference.

Llama-3.1-Nemotron-Ultra-253B-v1 is a model which offers a great tradeoff between model accuracy and efficiency. Efficiency (throughput) directly translates to savings. Using a novel Neural Architecture Search (NAS) approach, we greatly reduce the model’s memory footprint, enabling larger workloads, as well as reducing the number of GPUs required to run the model in a data center environment. This NAS approach enables the selection of a desired point in the accuracy-efficiency tradeoff. Furthermore, by using a novel method to vertically compress the model (see details here), it also offers a significant improvement in latency.

The model underwent a multi-phase post-training process to enhance both its reasoning and non-reasoning capabilities. This includes a supervised fine-tuning stage for Math, Code, Reasoning, Chat, and Tool Calling as well as multiple reinforcement learning (RL) stages using Group Relative Policy Optimization (GRPO) algorithms for reasoning, chat, and instruction-following.",2025-09-28 01:00:10+00:00,Natalia Martemianova,,,United States of America,Llama 3.1-405B,1.114817000000001e+24,"Knowledge Distillation + Continued pre-training + SFT: 

6 FLOP / parameter / token * 253000000000 parameters * 6033000000000 tokens [see dataset size notes] = 9.15354e+23 FLOP

RL: ""the whole training takes approximately 140k H100 hours""

989400000000000 FLOP / sec / GPU [bf16] * 140000 GPU-hours * 3600 sec / hour * 0.4 [assumed utilization] = 1.9946304e+23 FLOP

Total: 9.15354e+23 FLOP + 1.9946304e+23 FLOP = 1.114817e+24 FLOP
",,,,,,,,Industry,,,,,Unreleased,"GOVERNING TERMS: Your use of this model is governed by the NVIDIA Open Model License. Additional Information: Llama 3.3 Community License Agreement. Built with Llama.

https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1",True,,Llama Nemotron Ultra,,,,,3.8e+25,-4.80699999999935e+21,,2025-04-21T19:30:46.000Z,,,,True,,inf,,,,,0,,,FP32,,Operation counting,nvidia,,,,,,True,,,,,,,,,,llama nemotron ultra 253b,Llama-4-Maverick-17B-128E-Instruct-FP8,0.0068965517241379,2025-04-05,Meta AI,United States of America,2.244000000001e+24,"22T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md   

Maverick was trained using co-distillation from Llama 4 Behemoth. It isn't 100% clear that all 22T tokens used distillation, but we assume this for the time being.

Estimating training compute from parameters and tokens:
Compute = 6 FLOP per token per parameter * 17B active parameters * 22T tokens = 2.244e24 FLOP
(Implying mean throughput was 262 TFLOPS/GPU, or 13.2% MFU in FP8)


The model card also states that Llama 4 Maverick used 2.38M H100-hours.
The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:
Compute = 390 TFLOP/s * 2.38 million hours = 3.342e24 FLOP
(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Maverick than for Behemoth.)",0.004868154158215,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/QjvBS34fsngfBBMZUirSyh.eval,2025-04-08T10:17:16.175Z,QjvBS34fsngfBBMZUirSyh,llama-4-maverick-17b-128e-instruct-fp8,Llama Nemotron Ultra 253B Llama-4-Maverick-17B-128E-Instruct-FP8
Grok 4,"Language,Multimodal,Vision","Language modeling/generation,Question answering,Search,Visual question answering,Character recognition (OCR),Image captioning,Quantitative reasoning",,Training cost,,API access,https://x.ai/news/grok-4,,Grok 4,2025-07-09,xAI,,Rumored to be 2.4T params (https://x.com/kalomaze/status/1942996555088134592),5.0000000000001e+26,"We think that RL relative to pre-compute is between our estimate for o3 (10% of pre-training) and the 100% implied by this slide in the launch ( https://archive.is/f0vJU ). Assuming the same pre-training as Grok 3 (also implied by that slide, and much more consistent) and that Grok 3 used a tenth as much RL, we get:

2 * (grok3/1.1) in the high case (rl is 10% of grok 3, so grok3/1.1 is grok3 precompute, and in this case twice that is grok 4)
1.1 * (grok3/1.01) in the low case
The geometric mean is (rounded to one sig fig): 5e26
",Unspecified unreleased,,,,,,,,,,,"See https://colab.research.google.com/drive/1nAl9CJi6VFLYZszzVEOLIlkxx1NMc_Lv?usp=sharing

and 
https://docs.google.com/document/d/1gF9VLRaQx__TN2pdgs-P4K_UEW5PhMQ2tGwJPwPcV6A/edit?tab=t.0",,Speculative,"Grok 4 is the most intelligent model in the world. It includes native tool use and real-time search integration, and is available now to SuperGrok and Premium+ subscribers, as well as through the xAI API. We are also introducing a new SuperGrok Heavy tier with access to Grok 4 Heavy - the most powerful version of Grok 4.",2025-10-17 18:37:57+00:00,Tom Adamczewski,,,United States of America,,,,200000.0,,,"""we utilized Colossus, our 200,000 GPU cluster, to run reinforcement learning training that refines Grok's reasoning abilities at pretraining scale""",,,,Industry,,,,,Unreleased,,True,387830439.15113616,Grok 4,,,True,,,,,2025-07-08T23:07:05.000Z,,,,True,,1.2892232004645765e+18,,,,,0,,,FP32,,Comparison with other models,,,,,grok-4-0709,,True,,,,,,,,,10717018695.585222,grok 4,grok-4-0709,0.1211,2025-07-09,xAI,United States of America,5.0000000000001e+26,"We think that RL relative to pre-compute is between our estimate for o3 (10% of pre-training) and the 100% implied by this slide in the launch ( https://archive.is/f0vJU ). Assuming the same pre-training as Grok 3 (also implied by that slide, and much more consistent) and that Grok 3 used a tenth as much RL, we get:

2 * (grok3/1.1) in the high case (rl is 10% of grok 3, so grok3/1.1 is grok3 precompute, and in this case twice that is grok 4)
1.1 * (grok3/1.01) in the low case
The geometric mean is (rounded to one sig fig): 5e26
",0.0192,,,,gda5UeWrA8HcbDCRuLJ56H,grok-4-0709,Grok 4 grok-4-0709
