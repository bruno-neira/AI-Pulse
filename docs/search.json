[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI Pulse: Exploring the Drivers of AI Development",
    "section": "",
    "text": "1 Introduction\nFrom its humble beginnings in the 1950s, and through a cycle of dark and golden ages, AI finally exploded in the early 2020s with the introduction of large language models. Studying the history of AI development, we see breakthroughs resulting from innovations in various front. AlexNet in 2012 ushered in a new era of deep learning fueled by the availability of large datasets and shift towards GPU training. Five years later, in 2017, an algorithmic innovation, attention and the transformer architecture, ushered in yet another era of AI, namely the era of large language models which is still experiencing rapid growth today.\nWhether it be hardware infrastructure, data quantity and quality, or algorithmic innovations, each of these factors has played an important role in the development of new AI techniques or as bottlenecks to said breakthroughs. In this project we aim to explore how these factors have changed over time in order to have an in-depth understanding of the driving forces behind AI development. With this understanding, we hope to be better able to understand where AI may be heading in the future.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data",
    "section": "",
    "text": "2.1 Description\nOur data is collected by a non-profit organization called Epoch AI. From their (website)[https://epoch.ai/about], Epoch AI’s mission is:\nIn the past, Epoch AI’s work has been cited in Our World in Data, Time Magazine, by Google CEO Sundar Pichai, and other reputable sources.\nGenerally, we frame our work in terms of input factors and output results and further subdivide these into input factors for AI models, input factors for AI companies, output results for AI models, and output results for AI companies:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "2.2 Missing value analysis",
    "text": "2.2 Missing value analysis\n\n2.2.1 Compute Spend\n\n\nCode\nlibrary(redav)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\n\ncompute_spend &lt;- read_csv(\"./Data/ai_companies_dataset/ai_companies_compute_spend.csv\")\n\nusage_rates &lt;- read_csv(\"./Data/ai_companies_dataset/ai_companies_usage_reports.csv\")\n\nai_models &lt;- read_csv(\"./Data/ai_models_dataset/all_ai_models.csv\")\n\nfronteir_math &lt;- read_csv(\"./Data/benchmark_dataset/frontiermath.csv\")\n\ngpu_cluster_ds &lt;- read_csv(\"./Data/gpu_clusters_dataset/gpu_clusters.csv\")\n\n\np1 &lt;- plot_missing(compute_spend, max_cols = 10, num_char = 7, percent = FALSE)\np2 &lt;- plot_missing(compute_spend, num_char = 2, percent = TRUE)\np1\n\n\n\n\n\n\n\n\n\nCode\np2\n\n\n\n\n\n\n\n\n\nThe above graph describes the missing values of the compute spend dataset. Fortunately, most of the missing data in this dataset is meta-data like “Source 2” and “Source 3” (sources of the information) or “Exclude from graph view”. However, there are a few columns with high missing values that would be nice to have:\n\nInference compute spend (63% missing)\nR&D compute spend (72% missing)\nTotal compute spend (63% missing)\n\n\n\n2.2.2 Usage Rates\n\n\nCode\np3 &lt;- plot_missing(usage_rates, max_cols = 10, num_char = 7, percent = FALSE)\np4 &lt;- plot_missing(usage_rates, num_char = 2, percent = TRUE)\np3\n\n\n\n\n\n\n\n\n\nCode\np4\n\n\n\n\n\n\n\n\n\nThe usage reports datasets follows a similar missing values pattern as the compute spend dataset. Columns with the most missing values are meta-data (“Source 2”, “Graph Note”, “Exclude from graph view”).\nLikewise there are some important columns that are missing in most entries that would be lovely to have and present a limitation of the current data:\n\nActive Users (30% missing)\nActive Users time period (30% missing)\nDaily Tokens (82% missing)\nDaily Messages (76% missing)\n\n\n\n2.2.3 Frontier Math Benchmark\n\n\nCode\np7 &lt;- plot_missing(fronteir_math, max_cols = 10, num_char = 7, percent = FALSE)\np8 &lt;- plot_missing(fronteir_math, num_char = 2, percent = TRUE)\np7\n\n\n\n\n\n\n\n\n\nCode\np8\n\n\n\n\n\n\n\n\n\nIn the benchmark dataset we used, the FrontierMath benchmark, there are a few notable columns with large percentages of missing values. Notably “Log viewer” has 100% missing data. We expect that Epoch AI added this column to be consistent with other benchmarks but is not applicable to this dataset.\nOtherwise, the only two columns with large percentages of missing values are “Training compute (FLOP)” (75% missing) and “Training compute notes” (42%)\n\n\n2.2.4 AI Models\nOur GPU Clusters and AI Models dataset did not render properly for the missing values plot, so we will manually describe the missing values below.\nThe AI models dataset is likely Epoch’s most comprehensive dataset. However, Epoch has included many columns that are only applicable to a small subset of the models. Some of these columns are meta-data:\n\nNotability criteria (72% missing)\nNotability criteria notes (76% missing)\nArchived Links (99% missing)\nUtilization Notes (97% missing)\nFrontier Model (95% missing)\nHugging Face developer id (81% missing)\n\nand others. We expect not to use any meta-data columns so we are not concerned with the high level of missing values.\nThere are many training-related columns with high percentages of missing values, for example:\n\nHardware utilization (99% missing)\nPost-training compute (FLOP) (99% missing)\nTraining compute cost (92% missing)\nTraining chip-hours (93% missing)\nTraining data center (97% missing)\nTraining time (hours) (82% missing)\nParameters (35% missing)\n\nand others. While this provides a significant limitation to our exploration, we expect that the missing value levels are high since the dataset contains models dating back to the 1960s. We expect these percentages to be lower amongst newer models which we care more about.\n\n\n2.2.5 GPU Clusters\nLike previous datasets, the columns in the GPU clusters dataset are mostly meta-data (Sources 1 through 5, notes of various kinds).\nNotable columns useful for analysis with high missing values include:\n\nReported Power Capacity (MW) (83% missing)\nChip type (primary) (41% missing)\nOwner (36% missing)\nHardware cost (31% missing)\nLocation (22% missing)\nUsers (40% missing)\nFirst operational date (32% missing)\n\nThough some of these columns have higher than ideal levels of missing values, we expect to have enough data to provide useful analysis.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#description",
    "href": "data.html#description",
    "title": "2  Data",
    "section": "",
    "text": "Epoch AI is a multidisciplinary non-profit research institute investigating the future of artificial intelligence. We examine the driving forces behind AI and forecast its economic and societal impact. We emphasize making our research accessible through our reports, models and visualizations to help ground the discussion of AI on a solid empirical footing. Our goal is to create a healthy scientific environment, where claims about AI are discussed with the rigor they merit.\n\n\n\n\nAI Models:\n\nInputs:\n\nGPU Clusters\nCompute Spend\n\nOutputs:\n\nPerformance on model benchmarks\n\n\nAI Companies:\n\nInputs:\n\nStaffing\nFunding\n\nOutputs:\n\nUsage of AI tools\nRevenue\n\n\n\n\n2.1.1 Collection Methods\nEpoch AI collects data on variety of components related to AI progress including hardware advancements, GPU cluster advancements, model releases, AI company funding, etc. For the most part, Epoch uses a mix of automated collection methods (for example, automatic updates of AI benchmark scores) and manual data collection from public sources of information (company blog posts, news posts, public satellite imagery). Here we link to the full set of data collection methodologies:\n\nGPU Cluster Data\nAI Company Data\nAI Model Data\nAI Benchmark Data\n\n\n\n2.1.2 Datasets Overview\n\n\n\n\n\n\n\n\n\n\nTitle\nDescription\nFrequency of Updates\nDimensions\nNotes\n\n\n\n\nGPU Clusters\nDataset listing planned, existing, and previous GPU clusters\nDaily\n786 rows, 55 columns\nDaily refreshes, but data may not necessarily be updated daily.\n\n\nAI Companies\nVarious different datasets pertaining to AI company revenue, compute spend, staffing, funding rounds, user usage, and general information\nDaily\nVarious datasets\n\n\n\nAI Models\nDataset listing AI models over the years (including primitive models)\nDaily\n3000+ rows, 56 columns\nDivided into “Frontier”, “Notable”, and “Large Scale” models\n\n\nAI Benchmark Data\nA collection of different datasets of ML model performance on different benchmarks\nnot found\nVarious datasets\n\n\n\n\nPlease see links above for finer details on the datasets.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#other-problems-and-limitations-of-the-data",
    "href": "data.html#other-problems-and-limitations-of-the-data",
    "title": "2  Data",
    "section": "2.3 Other problems and limitations of the data",
    "text": "2.3 Other problems and limitations of the data\nOther than the missing values mentioned above. The data is limited in the fact that many of the columns across the datasets have inconsistent value formats. For example, the location column in the GPU clusters dataset may contain coordinates, addresses, or general regions where the cluster is located.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "3  Results",
    "section": "",
    "text": "3.1 Compute Spend and Usage\nData loading:\nLooking at active user metrics as they relate to cluster compute capacity – we wanted to start off simple with a scatterplot and trendline to give a taste of the scale and direction the industry of AI is heading towards. Let’s begin by looking at the elephant in the room, OpenAI:\nCode\ng1 &lt;- ggplot(usage_reports, aes(Date, `Active users`/ 1e9)) +\n  # geom_line(color = \"grey30\") +\n  geom_point(size = 1) +\n  geom_smooth(method = \"loess\", span = 0.5, se = FALSE) +\n  labs(title = \"Loess Curve of OpenAI's Monthly Active Users Over Time\",\n       x = \"Date\",\n       y = \"Number of Users (in billions)\") +\n  theme_minimal()\n\ng1\n\n\n\n\n\n\n\n\n\nCode\n# ggplot() +\n#   geom_col(data = openai_stacked, aes(x = Date, y = Amount, fill = Category), position = \"stack\") +\n#   geom_point(data = usage_reports, aes(x = Date, y = `Active users`), size = 1, color = \"black\") +\n#   geom_smooth(data = usage_reports, aes(x = Date, y = `Active users`), method = \"loess\", span = 0.5, se = FALSE) +\n#   labs(title = \"OpenAI's Inference Spending overlayed by Monthly Active Users\",\n#        x = \"Date\",\n#        y = \"Dollars Spent (stacked bars) / Users (loess curve)\") +\n#   theme_minimal()\nFor this graph, we used the Loess method to see if we see could extrapolate some patterns out of our noisy user-metrics - Notably, there was a dip in users of ChatGPT at the start of May 2025 which might be attributable to news of CEO Sam Altman having to testify before the U.S. Senate the same week (anti-AI sentiments) - Even so, the long term trend seems to point towards a steadily rising user base for OpenAI over the next couple of years\nNow let’s take a look at the relationship between the aggregate amount of dollars spent by foundation model developers over time. Comparing OpenAI to Anthropic, we can see how each company prioritizes where to invest in compute. We selected Inference compute spend, R&D compute spend, and Total compute spend columns in pivot longer to be aggregated in my stacked bar chart. This helped to mitigate some of the effects of relevant values being dispersed across other columns and the presence of null entries (more on this in the missing data section).\nCode\nggplot(compute_stacked, aes(x = Year, y = Amount, fill = Category)) +\n  geom_col(position = \"stack\") +  # Stacked bars\n  facet_wrap(~Company) +\n  labs(title = \"Breakdown of Total Compute Spend Over Time\", \n       y = \"Amount of Dollars Spent\",\n       fill = \"Expenditure Type\") +\n  theme_minimal() + \n  theme(\n    strip.text = element_text(),\n    strip.background = element_rect(fill = \"gray\", color = \"lightgray\"),\n  )\nFrom the bar chart, it appears that OpenAI’s aggregate compute spending is growing exponentially; however, we cannot confirm this is truly the case since at least one of the three categories of expenditures are missing from the years 2022-2024. - This is the same, maybe even more so, in the case of Anthropic’s spending breakdown. - Cost values are completely non-existent in 2022 and 2023 for Anthropic and OpenAI respectively. Poor data collection, perhaps. - At least in 2026, OpenAI’s expenditures seem to be relatively balanced, but unfortunately, we don’t know how their total compute spend decomposes into the other two categories. While we don’t know the rest of the expenditures for Anthropic in 2026, their inference takes up all of their total compute spend in the previous year– signaling a shift in company priorities.\nCombining the previous compute-cost time-series (specifically on inference) with the user-base time-series from the first plot for OpenAI, we can see how these trends unfold when graphed on top of each other:\nCode\ng2 &lt;- ggplot() +\n  geom_col(data = openai_stacked, aes(x = Date, y = Amount / 1e9, fill = Category), position = \"stack\") +\n  geom_point(data = usage_reports, aes(x = Date, y = `Active users` / 1e9), size = 1, color = \"black\") +\n  geom_smooth(data = usage_reports, aes(x = Date, y = `Active users` / 1e9), method = \"loess\", span = 0.5, se = FALSE) +\n  labs(title = \"OpenAI's Inference Spending overlayed by Monthly Active Users (in billions)\",\n       x = \"Date\",\n       y = \"Dollars Spent (bars) / Users (loess curve)\") +\n  theme_minimal() + \n  theme(legend.position = \"none\")\n \ng2\nNOTABLE LIMITATIONS: - We combined the y-axis of both graphs since the dollars spent and number of active users have roughly the same number of observations. This is not a recommended practice. - We understand how this can appear misleading to the viewer; this graph may not be suitable should the dollars spent in the future outpace the number of users by a (visually) significant amount or vice versa.\nBack to the analysis though, it just so happens that inference costs align almost 1:1 with the number of users. This is not entirely representative of our data since the total compute costs are not accounted for here! The real ratio of users to inference costs may likely be closer to 1:2, 1:3, etc. Overall, this preliminary look at how usage-compute capacity evolves over time gives us a lot of insight at the company level. Sadly these datasets don’t provide enough information at the moment to see if we are reaching (or will even reach) a plateau in compute and usage for the industry as a whole.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#gpu-clusters",
    "href": "results.html#gpu-clusters",
    "title": "3  Results",
    "section": "3.2 GPU Clusters",
    "text": "3.2 GPU Clusters\n\n3.2.1 GPU Clusters and Model Development Timeline\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\n\n# Load the combined dataset\ncombined_data &lt;- read_csv(\"./Data/combined_gpu_models.csv\")\n\n# Create owner buckets\nbucket_owners &lt;- function(owner) {\n  case_when(\n    grepl(\"Google DeepMind\", owner, ignore.case = TRUE) ~ \"Google DeepMind\",\n    grepl(\"OpenAI\", owner, ignore.case = TRUE) ~ \"OpenAI\",\n    grepl(\"Anthropic\", owner, ignore.case = TRUE) ~ \"Anthropic\",\n    grepl(\"xAI\", owner, ignore.case = TRUE) ~ \"xAI\",\n    grepl(\"Meta\", owner, ignore.case = TRUE) ~ \"Meta\",\n    grepl(\"Alibaba\", owner, ignore.case = TRUE) ~ \"Alibaba\",\n    grepl(\"Microsoft\", owner, ignore.case = TRUE) ~ \"Microsoft\",\n    grepl(\"Amazon\", owner, ignore.case = TRUE) ~ \"Amazon\",\n    TRUE ~ \"Other\"\n  )\n}\n\n# Apply bucketing\ncombined_data &lt;- combined_data |&gt;\n  mutate(owner_bucket = bucket_owners(owner))\n\n# Separate models and clusters\nmodels &lt;- combined_data |&gt; filter(type == \"model\")\nclusters &lt;- combined_data |&gt;\n  filter(type == \"cluster\", owner_bucket != \"Other\")  # Exclude \"Other\" clusters for lines\n\n# Find the date of the first GPU cluster\nfirst_cluster_date &lt;- min(combined_data$date[combined_data$type == \"cluster\"], na.rm = TRUE)\n\n# Filter both models and clusters to only include data after first cluster\nmodels &lt;- models |&gt; filter(date &gt;= first_cluster_date)\nclusters &lt;- clusters |&gt; filter(date &gt;= first_cluster_date)\n\n# Define color palette for the buckets\ncolor_palette &lt;- c(\n  \"Google DeepMind\" = \"#4285F4\",  # Google Blue\n  \"OpenAI\" = \"#10A37F\",            # OpenAI Green\n  \"Anthropic\" = \"#D4A574\",         # Anthropic Gold\n  \"xAI\" = \"#000000\",               # Black\n  \"Meta\" = \"#0668E1\",              # Meta Blue\n  \"Alibaba\" = \"#FF6A00\",           # Alibaba Orange\n  \"Microsoft\" = \"#00A4EF\",         # Microsoft Blue\n  \"Amazon\" = \"#FF9900\",            # Amazon Orange\n  \"Other\" = \"#808080\"              # Gray\n)\n\n# Create the plot\nggplot() +\n  # Add vertical lines for GPU cluster operational dates (exclude \"Other\")\n  geom_vline(data = clusters,\n             aes(xintercept = as.numeric(date), color = owner_bucket),\n             linetype = \"dashed\", alpha = 0.6, linewidth = 0.5) +\n  # Add scatter points for models\n  geom_point(data = models,\n             aes(x = date, y = training_compute, color = owner_bucket),\n             alpha = 0.7, size = 2) +\n  # Apply color palette\n  scale_color_manual(values = color_palette, name = \"Organization\") +\n  # Log scale for y-axis (training compute)\n  scale_y_log10(labels = scales::scientific) +\n  # Facet by organization\n  facet_wrap(~ owner_bucket, ncol = 3, scales = \"free_x\") +\n  # Labels and theme\n  labs(\n    title = \"AI Model Releases and GPU Cluster Deployment Timeline by Organization\",\n    subtitle = \"Each panel shows models and GPU clusters\\nfor a single organization\",\n    x = \"Date\",\n    y = \"Training Compute (FLOPs, log scale)\",\n    caption = \"Vertical dashed lines represent GPU cluster first operational dates\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\", size = 10)\n  )\n\n\n\n\n\n\n\n\n\nIn this graph we visualize the date of model releases (dots) from different leading technology and AI companies. Overlayed as dashed lines, we plot the first operational date of GPU clusters from those same companies.\nWe would expect that model releases are delayed in comparison to GPU cluster opening dates since it would take significant computational resources to train a new model, usually several months of training for the largest models.\nHowever, we see varied trends across different companies. OpenAI, for example, had released many models before ever opening their first GPU cluster, while Google had many GPU clusters before releasing their first model.\nxAI (behind the Grok set of models) provides an interesting example of a clear cluster-to-model gap. The company seems to follow a very clear trend of having around a half-year delay between cluster opening and model release, with new models being released slightly before or slight after a new cluster becoming operational.\nOverall, the this plot reveals the strengths and weaknesses of different players in the AI ecosystem. Google, being a legacy tech company which has always required heavy computational resources, already had many compute centers before releasing their first model. Alibaba follows a similar trend. On the other hand, an AI-first company like OpenAI likely had to outsource their model training compute to other companies before having their own clusters.\n\n\n3.2.2 Cumulative Compute Over Time by Organization\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\n\n# Load the cumulative compute timeline data\ncumulative_data &lt;- read_csv(\"./Data/cumulative_compute_timeline.csv\")\n\n# Convert date to datetime and training_compute to numeric\ncumulative_data &lt;- cumulative_data |&gt;\n  mutate(date = as.Date(date))\n\n# Filter to only OpenAI\ncumulative_data &lt;- cumulative_data |&gt; filter(owner_bucket == \"OpenAI\")\n\n# Separate clusters and models\nclusters &lt;- cumulative_data |&gt; filter(type == \"cluster\")\n\n# For models, only keep those with known training_compute and get top 10 by training_compute\nmodels &lt;- cumulative_data |&gt;\n  filter(type == \"model\", !is.na(training_compute)) |&gt;\n  arrange(desc(training_compute)) |&gt;\n  head(10)\n\nprint(paste(\"Number of model release lines:\", nrow(models)))\n\n\n[1] \"Number of model release lines: 10\"\n\n\nCode\n# Create the plot\nggplot() +\n  # Add step lines for cumulative compute (clusters only)\n  geom_step(data = clusters,\n            aes(x = date, y = cumulative_compute),\n            linewidth = 1, alpha = 0.8, color = \"#10A37F\", direction = \"hv\") +\n  geom_point(data = clusters,\n             aes(x = date, y = cumulative_compute),\n             size = 3, alpha = 0.6, color = \"#10A37F\") +\n  # Add vertical dashed lines for model releases\n  geom_vline(data = models,\n             aes(xintercept = as.numeric(date)),\n             linetype = \"dashed\", alpha = 0.4, linewidth = 0.5, color = \"#10A37F\") +\n  # Log scale for y-axis\n  scale_y_log10(labels = scales::comma) +\n  # Labels and theme\n  labs(\n    title = \"OpenAI: Cumulative GPU Cluster Compute Over Time\",\n    subtitle = \"Line shows compute growth\\nDashed lines indicate model releases\",\n    x = \"Date\",\n    y = \"Cumulative Compute (H100 equivalents, log scale)\",\n    caption = \"Only existing clusters with known compute capacity included\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nWe can further understand the relationship between cluster development and model development by looking at similar graphs which instead plot compute capacity over time with model release dates overlaid. Particularly we are plotting cumulative compute capacity for major companies over time. We derive the cumulative compute available to companies over time as the sum of compute for their data centers. For these graph, we define cluster compute in terms of number of H100 equivalents (H100 being a popular GPU). We only plot notable companies with clusters to focus our results. Above is an example with OpenAI.\n\n\n3.2.3 Cumulative Compute Over Time by Organization (Faceted)\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\n\n# Load the cumulative compute timeline data\ncumulative_data &lt;- read_csv(\"./Data/cumulative_compute_timeline.csv\")\n\n# Convert date to datetime\ncumulative_data &lt;- cumulative_data |&gt;\n  mutate(date = as.Date(date))\n\n# Get companies that have clusters\ncompanies_with_clusters &lt;- cumulative_data |&gt;\n  filter(type == \"cluster\") |&gt;\n  pull(owner_bucket) |&gt;\n  unique()\n\n# Filter to only companies with clusters and exclude \"Other\"\ncumulative_data &lt;- cumulative_data |&gt;\n  filter(owner_bucket %in% companies_with_clusters, owner_bucket != \"Other\")\n\n# Separate clusters\nclusters &lt;- cumulative_data |&gt; filter(type == \"cluster\")\n\n# For models, get top 10 by training_compute for each organization\nmodels &lt;- cumulative_data |&gt;\n  filter(type == \"model\", !is.na(training_compute)) |&gt;\n  group_by(owner_bucket) |&gt;\n  arrange(desc(training_compute)) |&gt;\n  slice_head(n = 10) |&gt;\n  ungroup()\n\n# Define color palette (consistent with previous graphs)\ncolor_palette &lt;- c(\n  \"Google DeepMind\" = \"#4285F4\",\n  \"OpenAI\" = \"#10A37F\",\n  \"Anthropic\" = \"#D4A574\",\n  \"xAI\" = \"#000000\",\n  \"Meta\" = \"#0668E1\",\n  \"Alibaba\" = \"#FF6A00\",\n  \"Amazon\" = \"#FF9900\"\n)\n\n# Create the plot\nggplot() +\n  # Add step lines for cumulative compute (clusters only)\n  geom_step(data = clusters,\n            aes(x = date, y = cumulative_compute, color = owner_bucket),\n            linewidth = 1, alpha = 0.8, direction = \"hv\") +\n  geom_point(data = clusters,\n             aes(x = date, y = cumulative_compute, color = owner_bucket),\n             size = 2, alpha = 0.6) +\n  # Add vertical dashed lines for model releases\n  geom_vline(data = models,\n             aes(xintercept = as.numeric(date), color = owner_bucket),\n             linetype = \"dashed\", alpha = 0.4, linewidth = 0.3) +\n  # Facet by organization\n  facet_wrap(~ owner_bucket, ncol = 2, scales = \"free_y\") +\n  # Apply color palette\n  scale_color_manual(values = color_palette, name = \"Organization\") +\n  # Log scale for y-axis\n  scale_y_log10(labels = scales::comma) +\n  # Labels and theme\n  labs(\n    title = \"Cumulative GPU Cluster Compute Over Time by Organization\",\n    subtitle = \"Lines show compute growth\\nDashed lines indicate top 10 model releases by training compute\",\n    x = \"Date\",\n    y = \"Cumulative Compute (H100 equivalents, log scale)\",\n    caption = \"Only organizations with GPU clusters; top 10 models by training compute shown\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",  # Hide legend since facets show organizations\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\", size = 10),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nHere plot plot notable companies and the increase in their available compute over time. First note that to improve readability of the graph, we free the y-axis. However, it seems like major companies have increased their compute nearly exponentially. As consistent with the previous graphs, cluster opening time do not follow a predictable pattern in terms of model release, though, generally, it seems like different companies take different approaches in either having built infrastructure first or release models first.\n\n\n3.2.4 GPU Cluster Characteristics\nLet’s take a step back from looking at cluster compute for each company for a moment and take a peak at the most frequently listed countries from the tibble below. The tibble has a subset (top 5) of all the locations where the GPU clusters originated from. From there making the mosaic plot from other qualitative features such as how certain each cluster project was going to be rolled out, the sector each project was associated with, and whether or not the project was a single cluster was relatively easy to implement through dplyr.\n\n\nCode\nmosaic_data &lt;- gpu_clusters_filtered |&gt;\n  filter(Certainty %in% c(\"Confirmed\", \"Likely\"),\n         Country %in% top_countries) |&gt;\n  mutate(country_short = str_trunc(Country, width = 15, side = \"right\"))\nmosaic_data |&gt; group_by(Country) |&gt; count()\n\n\n# A tibble: 6 × 2\n# Groups:   Country [6]\n  Country                      n\n  &lt;chr&gt;                    &lt;int&gt;\n1 France                      12\n2 Germany                     10\n3 Japan                       23\n4 Korea (Republic of)         10\n5 United States of America   135\n6 &lt;NA&gt;                        57\n\n\nCode\nmosaic_data |&gt;\n  dplyr::select(Certainty, Country, Sector, `Single cluster?`) |&gt;\n  table() |&gt;\n  pairs(\n        space = .15,\n        lower_panel = pairs_mosaic(highlighting = 2, spacing = spacing_equal(0)),\n        upper_panel = pairs_mosaic(spacing = spacing_equal(0)),\n        diag_panel = pairs_barplot(\n          gp_vartext = gpar(fontsize = 8),\n          gp_leveltext = gpar(fontsize = 8),\n          abbreviate = 2),\n        main = \"Pairwise Associations: GPU Cluster Characteristics\",\n        main_gp = gpar(fontsize = 12)\n      )\n\n\n\n\n\n\n\n\n\nNote: The data from GPU clusters is more suited for quantitative rather than qualitative analysis. Because there were simply too many variables we could include here, we decided to isolate it to just four vars. to make it as clean as possible in the pair plots! Visually, color did not add anything meaningful and scaling the plot had massive issues for readability so we kept it as plain as possible. - We see sharp associations between certainty of the GPU cluster’s status and the particular sector the frontier model developer is involved with (big differences in the rectangle areas on the bottom right triangle). When the certainty was likely, we see that the pair-plot section becomes dominated for sectors. - In each variable, the US had a fairly dominant proportion of each subrectangle. This makes sense intuitively from what we observe about day-to-day developments in AI; however, this could also suggest that our dataset is imbalanced (oversampling of US based companies).\n\n\n3.2.5 GPU Cluster Cost\n\n\nCode\nggplot(plot_data, aes(x = total_calculated_cost, y = owner_short)) +\n  geom_point(size=2, color='blue') +\n  facet_grid(Sector ~ ., scales = \"free_y\", space = \"free_y\") +\n  ggtitle(paste('Top', top_n, \"Total Calculated Cost in GPU Cluster Projects \\nGrouped By Owner\")) +\n  xlab(\"Total Calculated Cost (in billions of USD)\") +\n  ylab(\"\") +\n  theme_linedraw() +\n  theme(\n    axis.text.y = element_text(size = 7),  # smaller text\n    strip.text.y = element_text(size = 8, margin = margin(2, 2, 2, 2)),\n    legend.position = \"none\"\n  )",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#dataset-size-over-time",
    "href": "results.html#dataset-size-over-time",
    "title": "3  Results",
    "section": "3.3 Dataset Size Over Time",
    "text": "3.3 Dataset Size Over Time\nClearly, overall compute available to train model has been increasing at an exponential rate. However has this increase in compute been a proactive move to be prepared to train data-intensive models? Or has this been a reactive move in response to models that turned out to be more data intensive than expected?\nTo answer this question, we first need to look at trends in dataset size and model size.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(scales)\n\n# Load the notable AI models dataset\nmodels_data &lt;- read_csv(\"./Data/ai_models_dataset/notable_ai_models.csv\")\n\n# Filter to models with known dataset size and publication date\ndataset_plot_data &lt;- models_data |&gt;\n  filter(!is.na(`Training dataset size (gradients)`),\n         !is.na(`Publication date`)) |&gt;\n  mutate(`Publication date` = as.Date(`Publication date`),\n         `Training dataset size (gradients)` = as.numeric(`Training dataset size (gradients)`))\n\n# Create scatter plot\nggplot(dataset_plot_data, aes(x = `Publication date`, y = `Training dataset size (gradients)`)) +\n  geom_point(alpha = 0.6, size = 2, color = \"#4285F4\") +\n  # Log scale for y-axis\n  scale_y_log10(labels = comma) +\n  # Labels and theme\n  labs(\n    title = \"AI Model Dataset Size Over Time\",\n    subtitle = \"Training dataset size has grown exponentially\",\n    x = \"Publication Date\",\n    y = \"Training Dataset Size (gradients, log scale)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nThis graph simply plots dataset size over time. We adjust the y-axis to be on a log scale, meaning that seemingly linear increases actually correspond to exponential increases in non-log scales. At first it is hard to discern any notable trends other than a clear increase in recent year of dataset size.\n\n3.3.1 Dataset Size Over Time with Key Models\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(scales)\n\n# Load the notable AI models dataset\nmodels_data &lt;- read_csv(\"./Data/ai_models_dataset/notable_ai_models.csv\")\n\n# Filter to models with known dataset size and publication date\ndataset_plot_data &lt;- models_data |&gt;\n  filter(!is.na(`Training dataset size (gradients)`),\n         !is.na(`Publication date`)) |&gt;\n  mutate(`Publication date` = as.Date(`Publication date`),\n         `Training dataset size (gradients)` = as.numeric(`Training dataset size (gradients)`))\n\n# Define key model release dates\nalexnet_date &lt;- as.Date(\"2012-09-29\")\ntransformer_date &lt;- as.Date(\"2017-06-11\")\n\n# Create scatter plot with vertical lines for key models\nggplot(dataset_plot_data, aes(x = `Publication date`, y = `Training dataset size (gradients)`)) +\n  # Add vertical lines for key models\n  geom_vline(xintercept = as.numeric(alexnet_date),\n             linetype = \"dashed\", color = \"#EA4335\", linewidth = 0.8) +\n  geom_vline(xintercept = as.numeric(transformer_date),\n             linetype = \"dashed\", color = \"#34A853\", linewidth = 0.8) +\n  # Add scatter points\n  geom_point(alpha = 0.6, size = 2, color = \"#4285F4\") +\n  # Log scale for y-axis\n  scale_y_log10(labels = comma) +\n  # Labels and theme\n  labs(\n    title = \"AI Model Dataset Size Over Time\",\n    subtitle = \"Training dataset size has grown exponentially.\\nDashed lines show AlexNet (red) and Transformer (green) releases.\",\n    x = \"Publication Date\",\n    y = \"Training Dataset Size (gradients, log scale)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nHowever, much of the demand for more data has been driven by new machine learning approaches. First, the advent of deep learning create demand for larger datasets. After, the advent of the transformer architecture allowed feasible training on even larger datasets (partly why large language models are considered “large”). As such, we add two dashed lines, the first being the landmark AlexNet paper, which ushered in a new era of deep learning critically, thorough the use of GPU training. The green dashed line represent the release of the landmark Attention Is All You Need paper, which introduced the transformer architecture that underlies large language models today. Adding these separators, we begin to more clearly see trends in the data. Notably, after the introduction of the transformer, we see a sharp increase in dataset size.\n\n\n3.3.2 Dataset Size Growth Rate Before and After Transformer\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(scales)\n\n# Load the notable AI models dataset\nmodels_data &lt;- read_csv(\"./Data/ai_models_dataset/notable_ai_models.csv\")\n\n# Filter to models with known dataset size and publication date\ndataset_plot_data &lt;- models_data |&gt;\n  filter(!is.na(`Training dataset size (gradients)`),\n         !is.na(`Publication date`)) |&gt;\n  mutate(`Publication date` = as.Date(`Publication date`),\n         `Training dataset size (gradients)` = as.numeric(`Training dataset size (gradients)`))\n\n# Define key model release dates\nalexnet_date &lt;- as.Date(\"2012-09-29\")\ntransformer_date &lt;- as.Date(\"2017-06-11\")\n\n# Split data into pre-Transformer and post-Transformer periods\npre_transformer &lt;- dataset_plot_data |&gt; filter(`Publication date` &lt; transformer_date)\npost_transformer &lt;- dataset_plot_data |&gt; filter(`Publication date` &gt;= transformer_date)\n\n# Create scatter plot with two linear regression lines\nggplot(dataset_plot_data, aes(x = `Publication date`, y = `Training dataset size (gradients)`)) +\n  # Add vertical lines for key models\n  geom_vline(xintercept = as.numeric(alexnet_date),\n             linetype = \"dashed\", color = \"#EA4335\", linewidth = 0.8, alpha = 0.5) +\n  geom_vline(xintercept = as.numeric(transformer_date),\n             linetype = \"dashed\", color = \"#34A853\", linewidth = 0.8) +\n  # Add scatter points\n  geom_point(alpha = 0.6, size = 2, color = \"#4285F4\") +\n  # Add linear regression for pre-Transformer period\n  geom_smooth(data = pre_transformer,\n              method = \"lm\",\n              se = TRUE,\n              color = \"#FBBC04\",\n              fill = \"#FBBC04\",\n              alpha = 0.2,\n              linewidth = 1) +\n  # Add linear regression for post-Transformer period\n  geom_smooth(data = post_transformer,\n              method = \"lm\",\n              se = TRUE,\n              color = \"#34A853\",\n              fill = \"#34A853\",\n              alpha = 0.2,\n              linewidth = 1) +\n  # Log scale for y-axis\n  scale_y_log10(labels = comma) +\n  # Labels and theme\n  labs(\n    title = \"Dataset Size Growth Rate Before and After Transformer\",\n    subtitle = \"Yellow line: pre-Transformer growth.\\nGreen line: post-Transformer growth.\\nShaded areas show confidence intervals.\",\n    x = \"Publication Date\",\n    y = \"Training Dataset Size (gradients, log scale)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nTo further quantify the difference between the pre-transformer era and the post-transformer era of AI, we add regression lines of our data before the transformer and after the transformer. We already see an exponential increase in dataset size before the transformer, as denoted by the yellow line. However, after the transformer, this increase becomes much sharper, as shown by the green line.\nBy all accounts, dataset used to train ML models is experiencing lightning fast growth.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#model-parameters-over-time",
    "href": "results.html#model-parameters-over-time",
    "title": "3  Results",
    "section": "3.4 Model Parameters Over Time",
    "text": "3.4 Model Parameters Over Time\nDataset size is only one aspect that increases demand of compute, but the size of the model being trained is another factor that influences how much compute is required. We create a similar graph to those above, but this time plotting model size (number of trainable parameters).\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(scales)\n\n# Load the notable AI models dataset\nmodels_data &lt;- read_csv(\"./Data/ai_models_dataset/notable_ai_models.csv\")\n\n# Filter to models with known parameters and publication date\nparameters_plot_data &lt;- models_data |&gt;\n  filter(!is.na(Parameters),\n         !is.na(`Publication date`)) |&gt;\n  mutate(`Publication date` = as.Date(`Publication date`),\n         Parameters = as.numeric(Parameters))\n\n# Define key model release dates\nalexnet_date &lt;- as.Date(\"2012-09-29\")\ntransformer_date &lt;- as.Date(\"2017-06-11\")\n\n# Split data into pre-Transformer and post-Transformer periods\npre_transformer &lt;- parameters_plot_data |&gt; filter(`Publication date` &lt; transformer_date)\npost_transformer &lt;- parameters_plot_data |&gt; filter(`Publication date` &gt;= transformer_date)\n\n# Create scatter plot with two linear regression lines\nggplot(parameters_plot_data, aes(x = `Publication date`, y = Parameters)) +\n  # Add vertical lines for key models\n  geom_vline(xintercept = as.numeric(alexnet_date),\n             linetype = \"dashed\", color = \"#EA4335\", linewidth = 0.8, alpha = 0.5) +\n  geom_vline(xintercept = as.numeric(transformer_date),\n             linetype = \"dashed\", color = \"#34A853\", linewidth = 0.8) +\n  # Add scatter points\n  geom_point(alpha = 0.6, size = 2, color = \"#4285F4\") +\n  # Add linear regression for pre-Transformer period\n  geom_smooth(data = pre_transformer,\n              method = \"lm\",\n              se = TRUE,\n              color = \"#FBBC04\",\n              fill = \"#FBBC04\",\n              alpha = 0.2,\n              linewidth = 1) +\n  # Add linear regression for post-Transformer period\n  geom_smooth(data = post_transformer,\n              method = \"lm\",\n              se = TRUE,\n              color = \"#34A853\",\n              fill = \"#34A853\",\n              alpha = 0.2,\n              linewidth = 1) +\n  # Log scale for y-axis\n  scale_y_log10(labels = comma) +\n  # Labels and theme\n  labs(\n    title = \"Model Parameters Growth Rate Before and After Transformer\",\n    subtitle = \"Yellow line: pre-Transformer growth.\\nGreen line: post-Transformer growth. Shaded areas show confidence intervals.\",\n    x = \"Publication Date\",\n    y = \"Parameters (log scale)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nIn this plot we see a trend that follows almost exactly the same trend as the trends in dataset size. Until the the transformer, parameter count is increase rapidly (exponentially, in fact) already, but once the transformer is introduced we start to see an even more rapid increase.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#training-time-analysis",
    "href": "results.html#training-time-analysis",
    "title": "3  Results",
    "section": "3.5 Training Time Analysis",
    "text": "3.5 Training Time Analysis\nSo overall, we see both an exponential increases in the demand of compute (increases in dataset size and model size) and exponential increases in the supply of compute (GPU cluster computer availability). Naturally, we can wonder whether the supply of compute is keeping pace with the demand, or is the demand putting strain on the supply.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(scales)\n\n# Load the training time results\ntraining_times &lt;- read_csv(\"./Data/gpu_cluster_training_times.csv\")\n\n# Convert date to datetime\ntraining_times &lt;- training_times |&gt;\n  mutate(`First Operational Date` = as.Date(`First Operational Date`))\n\n# Create scatter plot\nggplot(training_times, aes(x = `First Operational Date`, y = `Training time (hours)`)) +\n  geom_point(alpha = 0.7, size = 3, color = \"#4285F4\") +\n  # Add a trend line\n  geom_smooth(method = \"loess\", se = TRUE, color = \"#D32F2F\", alpha = 0.2) +\n  # Log scale for y-axis to better show the range\n  scale_y_log10(labels = comma) +\n  # Format x-axis dates\n  scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\") +\n  labs(\n    title = \"Average Training Time for Top 10 Frontier Models\\non Rank-1 GPU Clusters\",\n    x = \"GPU Cluster First Operational Date\",\n    y = \"Average Training Time (hours, log scale)\",\n    caption = \"Based on top 10 frontier models by training compute released before each cluster's operational date\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nHere we plot the average training time in hours for the top 10 best models at the time of the first operational date of the best GPU cluster at the time of its opening. Simply put, we graph the training time of the best models on the best GPU cluster of the time. We see a really interesting trend here. If compute supply where to keep pace with compute demand, we would expect there to be no change in average training time. Instead, we see that in that in the early 2020s there is an exponential increase in the training time of best models (even using the best GPU cluster). However, nearing 2024 and 2025, we see that compute availability starts to catch up.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#benchmarks",
    "href": "results.html#benchmarks",
    "title": "3  Results",
    "section": "3.6 Benchmarks",
    "text": "3.6 Benchmarks\n\n\nCode\n# names(df_num_imputed)\n# constant_cols &lt;- sapply(df_num_imputed, function(x) length(unique(x[!is.na(x)])) == 1)\n# names(df_num_imputed)[constant_cols]\np1 &lt;- draw_biplot(\n  df_num_imputed,\n  key_axis = \"Best score (across scorers)\",\n  fix_sign = TRUE,\n  point_color = \"grey20\",\n  point_labels = TRUE,\n  arrows = FALSE,\n  mult = 5\n)\n\np2 &lt;- draw_biplot(\n  df_num_imputed,\n  fix_sign = TRUE,\n  point_labels = FALSE,\n  point_color = \"grey20\",\n  arrows = TRUE,\n  mult = 5\n)\n\n# Side by side\np1 + p2 + \n  plot_annotation(\n    title = \"Frontier Model Benchmarks (PCA biplots)\",\n    subtitle = \"Best Scoring Model Indicators\"\n  )\n\n\n\n\n\n\n\n\n\nCode\n# install.packages('ggfortify')\n# library(ggfortify)\n# p &lt;- prcomp(df_num_imputed_clean, scale = TRUE)\n# \n# autoplot(\n#   p,\n#   loadings = TRUE, # arrows\n#   loadings.label = TRUE, # text\n#   loadings.colour = 'deepskyblue3',\n#   loadings.label.colour = 'deepskyblue3',\n#   loadings.label.repel = TRUE # overlap\n# )\n\n# scores &lt;- pca$x[,1:2]\n# k &lt;- kmeans(scores, centers = 6)\n# scores &lt;- data.frame(scores) |&gt;\n#   mutate(cluster = factor(k$cluster), country = ratings$country)\n# g4 &lt;- ggplot(scores, aes(PC1, PC2, color = cluster, label = country)) +\n#   geom_point() +\n#   geom_text(nudge_y = .2) +\n#   guides(color=\"none\")\n# g4\n\npca &lt;- prcomp(df_num_pca, scale. = TRUE)\nsummary(pca)\n\n\nImportance of components:\n                          PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     2.5312 1.8018 1.6492 1.3765 0.87135 0.76408 0.53317\nProportion of Variance 0.4004 0.2029 0.1700 0.1184 0.04745 0.03649 0.01777\nCumulative Proportion  0.4004 0.6034 0.7733 0.8918 0.93922 0.97571 0.99347\n                           PC8       PC9     PC10      PC11      PC12      PC13\nStandard deviation     0.32312 2.058e-15 2.83e-16 1.215e-16 8.629e-17 6.105e-17\nProportion of Variance 0.00653 0.000e+00 0.00e+00 0.000e+00 0.000e+00 0.000e+00\nCumulative Proportion  1.00000 1.000e+00 1.00e+00 1.000e+00 1.000e+00 1.000e+00\n                            PC14     PC15      PC16\nStandard deviation     3.786e-17 3.09e-17 5.978e-33\nProportion of Variance 0.000e+00 0.00e+00 0.000e+00\nCumulative Proportion  1.000e+00 1.00e+00 1.000e+00\n\n\n\n\nCode\nprint('Sanity check with total contribution to PC1 & PC2')\n\n\n[1] \"Sanity check with total contribution to PC1 & PC2\"\n\n\nCode\nloadings_12 &lt;- pca$rotation[, 1:2]\ntotal_contribution &lt;- sqrt(loadings_12[,1]^2 + loadings_12[,2]^2)\nsort(total_contribution, decreasing = TRUE)\n\n\n                   Training compute (FLOP)_math \n                                     0.49605964 \n               Training compute cost (2023 USD) \n                                     0.46668279 \n                Training compute cost (upfront) \n                                     0.46501992 \n                              Hardware quantity \n                                     0.46501992 \n                  Training compute (FLOP)_model \n                                     0.44961325 \n                                  Hardware FP16 \n                                     0.39030088 \n                                  Hardware FP32 \n                                     0.39030088 \n                                  Hardware TF16 \n                                     0.39002152 \n          Assumed hardware performance (FLOP/s) \n                                     0.39002152 \n                                  Power per GPU \n                                     0.37974732 \n                                         stderr \n                                     0.24851261 \n(DEPRECATED) Training dataset size (datapoints) \n                                     0.16681305 \n                                     Parameters \n                                     0.16681305 \n                   Training compute lower bound \n                                     0.12107563 \n                                   Hardware age \n                                     0.11106774 \n                                 Hardware count \n                                     0.06304506",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#gpu-cluster-geographic-distribution",
    "href": "results.html#gpu-cluster-geographic-distribution",
    "title": "3  Results",
    "section": "3.7 GPU Cluster Geographic Distribution",
    "text": "3.7 GPU Cluster Geographic Distribution\nClearly, AI developers are prioritizing increasing their compute capacity as evident by the exponential increase in compute capacity that every major AI-developing company has experienced. Naturally, the construction and operation of these GPU clusters will have huge effects in the communities they are built, their governance will be influenced by their location, and localities with more compute capacity will have more leverage on growing tech and companies. To explore this further, we map the number of GPU clusters per country.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(maps)\nlibrary(viridis)\nlibrary(RColorBrewer)\nlibrary(classInt)\n\n# Load GPU clusters dataset\ngpu_clusters &lt;- read_csv(\"./Data/gpu_clusters_dataset/gpu_clusters.csv\")\n\n# Get world map data\nworld_map &lt;- map_data(\"world\")\n\n# Count existing clusters by country\nexisting_clusters &lt;- gpu_clusters |&gt;\n  filter(Status == \"Existing\") |&gt;\n  group_by(Country) |&gt;\n  summarise(count = n()) |&gt;\n  rename(region = Country)\n\n# Count planned clusters by country\nplanned_clusters &lt;- gpu_clusters |&gt;\n  filter(Status == \"Planned\") |&gt;\n  group_by(Country) |&gt;\n  summarise(count = n()) |&gt;\n  rename(region = Country)\n\n# Standardize country names to match map_data\nstandardize_country_names &lt;- function(df) {\n  df |&gt;\n    mutate(region = case_when(\n      region == \"United States of America\" ~ \"USA\",\n      region == \"United Kingdom of Great Britain and Northern Ireland\" ~ \"UK\",\n      region == \"Russian Federation\" ~ \"Russia\",\n      region == \"Republic of Korea\" ~ \"South Korea\",\n      region == \"Viet Nam\" ~ \"Vietnam\",\n      region == \"United Arab Emirates\" ~ \"UAE\",\n      TRUE ~ region\n    ))\n}\n\nexisting_clusters &lt;- standardize_country_names(existing_clusters)\nplanned_clusters &lt;- standardize_country_names(planned_clusters)\n\n# Join with world map\nworld_existing &lt;- world_map |&gt;\n  left_join(existing_clusters, by = \"region\")\n\nworld_planned &lt;- world_map |&gt;\n  left_join(planned_clusters, by = \"region\")\n\n# Calculate Jenks natural breaks for existing clusters\nexisting_breaks &lt;- classIntervals(existing_clusters$count, n = 5, style = \"jenks\")\nexisting_clusters &lt;- existing_clusters |&gt;\n  mutate(count_binned = cut(count, breaks = existing_breaks$brks, include.lowest = TRUE, dig.lab = 10))\n\nworld_existing &lt;- world_map |&gt;\n  left_join(existing_clusters, by = \"region\")\n\n# Calculate Jenks natural breaks for planned clusters\nplanned_breaks &lt;- classIntervals(planned_clusters$count, n = 5, style = \"jenks\")\nplanned_clusters &lt;- planned_clusters |&gt;\n  mutate(count_binned = cut(count, breaks = planned_breaks$brks, include.lowest = TRUE, dig.lab = 10))\n\nworld_planned &lt;- world_map |&gt;\n  left_join(planned_clusters, by = \"region\")\n\n# Map 1: Existing GPU Clusters\nmap_existing &lt;- ggplot(world_existing, aes(x = long, y = lat, group = group, fill = count_binned)) +\n  geom_polygon(color = \"white\", linewidth = 0.1) +\n  scale_fill_brewer(palette = \"Blues\", na.value = \"gray90\", name = \"Count\") +\n  labs(\n    title = \"Existing GPU Clusters by Country\",\n    subtitle = \"Geographic distribution of operational GPU clusters\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank()\n  ) +\n  coord_fixed(1.3)\n\n# Map 2: Planned GPU Clusters\nmap_planned &lt;- ggplot(world_planned, aes(x = long, y = lat, group = group, fill = count_binned)) +\n  geom_polygon(color = \"white\", linewidth = 0.1) +\n  scale_fill_brewer(palette = \"Blues\", na.value = \"gray90\", name = \"Count\") +\n  labs(\n    title = \"Planned GPU Clusters by Country\",\n    subtitle = \"Geographic distribution of planned GPU clusters\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank()\n  ) +\n  coord_fixed(1.3)\n\n# Map 3: Combined (Existing + Planned Total)\ncombined_clusters &lt;- gpu_clusters |&gt;\n  filter(Status %in% c(\"Existing\", \"Planned\")) |&gt;\n  group_by(Country) |&gt;\n  summarise(count = n(), .groups = \"drop\") |&gt;\n  rename(region = Country)\n\ncombined_clusters &lt;- standardize_country_names(combined_clusters)\n\n# Calculate Jenks natural breaks for combined clusters\ncombined_breaks &lt;- classIntervals(combined_clusters$count, n = 5, style = \"jenks\")\ncombined_clusters &lt;- combined_clusters |&gt;\n  mutate(count_binned = cut(count, breaks = combined_breaks$brks, include.lowest = TRUE, dig.lab = 10))\n\nworld_combined &lt;- world_map |&gt;\n  left_join(combined_clusters, by = \"region\")\n\nmap_combined &lt;- ggplot(world_combined, aes(x = long, y = lat, group = group, fill = count_binned)) +\n  geom_polygon(color = \"white\", linewidth = 0.1) +\n  scale_fill_brewer(palette = \"Blues\", na.value = \"gray90\", name = \"Count\") +\n  labs(\n    title = \"Total GPU Clusters by Country (Existing + Planned)\",\n    subtitle = \"Combined geographic distribution of all GPU clusters\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank()\n  ) +\n  coord_fixed(1.3)\n\n# Display the maps\nmap_existing\n\n\n\n\n\n\n\n\n\nFirst we map the number of GPU clusters already existing globally. We can see that the two major players in compute clusters are the United States and China, with some development in Europe, and even lighter development in Latin America, the Middle East, South Asia, and Oceania.\n\n\nCode\nmap_planned\n\n\n\n\n\n\n\n\n\nTo understand the future of GPU cluster allocation, we also map the number of planned GPU clusters in each country. Here we see similar trends to the previous map: the United States and China lead the way in terms of GPU cluster development, while the rest of the world lags behind. Outside of the top two countries, we do see France maintaining their strength in compute within Europe, Brazil maintaining its strength in Latin America, and Saudi Arabia maintaining its strength in the middle east.\n\n\nCode\nmap_combined\n\n\n\n\n\n\n\n\n\nCombining existing and planned GPU cluster maps, we see that the countries that have previously benefitted from AI development and GPU cluster development (namely the US and China) will continue to be the countries that benefit from further development, while most of the rest of the world sees lagging development.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "4  Conclusion",
    "section": "",
    "text": "4.0.1 Takeaways\nOur analysis of GPU clusters and AI model development reveals several key insights:\nCompute Infrastructure and Model Development:\n\nDifferent AI companies follow distinct strategies for GPU cluster deployment versus model releases. Legacy tech companies like Google and Alibaba built substantial compute infrastructure before releasing models, while AI-first companies like OpenAI released many models before establishing their own GPU clusters (likely relying on cloud providers initially).\nxAI demonstrates a clear pattern of approximately half-year delays between cluster opening and model releases, showing a systematic approach to infrastructure-first development.\n\nCumulative Compute Growth:\n\nMajor AI companies have increased their available compute capacity nearly exponentially, demonstrating the prioritization of infrastructure investment across the industry.\nCluster opening times do not follow a predictable pattern relative to model releases, suggesting companies take varied approaches to balancing infrastructure development and model deployment.\n\nExponential Growth in Compute Demand:\n\nBoth dataset size and model parameters have grown exponentially over time, with this growth dramatically accelerating after the introduction of the Transformer architecture in 2017.\n\nSupply vs. Demand Dynamics:\n\nTraining time analysis reveals that compute supply initially struggled to keep pace with demand—in the early 2020s, there was an exponential increase in training time for the best models even when using the best available GPU clusters.\nHowever, nearing 2024 and 2025, compute availability has started to catch up with demand, suggesting that infrastructure investments are beginning to match the computational requirements of frontier models.\n\nGeographic Concentration:\n\nGPU cluster development is highly concentrated in the United States and China, with these two countries dominating both existing and planned infrastructure.\nThis geographic concentration extends to future development—countries that have previously benefitted from AI infrastructure will continue to benefit, while most of the rest of the world sees lagging development.\nWithin other regions, certain countries maintain regional leadership: France in Europe, Brazil in Latin America, and Saudi Arabia in the Middle East.\n\n\n\n4.0.2 Limitations\nOur exploration is limited by the lack of data on certain topics. For example, the data on company compute spend was lacking, with less than 10 rows of data to work with. Other datasets may have had more rows, but had many columns with large percentages of missing data, which limited any analysis on those columns.\n\n\n4.0.3 Future Directions\nIn the future, we would like to see further work in two particular regions:\n\nMore in-depth data on compute spend (as mentioned above, data is lacking). It would be particularly interesting to have a more in-depth breakdown of trends in R&D investment vs. inference spend, and how that data has changed over time. This work stream will largely be dependent on AI companies being open with some of their data, however.\nDecomposing the different usages of GPU clusters. Clusters can be used for model training, inference, and a variety of other compute tasks, so it would be interesting to see trends on how compute usage has changed over time. For example, we may expect a slowdown in AI progress if a larger share of compute goes to inference.\n\n\n\n4.0.4 Lessons Learned\nThe main lesson learned with this project was to look into data beyond the surface level. Some of our personal favorite graph/insights came from asking more of our data. For example, for the dataset and model size scatter plots, the scatterplots themselves were simple, but once we add in historical data (AlexNet, transformer), we see a more interesting trend/insight. Looking at our graph of average training time for top 10 models with the rank-1 GPU clusters, we ask a multifaceted question, had to do a lot of pre-processing to isolate the top 10 models at the time of a cluster opening and to get the best cluster, but we ended up finding a very interesting trend.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  }
]