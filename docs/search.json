[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI Pulse: Exploring the Drivers of AI Development",
    "section": "",
    "text": "1 Introduction\nFrom it’s humble beginnings in the 1950s, and throught a cycle of dark and golder ages, AI finally exploded in the early 2020s with the introduction of large language models. Studying the history of AI development, we see breakthoughs resulting for innovations in various front. AlexNet in 2012 ushered in a new era of deep learning fueled by the availability of large datasets and shift towards GPU training. Five years later, in 2017, an algorithmic innovation, attenion and the transformer architecture, ushered in yet another era of AI, namely the era of large language models which is still experiencing rapid growth today.\nWhether it be hardware infrastructure, data quantity and quality, or algorithmic innovations, each of these factors has played an important role in the development of new AI techniques or as bottlenecks to said breakthroughs. In this project we aim to explore how these factors have changed over time in order to have an in-depth understanding of the driving forces behind AI development. With this understanding, we hope to be better able to understand where AI may be heading in the future.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data",
    "section": "",
    "text": "2.1 Description\nOur data is collected by an non-profit organization called Epoch AI. From their (website)[https://epoch.ai/about], Epoch AI’s mission is:\nIn the past, Epoch AI’s work has been cited in Our World in Data, Time Magazine, by Google CEO Sundar Pichai, and other reputable sources.\nGenerally, we frame our work in terms of inputs factors and output results and further subdivide this into input factors for AI models, input factors for AI companies, output results for AI models, and output results for AI companies:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "2.2 Missing value analysis",
    "text": "2.2 Missing value analysis",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#description",
    "href": "data.html#description",
    "title": "2  Data",
    "section": "",
    "text": "Epoch AI is a multidisciplinary non-profit research institute investigating the future of artificial intelligence. We examine the driving forces behind AI and forecast its economic and societal impact. We emphasize making our research accessible through our reports, models and visualizations to help ground the discussion of AI on a solid empirical footing. Our goal is to create a healthy scientific environment, where claims about AI are discussed with the rigor they merit.\n\n\n\n\nAI Models:\n\nInputs:\n\nGPU Clusters\nCompute Spend\nHardware advacements\n\nOutpus:\n\nPerformance on model benchmarks\n\n\nAI Companies:\n\nInputs:\n\nStaffing\nFunding\n\nOutputs:\n\nUsage of AI tools\nRevenue\n\n\n\n\n2.1.1 Collection Methods\nEpoch AI collects data on variety of different components related to AI progress including hardware advancements, GPU cluster advanacements, model releases, AI company funding, etc. For the most part, Epoch uses a mix of automated collections methods (for example, automatic updates of AI benchmark scores) and manual data collection from public sources of information (company blog posts, new posts, public satellite imagery). Here we link to the full set of data collection methodologies:\n\nML Hardware Data\nGPU Cluster Data\nAI Company Data\nAI Model Data\nAI Benchmark Data\n\n\n\n2.1.2 Datasets Overview\n\n\n\n\n\n\n\n\n\n\nTitle\nDescription\nFrequency of Updates\nDimensions\nNotes\n\n\n\n\nML Hardware\nDataset listing hardward used to train ML models including GPUs, TPUs, and other less common hardware types\nDaily\n170 rows, 38 columns\n\n\n\nGPU Clusters\nDataset listing planned, existing, and previous GPU clusters\nDaily\n786 rows, 55 columns\nDaily refreshes, but data may not necesarily be updated daily.\n\n\nAI Companies\nVarious different datasets pertaining to AI company revenue, compute spend, staffing, funding rounds, user usage, and general information\nDaily\nVarious datasets\n\n\n\nAI Models\nDataset listing AI models over the years (including primitive models)\nDaily\n3000+ rows, 56 columns\nDivided into “Frontier”, “Notable”, and “Large Scale” models\n\n\nAI Benchmark Data\nA collection of different datasets of ML model performance on different benchmarks\nnot found\nVarious datasets\n\n\n\n\nPlease see links above for finer details on the datasets.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#other-problems-and-limitations-of-the-data",
    "href": "data.html#other-problems-and-limitations-of-the-data",
    "title": "2  Data",
    "section": "2.3 Other problems and limitations of the data",
    "text": "2.3 Other problems and limitations of the data\n\ninconsistent location format in the GPU cluster dataset",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "3  Results",
    "section": "",
    "text": "3.1 Compute Spend\nData loading:\nCode\ng1 &lt;- ggplot(usage_reports, aes(Date, `Active users`/ 1e9)) +\n  # geom_line(color = \"grey30\") +\n  geom_point(size = 1) +\n  geom_smooth(method = \"loess\", span = 0.5, se = FALSE) +\n  labs(title = \"Loess Curve of OpenAI's Monthly Active Users Over Time\",\n       x = \"Date\",\n       y = \"Number of Users (in billions)\") +\n  theme_minimal()\n\ng1\n\n\n\n\n\n\n\n\n\nCode\n# ggplot() +\n#   geom_col(data = openai_stacked, aes(x = Date, y = Amount, fill = Category), position = \"stack\") +\n#   geom_point(data = usage_reports, aes(x = Date, y = `Active users`), size = 1, color = \"black\") +\n#   geom_smooth(data = usage_reports, aes(x = Date, y = `Active users`), method = \"loess\", span = 0.5, se = FALSE) +\n#   labs(title = \"OpenAI's Inference Spending overlayed by Monthly Active Users\",\n#        x = \"Date\",\n#        y = \"Dollars Spent (stacked bars) / Users (loess curve)\") +\n#   theme_minimal()\n(a). Examine the overall amount spent on compute by foundation model developers over time. Filter by compute expenditure category.\nCode\nggplot(compute_stacked, aes(x = Year, y = Amount, fill = Category)) +\n  geom_col(position = \"stack\") +  # Stacked bars\n  facet_wrap(~Company) +\n  labs(title = \"Breakdown of Total Compute Spend Over Time\", \n       y = \"Amount of Dollars Spent\",\n       fill = \"Expenditure Type\") +\n  theme_minimal() + \n  theme(\n    strip.text = element_text(),\n    strip.background = element_rect(fill = \"gray\", color = \"lightgray\"),\n  )\nCode\ng2 &lt;- ggplot() +\n  geom_col(data = openai_stacked, aes(x = Date, y = Amount / 1e9, fill = Category), position = \"stack\") +\n  geom_point(data = usage_reports, aes(x = Date, y = `Active users` / 1e9), size = 1, color = \"black\") +\n  geom_smooth(data = usage_reports, aes(x = Date, y = `Active users` / 1e9), method = \"loess\", span = 0.5, se = FALSE) +\n  labs(title = \"OpenAI's Inference Spending overlayed by Monthly Active Users (in billions)\",\n       x = \"Date\",\n       y = \"Dollars Spent (bars) / Users (loess curve)\") +\n  theme_minimal() + \n  theme(legend.position = \"none\")\n \ng2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#gpu-clusters",
    "href": "results.html#gpu-clusters",
    "title": "3  Results",
    "section": "3.2 GPU Clusters",
    "text": "3.2 GPU Clusters\n\n3.2.1 GPU Clusters and Model Development Timeline\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\n\n# Load the combined dataset\ncombined_data &lt;- read_csv(\"./Data/combined_gpu_models.csv\")\n\n# Create owner buckets\nbucket_owners &lt;- function(owner) {\n  case_when(\n    grepl(\"Google DeepMind\", owner, ignore.case = TRUE) ~ \"Google DeepMind\",\n    grepl(\"OpenAI\", owner, ignore.case = TRUE) ~ \"OpenAI\",\n    grepl(\"Anthropic\", owner, ignore.case = TRUE) ~ \"Anthropic\",\n    grepl(\"xAI\", owner, ignore.case = TRUE) ~ \"xAI\",\n    grepl(\"Meta\", owner, ignore.case = TRUE) ~ \"Meta\",\n    grepl(\"Alibaba\", owner, ignore.case = TRUE) ~ \"Alibaba\",\n    grepl(\"Microsoft\", owner, ignore.case = TRUE) ~ \"Microsoft\",\n    grepl(\"Amazon\", owner, ignore.case = TRUE) ~ \"Amazon\",\n    TRUE ~ \"Other\"\n  )\n}\n\n# Apply bucketing\ncombined_data &lt;- combined_data |&gt;\n  mutate(owner_bucket = bucket_owners(owner))\n\n# Separate models and clusters\nmodels &lt;- combined_data |&gt; filter(type == \"model\")\nclusters &lt;- combined_data |&gt;\n  filter(type == \"cluster\", owner_bucket != \"Other\")  # Exclude \"Other\" clusters for lines\n\n# Find the date of the first GPU cluster\nfirst_cluster_date &lt;- min(combined_data$date[combined_data$type == \"cluster\"], na.rm = TRUE)\n\n# Filter both models and clusters to only include data after first cluster\nmodels &lt;- models |&gt; filter(date &gt;= first_cluster_date)\nclusters &lt;- clusters |&gt; filter(date &gt;= first_cluster_date)\n\n# Define color palette for the buckets\ncolor_palette &lt;- c(\n  \"Google DeepMind\" = \"#4285F4\",  # Google Blue\n  \"OpenAI\" = \"#10A37F\",            # OpenAI Green\n  \"Anthropic\" = \"#D4A574\",         # Anthropic Gold\n  \"xAI\" = \"#000000\",               # Black\n  \"Meta\" = \"#0668E1\",              # Meta Blue\n  \"Alibaba\" = \"#FF6A00\",           # Alibaba Orange\n  \"Microsoft\" = \"#00A4EF\",         # Microsoft Blue\n  \"Amazon\" = \"#FF9900\",            # Amazon Orange\n  \"Other\" = \"#808080\"              # Gray\n)\n\n# Create the plot\nggplot() +\n  # Add vertical lines for GPU cluster operational dates (exclude \"Other\")\n  geom_vline(data = clusters,\n             aes(xintercept = as.numeric(date), color = owner_bucket),\n             linetype = \"dashed\", alpha = 0.6, linewidth = 0.5) +\n  # Add scatter points for models\n  geom_point(data = models,\n             aes(x = date, y = training_compute, color = owner_bucket),\n             alpha = 0.7, size = 2) +\n  # Apply color palette\n  scale_color_manual(values = color_palette, name = \"Organization\") +\n  # Log scale for y-axis (training compute)\n  scale_y_log10(labels = scales::scientific) +\n  # Facet by organization\n  facet_wrap(~ owner_bucket, ncol = 3, scales = \"free_x\") +\n  # Labels and theme\n  labs(\n    title = \"AI Model Releases and GPU Cluster Deployment Timeline by Organization\",\n    subtitle = \"Each panel shows models and GPU clusters\\nfor a single organization\",\n    x = \"Date\",\n    y = \"Training Compute (FLOPs, log scale)\",\n    caption = \"Vertical dashed lines represent GPU cluster first operational dates\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\", size = 10)\n  )\n\n\n\n\n\n\n\n\n\nIn this graph we visualize the date of model releases (dots) from different leading technology and AI companies. Overlayed as dashed lines, we plot the first operational date of GPU clusters from those same companies.\nWe would expect that model releases are delayed in comparison to GPU cluster opening dates since it would take significant computational resources to train a new model, usually several months of training for the largest models.\nHowever, we see varied trends across different companies. OpenAI, for example, had released many models before ever opening their first GPU cluster, while Google had many GPU clusters before releasing their first model.\nxAI (behind the Grok set of models) provides an interesting example of a clear cluster-to-model gap. The company seems to follow a very clear trend of having around a half-year delay between cluster opening and model release, with new models being released slightly before or slight after a new cluster becoming operational.\nOverall, the this plot reveals the strengths and weaknesses of different players in the AI ecosystem. Google, being a legacy tech company which has always required heavy computational resources, already had many compute centers before releasing their first model. Alibaba follows a similar trend. On the other hand, an AI-first company like OpenAI likely had to outsource their model training compute to other companies before having their own clusters.\n\n\n3.2.2 Cumulative Compute Over Time by Organization\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\n\n# Load the cumulative compute timeline data\ncumulative_data &lt;- read_csv(\"./Data/cumulative_compute_timeline.csv\")\n\n# Convert date to datetime and training_compute to numeric\ncumulative_data &lt;- cumulative_data |&gt;\n  mutate(date = as.Date(date))\n\n# Filter to only OpenAI\ncumulative_data &lt;- cumulative_data |&gt; filter(owner_bucket == \"OpenAI\")\n\n# Separate clusters and models\nclusters &lt;- cumulative_data |&gt; filter(type == \"cluster\")\n\n# For models, only keep those with known training_compute and get top 10 by training_compute\nmodels &lt;- cumulative_data |&gt;\n  filter(type == \"model\", !is.na(training_compute)) |&gt;\n  arrange(desc(training_compute)) |&gt;\n  head(10)\n\nprint(paste(\"Number of model release lines:\", nrow(models)))\n\n\n[1] \"Number of model release lines: 10\"\n\n\nCode\n# Create the plot\nggplot() +\n  # Add step lines for cumulative compute (clusters only)\n  geom_step(data = clusters,\n            aes(x = date, y = cumulative_compute),\n            linewidth = 1, alpha = 0.8, color = \"#10A37F\", direction = \"hv\") +\n  geom_point(data = clusters,\n             aes(x = date, y = cumulative_compute),\n             size = 3, alpha = 0.6, color = \"#10A37F\") +\n  # Add vertical dashed lines for model releases\n  geom_vline(data = models,\n             aes(xintercept = as.numeric(date)),\n             linetype = \"dashed\", alpha = 0.4, linewidth = 0.5, color = \"#10A37F\") +\n  # Log scale for y-axis\n  scale_y_log10(labels = scales::comma) +\n  # Labels and theme\n  labs(\n    title = \"OpenAI: Cumulative GPU Cluster Compute Over Time\",\n    subtitle = \"Line shows compute growth\\nDashed lines indicate model releases\",\n    x = \"Date\",\n    y = \"Cumulative Compute (H100 equivalents, log scale)\",\n    caption = \"Only existing clusters with known compute capacity included\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nWe can further understand the relationship between cluster development and model development by looking at similar graphs which instead plot compute capacity over time with model release dates overlaid. Particularly we are plotting cumulative compute capacity for major companies over time. We derive the cumulative compute available to companies over time as the sum of compute for their data centers. For these graph, we define cluster compute in terms of number of H100 equivalents (H100 being a popular GPU). We only plot notable companies with clusters to focus our results. Above is an example with OpenAI.\n\n\n3.2.3 Cumulative Compute Over Time by Organization (Faceted)\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\n\n# Load the cumulative compute timeline data\ncumulative_data &lt;- read_csv(\"./Data/cumulative_compute_timeline.csv\")\n\n# Convert date to datetime\ncumulative_data &lt;- cumulative_data |&gt;\n  mutate(date = as.Date(date))\n\n# Get companies that have clusters\ncompanies_with_clusters &lt;- cumulative_data |&gt;\n  filter(type == \"cluster\") |&gt;\n  pull(owner_bucket) |&gt;\n  unique()\n\n# Filter to only companies with clusters and exclude \"Other\"\ncumulative_data &lt;- cumulative_data |&gt;\n  filter(owner_bucket %in% companies_with_clusters, owner_bucket != \"Other\")\n\n# Separate clusters\nclusters &lt;- cumulative_data |&gt; filter(type == \"cluster\")\n\n# For models, get top 10 by training_compute for each organization\nmodels &lt;- cumulative_data |&gt;\n  filter(type == \"model\", !is.na(training_compute)) |&gt;\n  group_by(owner_bucket) |&gt;\n  arrange(desc(training_compute)) |&gt;\n  slice_head(n = 10) |&gt;\n  ungroup()\n\n# Define color palette (consistent with previous graphs)\ncolor_palette &lt;- c(\n  \"Google DeepMind\" = \"#4285F4\",\n  \"OpenAI\" = \"#10A37F\",\n  \"Anthropic\" = \"#D4A574\",\n  \"xAI\" = \"#000000\",\n  \"Meta\" = \"#0668E1\",\n  \"Alibaba\" = \"#FF6A00\",\n  \"Amazon\" = \"#FF9900\"\n)\n\n# Create the plot\nggplot() +\n  # Add step lines for cumulative compute (clusters only)\n  geom_step(data = clusters,\n            aes(x = date, y = cumulative_compute, color = owner_bucket),\n            linewidth = 1, alpha = 0.8, direction = \"hv\") +\n  geom_point(data = clusters,\n             aes(x = date, y = cumulative_compute, color = owner_bucket),\n             size = 2, alpha = 0.6) +\n  # Add vertical dashed lines for model releases\n  geom_vline(data = models,\n             aes(xintercept = as.numeric(date), color = owner_bucket),\n             linetype = \"dashed\", alpha = 0.4, linewidth = 0.3) +\n  # Facet by organization\n  facet_wrap(~ owner_bucket, ncol = 2, scales = \"free_y\") +\n  # Apply color palette\n  scale_color_manual(values = color_palette, name = \"Organization\") +\n  # Log scale for y-axis\n  scale_y_log10(labels = scales::comma) +\n  # Labels and theme\n  labs(\n    title = \"Cumulative GPU Cluster Compute Over Time by Organization\",\n    subtitle = \"Lines show compute growth\\nDashed lines indicate top 10 model releases by training compute\",\n    x = \"Date\",\n    y = \"Cumulative Compute (H100 equivalents, log scale)\",\n    caption = \"Only organizations with GPU clusters; top 10 models by training compute shown\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",  # Hide legend since facets show organizations\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\", size = 10),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nHere plot plot notable companies and the increase in their available compute over time. First note that to improve readability of the graph, we free the y-axis. However, it seems like major companies have increased their compute nearly exponentially. As consistent with the previous graphs, cluster opening time do not follow a predictable pattern in terms of model release, though, generally, it seems like different companies take different approaches in either having built infrastructure first or release models first.\n\n\nCode\nmosaic_data &lt;- gpu_clusters_filtered |&gt;\n  filter(Certainty %in% c(\"Confirmed\", \"Likely\"),\n         Country %in% top_countries) |&gt;\n  mutate(country_short = str_trunc(Country, width = 15, side = \"right\"))\nmosaic_data |&gt; group_by(Country) |&gt; count()\n\n\n# A tibble: 6 × 2\n# Groups:   Country [6]\n  Country                      n\n  &lt;chr&gt;                    &lt;int&gt;\n1 France                      12\n2 Germany                     10\n3 Japan                       23\n4 Korea (Republic of)         10\n5 United States of America   135\n6 &lt;NA&gt;                        57\n\n\nCode\nmosaic_data |&gt;\n  dplyr::select(Certainty, Country, Sector, `Single cluster?`) |&gt;\n  table() |&gt;\n  pairs(\n        space = .15,\n        lower_panel = pairs_mosaic(highlighting = 2, spacing = spacing_equal(0)),\n        upper_panel = pairs_mosaic(spacing = spacing_equal(0)),\n        diag_panel = pairs_barplot(\n          gp_vartext = gpar(fontsize = 8),\n          gp_leveltext = gpar(fontsize = 8),\n          abbreviate = 2),\n        main = \"Pairwise Associations: GPU Cluster Characteristics\",\n        main_gp = gpar(fontsize = 12)\n      )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(plot_data, aes(x = total_calculated_cost, y = owner_short)) +\n  geom_point(size=2, color='blue') +\n  facet_grid(Sector ~ ., scales = \"free_y\", space = \"free_y\") +\n  ggtitle(paste('Top', top_n, \"Total Calculated Cost in GPU Cluster Projects \\nGrouped By Owner\")) +\n  xlab(\"Total Calculated Cost (in billions of USD)\") +\n  ylab(\"\") +\n  theme_linedraw() +\n  theme(\n    axis.text.y = element_text(size = 7),  # smaller text\n    strip.text.y = element_text(size = 8, margin = margin(2, 2, 2, 2)),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n3.2.4 Dataset Size Over Time\nClearly, overall compute available to train model has been increasing at an exponential rate. However has this increase in compute been a proactive move to be prepared to train data-intensive models? Or has this been a reactive move in response to models that turned out to be more data intensive than expected?\nTo answer this question, we first need to look at trends in dataset size and model size.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(scales)\n\n# Load the notable AI models dataset\nmodels_data &lt;- read_csv(\"./Data/ai_models_dataset/notable_ai_models.csv\")\n\n# Filter to models with known dataset size and publication date\ndataset_plot_data &lt;- models_data |&gt;\n  filter(!is.na(`Training dataset size (gradients)`),\n         !is.na(`Publication date`)) |&gt;\n  mutate(`Publication date` = as.Date(`Publication date`),\n         `Training dataset size (gradients)` = as.numeric(`Training dataset size (gradients)`))\n\n# Create scatter plot\nggplot(dataset_plot_data, aes(x = `Publication date`, y = `Training dataset size (gradients)`)) +\n  geom_point(alpha = 0.6, size = 2, color = \"#4285F4\") +\n  # Log scale for y-axis\n  scale_y_log10(labels = comma) +\n  # Labels and theme\n  labs(\n    title = \"AI Model Dataset Size Over Time\",\n    subtitle = \"Training dataset size has grown exponentially\",\n    x = \"Publication Date\",\n    y = \"Training Dataset Size (gradients, log scale)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nThis graph simply plots dataset size over time. We adjust the y-axis to be on a log scale, meaning that seemingly linear increases actually correspond to exponential increases in non-log scales. At first it is hard to discern any notable trends other than a clear increase in recent year of dataset size.\n\n\n3.2.5 Dataset Size Over Time with Key Models\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(scales)\n\n# Load the notable AI models dataset\nmodels_data &lt;- read_csv(\"./Data/ai_models_dataset/notable_ai_models.csv\")\n\n# Filter to models with known dataset size and publication date\ndataset_plot_data &lt;- models_data |&gt;\n  filter(!is.na(`Training dataset size (gradients)`),\n         !is.na(`Publication date`)) |&gt;\n  mutate(`Publication date` = as.Date(`Publication date`),\n         `Training dataset size (gradients)` = as.numeric(`Training dataset size (gradients)`))\n\n# Define key model release dates\nalexnet_date &lt;- as.Date(\"2012-09-29\")\ntransformer_date &lt;- as.Date(\"2017-06-11\")\n\n# Create scatter plot with vertical lines for key models\nggplot(dataset_plot_data, aes(x = `Publication date`, y = `Training dataset size (gradients)`)) +\n  # Add vertical lines for key models\n  geom_vline(xintercept = as.numeric(alexnet_date),\n             linetype = \"dashed\", color = \"#EA4335\", linewidth = 0.8) +\n  geom_vline(xintercept = as.numeric(transformer_date),\n             linetype = \"dashed\", color = \"#34A853\", linewidth = 0.8) +\n  # Add scatter points\n  geom_point(alpha = 0.6, size = 2, color = \"#4285F4\") +\n  # Log scale for y-axis\n  scale_y_log10(labels = comma) +\n  # Labels and theme\n  labs(\n    title = \"AI Model Dataset Size Over Time\",\n    subtitle = \"Training dataset size has grown exponentially.\\nDashed lines show AlexNet (red) and Transformer (green) releases.\",\n    x = \"Publication Date\",\n    y = \"Training Dataset Size (gradients, log scale)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nHowever, much of the demand for more data has been driven by new machine learning approaches. First, the advent of deep learning create demand for larger datasets. After, the advent of the transformer architecture allowed feasible training on even larger datasets (partly why large language models are considered “large”). As such, we add two dashed lines, the first being the landmark AlexNet paper, which ushered in a new era of deep learning critically, thorough the use of GPU training. The green dashed line represent the release of the landmark Attention Is All You Need paper, which introduced the transformer architecture that underlies large language models today. Adding these separators, we begin to more clearly see trends in the data. Notably, after the introduction of the transformer, we see a sharp increase in dataset size.\n\n\n3.2.6 Dataset Size Growth Rate Before and After Transformer\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(scales)\n\n# Load the notable AI models dataset\nmodels_data &lt;- read_csv(\"./Data/ai_models_dataset/notable_ai_models.csv\")\n\n# Filter to models with known dataset size and publication date\ndataset_plot_data &lt;- models_data |&gt;\n  filter(!is.na(`Training dataset size (gradients)`),\n         !is.na(`Publication date`)) |&gt;\n  mutate(`Publication date` = as.Date(`Publication date`),\n         `Training dataset size (gradients)` = as.numeric(`Training dataset size (gradients)`))\n\n# Define key model release dates\nalexnet_date &lt;- as.Date(\"2012-09-29\")\ntransformer_date &lt;- as.Date(\"2017-06-11\")\n\n# Split data into pre-Transformer and post-Transformer periods\npre_transformer &lt;- dataset_plot_data |&gt; filter(`Publication date` &lt; transformer_date)\npost_transformer &lt;- dataset_plot_data |&gt; filter(`Publication date` &gt;= transformer_date)\n\n# Create scatter plot with two linear regression lines\nggplot(dataset_plot_data, aes(x = `Publication date`, y = `Training dataset size (gradients)`)) +\n  # Add vertical lines for key models\n  geom_vline(xintercept = as.numeric(alexnet_date),\n             linetype = \"dashed\", color = \"#EA4335\", linewidth = 0.8, alpha = 0.5) +\n  geom_vline(xintercept = as.numeric(transformer_date),\n             linetype = \"dashed\", color = \"#34A853\", linewidth = 0.8) +\n  # Add scatter points\n  geom_point(alpha = 0.6, size = 2, color = \"#4285F4\") +\n  # Add linear regression for pre-Transformer period\n  geom_smooth(data = pre_transformer,\n              method = \"lm\",\n              se = TRUE,\n              color = \"#FBBC04\",\n              fill = \"#FBBC04\",\n              alpha = 0.2,\n              linewidth = 1) +\n  # Add linear regression for post-Transformer period\n  geom_smooth(data = post_transformer,\n              method = \"lm\",\n              se = TRUE,\n              color = \"#34A853\",\n              fill = \"#34A853\",\n              alpha = 0.2,\n              linewidth = 1) +\n  # Log scale for y-axis\n  scale_y_log10(labels = comma) +\n  # Labels and theme\n  labs(\n    title = \"Dataset Size Growth Rate Before and After Transformer\",\n    subtitle = \"Yellow line: pre-Transformer growth.\\nGreen line: post-Transformer growth.\\nShaded areas show confidence intervals.\",\n    x = \"Publication Date\",\n    y = \"Training Dataset Size (gradients, log scale)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nTo further quantify the difference between the pre-transformer era and the post-transformer era of AI, we add regression lines of our data before the transformer and after the transformer. We already see an exponential increase in dataset size before the transformer, as denoted by the yellow line. However, after the transformer, this increase becomes much sharper, as shown by the green line.\nBy all accounts, dataset used to train ML models is experiencing lightning fast growth.\n\n\n3.2.7 Model Parameters Over Time\nDataset size is only one aspect that increases demand of compute, but the size of the model being trained is another factor that influences how much compute is required. We create a similar graph to those above, but this time plotting model size (number of trainable parameters).\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(scales)\n\n# Load the notable AI models dataset\nmodels_data &lt;- read_csv(\"./Data/ai_models_dataset/notable_ai_models.csv\")\n\n# Filter to models with known parameters and publication date\nparameters_plot_data &lt;- models_data |&gt;\n  filter(!is.na(Parameters),\n         !is.na(`Publication date`)) |&gt;\n  mutate(`Publication date` = as.Date(`Publication date`),\n         Parameters = as.numeric(Parameters))\n\n# Define key model release dates\nalexnet_date &lt;- as.Date(\"2012-09-29\")\ntransformer_date &lt;- as.Date(\"2017-06-11\")\n\n# Split data into pre-Transformer and post-Transformer periods\npre_transformer &lt;- parameters_plot_data |&gt; filter(`Publication date` &lt; transformer_date)\npost_transformer &lt;- parameters_plot_data |&gt; filter(`Publication date` &gt;= transformer_date)\n\n# Create scatter plot with two linear regression lines\nggplot(parameters_plot_data, aes(x = `Publication date`, y = Parameters)) +\n  # Add vertical lines for key models\n  geom_vline(xintercept = as.numeric(alexnet_date),\n             linetype = \"dashed\", color = \"#EA4335\", linewidth = 0.8, alpha = 0.5) +\n  geom_vline(xintercept = as.numeric(transformer_date),\n             linetype = \"dashed\", color = \"#34A853\", linewidth = 0.8) +\n  # Add scatter points\n  geom_point(alpha = 0.6, size = 2, color = \"#4285F4\") +\n  # Add linear regression for pre-Transformer period\n  geom_smooth(data = pre_transformer,\n              method = \"lm\",\n              se = TRUE,\n              color = \"#FBBC04\",\n              fill = \"#FBBC04\",\n              alpha = 0.2,\n              linewidth = 1) +\n  # Add linear regression for post-Transformer period\n  geom_smooth(data = post_transformer,\n              method = \"lm\",\n              se = TRUE,\n              color = \"#34A853\",\n              fill = \"#34A853\",\n              alpha = 0.2,\n              linewidth = 1) +\n  # Log scale for y-axis\n  scale_y_log10(labels = comma) +\n  # Labels and theme\n  labs(\n    title = \"Model Parameters Growth Rate Before and After Transformer\",\n    subtitle = \"Yellow line: pre-Transformer growth.\\nGreen line: post-Transformer growth. Shaded areas show confidence intervals.\",\n    x = \"Publication Date\",\n    y = \"Parameters (log scale)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nIn this plot we see a trend that follows almost exactly the same trend as the trends in dataset size. Until the the transformer, parameter count is increase rapidly (exponentially, in fact) already, but once the transformer is introduced we start to see an even more rapid increase.\n\n\n3.2.8 Training Time Analysis\nSo overall, we see both an exponential increases in the demand of compute (increases in dataset size and model size) and exponential increases in the supply of compute (GPU cluster computer availability). Naturally, we can wonder whether the supply of compute is keeping pace with the demand, or is the demand putting strain on the supply.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(scales)\n\n# Load the training time results\ntraining_times &lt;- read_csv(\"./Data/gpu_cluster_training_times.csv\")\n\n# Convert date to datetime\ntraining_times &lt;- training_times |&gt;\n  mutate(`First Operational Date` = as.Date(`First Operational Date`))\n\n# Create scatter plot\nggplot(training_times, aes(x = `First Operational Date`, y = `Training time (hours)`)) +\n  geom_point(alpha = 0.7, size = 3, color = \"#4285F4\") +\n  # Add a trend line\n  geom_smooth(method = \"loess\", se = TRUE, color = \"#D32F2F\", alpha = 0.2) +\n  # Log scale for y-axis to better show the range\n  scale_y_log10(labels = comma) +\n  # Format x-axis dates\n  scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\") +\n  labs(\n    title = \"Average Training Time for Top 10 Frontier Models\\non Rank-1 GPU Clusters\",\n    x = \"GPU Cluster First Operational Date\",\n    y = \"Average Training Time (hours, log scale)\",\n    caption = \"Based on top 10 frontier models by training compute released before each cluster's operational date\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nHere we plot the average training time in hours for the top 10 best models at the time of the first operational date of the best GPU cluster at the time of its opening. Simply put, we graph the training time of the best models on the best GPU cluster of the time. We see a really interesting trend here. If compute supply where to keep pace with compute demand, we would expect there to be no change in average training time. Instead, we see that in that in the early 2020s there is an exponential increase in the training time of best models (even using the best GPU cluster). However, nearing 2024 and 2025, we see that compute availability starts to catch up.\n\n\n3.2.9 Benchmarks\n\n\nCode\n# names(df_num_imputed)\n# constant_cols &lt;- sapply(df_num_imputed, function(x) length(unique(x[!is.na(x)])) == 1)\n# names(df_num_imputed)[constant_cols]\np1 &lt;- draw_biplot(\n  df_num_imputed,\n  key_axis = \"Best score (across scorers)\",\n  fix_sign = TRUE,\n  point_color = \"grey20\",\n  point_labels = TRUE,\n  arrows = FALSE,\n  mult = 5\n)\n\np2 &lt;- draw_biplot(\n  df_num_imputed,\n  fix_sign = TRUE,\n  point_labels = FALSE,\n  point_color = \"grey20\",\n  arrows = TRUE,\n  mult = 5\n)\n\n# Side by side\np1 + p2 + \n  plot_annotation(\n    title = \"Frontier Model Benchmarks (PCA biplots)\",\n    subtitle = \"Best Scoring Model Indicators\"\n  )\n\n\n\n\n\n\n\n\n\nCode\n# install.packages('ggfortify')\n# library(ggfortify)\n# p &lt;- prcomp(df_num_imputed_clean, scale = TRUE)\n# \n# autoplot(\n#   p,\n#   loadings = TRUE, # arrows\n#   loadings.label = TRUE, # text\n#   loadings.colour = 'deepskyblue3',\n#   loadings.label.colour = 'deepskyblue3',\n#   loadings.label.repel = TRUE # overlap\n# )\n\n# scores &lt;- pca$x[,1:2]\n# k &lt;- kmeans(scores, centers = 6)\n# scores &lt;- data.frame(scores) |&gt;\n#   mutate(cluster = factor(k$cluster), country = ratings$country)\n# g4 &lt;- ggplot(scores, aes(PC1, PC2, color = cluster, label = country)) +\n#   geom_point() +\n#   geom_text(nudge_y = .2) +\n#   guides(color=\"none\")\n# g4\n\npca &lt;- prcomp(df_num_pca, scale. = TRUE)\nsummary(pca)\n\n\nImportance of components:\n                          PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     2.5312 1.8018 1.6492 1.3765 0.87135 0.76408 0.53317\nProportion of Variance 0.4004 0.2029 0.1700 0.1184 0.04745 0.03649 0.01777\nCumulative Proportion  0.4004 0.6034 0.7733 0.8918 0.93922 0.97571 0.99347\n                           PC8       PC9     PC10      PC11      PC12      PC13\nStandard deviation     0.32312 2.058e-15 2.83e-16 1.215e-16 8.629e-17 6.105e-17\nProportion of Variance 0.00653 0.000e+00 0.00e+00 0.000e+00 0.000e+00 0.000e+00\nCumulative Proportion  1.00000 1.000e+00 1.00e+00 1.000e+00 1.000e+00 1.000e+00\n                            PC14     PC15      PC16\nStandard deviation     3.786e-17 3.09e-17 5.978e-33\nProportion of Variance 0.000e+00 0.00e+00 0.000e+00\nCumulative Proportion  1.000e+00 1.00e+00 1.000e+00\n\n\n\n\nCode\nprint('Sanity check with total contribution to PC1 & PC2')\n\n\n[1] \"Sanity check with total contribution to PC1 & PC2\"\n\n\nCode\nloadings_12 &lt;- pca$rotation[, 1:2]\ntotal_contribution &lt;- sqrt(loadings_12[,1]^2 + loadings_12[,2]^2)\nsort(total_contribution, decreasing = TRUE)\n\n\n                   Training compute (FLOP)_math \n                                     0.49605964 \n               Training compute cost (2023 USD) \n                                     0.46668279 \n                Training compute cost (upfront) \n                                     0.46501992 \n                              Hardware quantity \n                                     0.46501992 \n                  Training compute (FLOP)_model \n                                     0.44961325 \n                                  Hardware FP16 \n                                     0.39030088 \n                                  Hardware FP32 \n                                     0.39030088 \n                                  Hardware TF16 \n                                     0.39002152 \n          Assumed hardware performance (FLOP/s) \n                                     0.39002152 \n                                  Power per GPU \n                                     0.37974732 \n                                         stderr \n                                     0.24851261 \n(DEPRECATED) Training dataset size (datapoints) \n                                     0.16681305 \n                                     Parameters \n                                     0.16681305 \n                   Training compute lower bound \n                                     0.12107563 \n                                   Hardware age \n                                     0.11106774 \n                                 Hardware count \n                                     0.06304506 \n\n\n\n\n3.2.10 GPU Cluster Geographic Distribution\nClearly, AI developers are prioritizing increasing their compute capacity as evident by the exponential increase in compute capacity that every major AI-developing company has experienced. Naturally, the construction and operation of these GPU clusters will have huge effects in the communities they are built, their governance will be influenced by their location, and localities with more compute capacity will have more leverage on growing tech and companies. To explore this further, we map the number of GPU clusters per country.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(maps)\nlibrary(viridis)\nlibrary(RColorBrewer)\nlibrary(classInt)\n\n# Load GPU clusters dataset\ngpu_clusters &lt;- read_csv(\"./Data/gpu_clusters_dataset/gpu_clusters.csv\")\n\n# Get world map data\nworld_map &lt;- map_data(\"world\")\n\n# Count existing clusters by country\nexisting_clusters &lt;- gpu_clusters |&gt;\n  filter(Status == \"Existing\") |&gt;\n  group_by(Country) |&gt;\n  summarise(count = n()) |&gt;\n  rename(region = Country)\n\n# Count planned clusters by country\nplanned_clusters &lt;- gpu_clusters |&gt;\n  filter(Status == \"Planned\") |&gt;\n  group_by(Country) |&gt;\n  summarise(count = n()) |&gt;\n  rename(region = Country)\n\n# Standardize country names to match map_data\nstandardize_country_names &lt;- function(df) {\n  df |&gt;\n    mutate(region = case_when(\n      region == \"United States of America\" ~ \"USA\",\n      region == \"United Kingdom of Great Britain and Northern Ireland\" ~ \"UK\",\n      region == \"Russian Federation\" ~ \"Russia\",\n      region == \"Republic of Korea\" ~ \"South Korea\",\n      region == \"Viet Nam\" ~ \"Vietnam\",\n      region == \"United Arab Emirates\" ~ \"UAE\",\n      TRUE ~ region\n    ))\n}\n\nexisting_clusters &lt;- standardize_country_names(existing_clusters)\nplanned_clusters &lt;- standardize_country_names(planned_clusters)\n\n# Join with world map\nworld_existing &lt;- world_map |&gt;\n  left_join(existing_clusters, by = \"region\")\n\nworld_planned &lt;- world_map |&gt;\n  left_join(planned_clusters, by = \"region\")\n\n# Calculate Jenks natural breaks for existing clusters\nexisting_breaks &lt;- classIntervals(existing_clusters$count, n = 5, style = \"jenks\")\nexisting_clusters &lt;- existing_clusters |&gt;\n  mutate(count_binned = cut(count, breaks = existing_breaks$brks, include.lowest = TRUE, dig.lab = 10))\n\nworld_existing &lt;- world_map |&gt;\n  left_join(existing_clusters, by = \"region\")\n\n# Calculate Jenks natural breaks for planned clusters\nplanned_breaks &lt;- classIntervals(planned_clusters$count, n = 5, style = \"jenks\")\nplanned_clusters &lt;- planned_clusters |&gt;\n  mutate(count_binned = cut(count, breaks = planned_breaks$brks, include.lowest = TRUE, dig.lab = 10))\n\nworld_planned &lt;- world_map |&gt;\n  left_join(planned_clusters, by = \"region\")\n\n# Map 1: Existing GPU Clusters\nmap_existing &lt;- ggplot(world_existing, aes(x = long, y = lat, group = group, fill = count_binned)) +\n  geom_polygon(color = \"white\", linewidth = 0.1) +\n  scale_fill_brewer(palette = \"Blues\", na.value = \"gray90\", name = \"Count\") +\n  labs(\n    title = \"Existing GPU Clusters by Country\",\n    subtitle = \"Geographic distribution of operational GPU clusters\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank()\n  ) +\n  coord_fixed(1.3)\n\n# Map 2: Planned GPU Clusters\nmap_planned &lt;- ggplot(world_planned, aes(x = long, y = lat, group = group, fill = count_binned)) +\n  geom_polygon(color = \"white\", linewidth = 0.1) +\n  scale_fill_brewer(palette = \"Blues\", na.value = \"gray90\", name = \"Count\") +\n  labs(\n    title = \"Planned GPU Clusters by Country\",\n    subtitle = \"Geographic distribution of planned GPU clusters\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank()\n  ) +\n  coord_fixed(1.3)\n\n# Map 3: Combined (Existing + Planned Total)\ncombined_clusters &lt;- gpu_clusters |&gt;\n  filter(Status %in% c(\"Existing\", \"Planned\")) |&gt;\n  group_by(Country) |&gt;\n  summarise(count = n(), .groups = \"drop\") |&gt;\n  rename(region = Country)\n\ncombined_clusters &lt;- standardize_country_names(combined_clusters)\n\n# Calculate Jenks natural breaks for combined clusters\ncombined_breaks &lt;- classIntervals(combined_clusters$count, n = 5, style = \"jenks\")\ncombined_clusters &lt;- combined_clusters |&gt;\n  mutate(count_binned = cut(count, breaks = combined_breaks$brks, include.lowest = TRUE, dig.lab = 10))\n\nworld_combined &lt;- world_map |&gt;\n  left_join(combined_clusters, by = \"region\")\n\nmap_combined &lt;- ggplot(world_combined, aes(x = long, y = lat, group = group, fill = count_binned)) +\n  geom_polygon(color = \"white\", linewidth = 0.1) +\n  scale_fill_brewer(palette = \"Blues\", na.value = \"gray90\", name = \"Count\") +\n  labs(\n    title = \"Total GPU Clusters by Country (Existing + Planned)\",\n    subtitle = \"Combined geographic distribution of all GPU clusters\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank()\n  ) +\n  coord_fixed(1.3)\n\n# Display the maps\nmap_existing\n\n\n\n\n\n\n\n\n\nFirst we map the number of GPU clusters already existing globally. We can see that the two major players in compute clusters are the United States and China, with some development in Europe, and even lighter development in Latin America, the Middle East, South Asia, and Oceania.\n\n\nCode\nmap_planned\n\n\n\n\n\n\n\n\n\nTo understand the future of GPU cluster allocation, we also map the number of planned GPU clusters in each country. Here we see similar trends to the previous map: the United States and China lead the way in terms of GPU cluster development, while the rest of the world lags behind. Outside of the top two countries, we do see France maintaining their strength in compute within Europe, Brazil maintaining its strength in Latin America, and Saudi Arabia maintaining its strength in the middle east.\n\n\nCode\nmap_combined\n\n\n\n\n\n\n\n\n\nCombining existing and planned GPU cluster maps, we see that the countries that have previously benefitted from AI development and GPU cluster development (namely the US and China) will continue to be the countries that benefit from further development, while most of the rest of the world sees lagging development.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  }
]