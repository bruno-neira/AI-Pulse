# Project Proposal: AI Compute and Usage Trends Analysis
**Team Members:** Michael Chen, Bruno Neira
**Date:** October 30, 2025


## 1 Introduction 

Artificial intelligence has seen an explosion in growth and adoption in the last 3 years. Generative AI models have been integrated into workflows of essentially every major company. Furthermore, many philosophers and technologists have speculated over the future of artificial intelligence and how it will evolve beyond the current moment. In order to do this effectively, it is first important to understand the current and historical context around macro-trends of AI. 

*In addition, there has been much discussion around the factors that drive growth in AI. Some major factors include compute spend and usage rates-- one focus of our study. Also of note, mapping moments of major advancement, whether in adoption or new model releases, with these other factors will help us understand what might be the bottlenecks of AI development today. 

- For an aggregate overview of all foundation model developers, we hypothesize that as compute capacity increases, usage will likewise scale-up but eventually, it will plateau as there is a ceiling to how much usage is even possible (only so many people in the globe with access to technology)
- We also theorize that inverse relationships between compute and usage can be attributable to research developments in new model architectures, data quantity/quality, and others factors outside of direct usage metrics from these datasets-- which signifies that low-usage AI companies may not necessarily be underperforming or that overperforming companies are directing their compute resources efficiently. 

Some questions we are considering:

- How do reported active user metrics rela  te to cluster compute capacity (H100 equivalents, Power MW)?
    - Common companies between datasets
    - How usage-compute capacity evolves over time
    - Which countries/industry sectors dominate compute-usage as a whole?
- Is there a correlation between a company’s reported activity (tokens/messages per day) and the size/cost of their clusters?
    - Any high compute, low usage companies? (underperformers)
    - Any high usage, low compute companies? (overachievers)
- When there has been a major advancement in AI capabilities, how do those moments of breakthroughs relate to changes in compute capacity?
    - Ex: GPT-4 launch, LLaMA launch, etc.
    - Do companies tend to increase compute capacity before or after major model launches?


## 2 Data

We have decided to use EpochAI’s repository on said macro-trends, focusing specifically on two datasets. The main dataset– gpu_clusters (786 rows × 55 cols)– contains details on existing global aggregate GPU cluster performance as of March 2025. This tabulated data contains geospatial information on large hardware facilities, including those used for AI training and inference. Combined with our second file– ai_companies_usage_reports (40 rows x 16 cols)– on the usage rates of llms across different foundation model developers from 2024-2025,  we hope to identify regional trends on compute concentration, model scaling, and ecosystem trends.


How EpochAI collects the data (https://epoch.ai/data/ai-companies#explore-the-data): “In this database, we collect data from direct statements from AI companies and their executives and staff, as well as information reported in established media outlets (generally from insider sources or documents provided to journalists). We rely on public reports rather than proprietary databases, since our goal is to provide a free resource with transparent sourcing”. As for the cluster data on the GPU’s, the methodology is less transparent but the validity of each datapoint is listed according to a certainty field feature.


We anticipate that there may be issues merging the datasets because they aren't perfectly aligned on the columns (Order, Name). There may also be time jumps in our data which could complicate our time series analysis and graphics comparing companies, AI usage, and compute capabilities from year to year.

Sources:

1. GPU: https://epoch.ai/data/gpu-clusters?view=table
2. Usage: https://epoch.ai/data/ai-companies
