# Results

## GPU Clusters

### GPU Clusters and Model Development Timeline

```{r}
library(ggplot2)
library(dplyr)
library(readr)

# Load the combined dataset
combined_data <- read_csv("./Data/combined_gpu_models.csv")

# Create owner buckets
bucket_owners <- function(owner) {
  case_when(
    grepl("Google DeepMind", owner, ignore.case = TRUE) ~ "Google DeepMind",
    grepl("OpenAI", owner, ignore.case = TRUE) ~ "OpenAI",
    grepl("Anthropic", owner, ignore.case = TRUE) ~ "Anthropic",
    grepl("xAI", owner, ignore.case = TRUE) ~ "xAI",
    grepl("Meta", owner, ignore.case = TRUE) ~ "Meta",
    grepl("Alibaba", owner, ignore.case = TRUE) ~ "Alibaba",
    grepl("Microsoft", owner, ignore.case = TRUE) ~ "Microsoft",
    grepl("Amazon", owner, ignore.case = TRUE) ~ "Amazon",
    TRUE ~ "Other"
  )
}

# Apply bucketing
combined_data <- combined_data %>%
  mutate(owner_bucket = bucket_owners(owner))

# Separate models and clusters
models <- combined_data %>% filter(type == "model")
clusters <- combined_data %>%
  filter(type == "cluster", owner_bucket != "Other")  # Exclude "Other" clusters for lines

# Find the date of the first GPU cluster
first_cluster_date <- min(combined_data$date[combined_data$type == "cluster"], na.rm = TRUE)

# Filter both models and clusters to only include data after first cluster
models <- models %>% filter(date >= first_cluster_date)
clusters <- clusters %>% filter(date >= first_cluster_date)

# Define color palette for the buckets
color_palette <- c(
  "Google DeepMind" = "#4285F4",  # Google Blue
  "OpenAI" = "#10A37F",            # OpenAI Green
  "Anthropic" = "#D4A574",         # Anthropic Gold
  "xAI" = "#000000",               # Black
  "Meta" = "#0668E1",              # Meta Blue
  "Alibaba" = "#FF6A00",           # Alibaba Orange
  "Microsoft" = "#00A4EF",         # Microsoft Blue
  "Amazon" = "#FF9900",            # Amazon Orange
  "Other" = "#808080"              # Gray
)

# Create the plot
ggplot() +
  # Add vertical lines for GPU cluster operational dates (exclude "Other")
  geom_vline(data = clusters,
             aes(xintercept = as.numeric(date), color = owner_bucket),
             linetype = "dashed", alpha = 0.6, linewidth = 0.5) +
  # Add scatter points for models
  geom_point(data = models,
             aes(x = date, y = training_compute, color = owner_bucket),
             alpha = 0.7, size = 2) +
  # Apply color palette
  scale_color_manual(values = color_palette, name = "Organization") +
  # Log scale for y-axis (training compute)
  scale_y_log10(labels = scales::scientific) +
  # Facet by organization
  facet_wrap(~ owner_bucket, ncol = 3, scales = "free_x") +
  # Labels and theme
  labs(
    title = "AI Model Releases and GPU Cluster Deployment Timeline by Organization",
    subtitle = "Each panel shows models and GPU clusters for a single organization",
    x = "Date",
    y = "Training Compute (FLOPs, log scale)",
    caption = "Vertical dashed lines represent GPU cluster first operational dates"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold", size = 10)
  )
```

In this graph we visualize the date of model releases (dots) from different leading technolgy and AI companies. Overlayed as dashed lines, we plot the first operational date of GPU clusters from those same companies.

We would expect that model releases are delayed in comparison to GPU cluster opening dates since it would take significant computational resources to train a new model, usually several months of training for the largest models.

However, we see varied trends across different companies. OpenAI, for example, had released many models before ever opening their first GPU cluster, while Google had many GPU clusters before releasing their first model.

xAI (behind the Grok set of models) provides and intersting example of a clear cluster-to-model gap. The company seems to follow a very clear trend of having around a half-year delay between cluster opening and model release, with new models being released slightly before or slight after a new cluster becoming operational.

Overall, the this plot reveals the strengths and weaknesses of different players in the AI ecosystem. Google, being a legacy tech company which has always required heavy computational resouces, already had many compute centers before releasing their first model. Alibaba follows a similar trend. On the other hand, an AI-first company like OpenAI likely had to outsource their model training compute to other companies before having their own clusters. 

### Cumulative Compute Over Time by Organization

```{r}
library(ggplot2)
library(dplyr)
library(readr)

# Load the cumulative compute timeline data
cumulative_data <- read_csv("./Data/cumulative_compute_timeline.csv")

# Convert date to datetime and training_compute to numeric
cumulative_data <- cumulative_data %>%
  mutate(date = as.Date(date))

# Filter to only OpenAI
cumulative_data <- cumulative_data %>% filter(owner_bucket == "OpenAI")

# Separate clusters and models
clusters <- cumulative_data %>% filter(type == "cluster")

# For models, only keep those with known training_compute and get top 10 by training_compute
models <- cumulative_data %>%
  filter(type == "model", !is.na(training_compute)) %>%
  arrange(desc(training_compute)) %>%
  head(10)

print(paste("Number of model release lines:", nrow(models)))

# Create the plot
ggplot() +
  # Add step lines for cumulative compute (clusters only)
  geom_step(data = clusters,
            aes(x = date, y = cumulative_compute),
            linewidth = 1, alpha = 0.8, color = "#10A37F", direction = "hv") +
  geom_point(data = clusters,
             aes(x = date, y = cumulative_compute),
             size = 3, alpha = 0.6, color = "#10A37F") +
  # Add vertical dashed lines for model releases
  geom_vline(data = models,
             aes(xintercept = as.numeric(date)),
             linetype = "dashed", alpha = 0.4, linewidth = 0.5, color = "#10A37F") +
  # Log scale for y-axis
  scale_y_log10(labels = scales::comma) +
  # Labels and theme
  labs(
    title = "OpenAI: Cumulative GPU Cluster Compute Over Time",
    subtitle = "Line shows compute growth; dashed lines indicate model releases",
    x = "Date",
    y = "Cumulative Compute (H100 equivalents, log scale)",
    caption = "Only existing clusters with known compute capacity included"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank()
  )
```

We can further understand the relationship between cluster development and model development by looking at similar graphs which instead plot compute capacity over time with model release dates overlaid. Particularly we are plotting cumulative compute capacity for major companies over time. We derive the cumulative compute available to companies over time as the sum of compute for their data centers. For these graph, we define cluster compute in terms of number of H100 equivalents (H100 being a popular GPU). We only plot notable companies with clusters to focus our results. Above is an example with OpenAI.

### Cumulative Compute Over Time by Organization (Faceted)

```{r}
library(ggplot2)
library(dplyr)
library(readr)

# Load the cumulative compute timeline data
cumulative_data <- read_csv("./Data/cumulative_compute_timeline.csv")

# Convert date to datetime
cumulative_data <- cumulative_data %>%
  mutate(date = as.Date(date))

# Get companies that have clusters
companies_with_clusters <- cumulative_data %>%
  filter(type == "cluster") %>%
  pull(owner_bucket) %>%
  unique()

# Filter to only companies with clusters and exclude "Other"
cumulative_data <- cumulative_data %>%
  filter(owner_bucket %in% companies_with_clusters, owner_bucket != "Other")

# Separate clusters
clusters <- cumulative_data %>% filter(type == "cluster")

# For models, get top 10 by training_compute for each organization
models <- cumulative_data %>%
  filter(type == "model", !is.na(training_compute)) %>%
  group_by(owner_bucket) %>%
  arrange(desc(training_compute)) %>%
  slice_head(n = 10) %>%
  ungroup()

# Define color palette (consistent with previous graphs)
color_palette <- c(
  "Google DeepMind" = "#4285F4",
  "OpenAI" = "#10A37F",
  "Anthropic" = "#D4A574",
  "xAI" = "#000000",
  "Meta" = "#0668E1",
  "Alibaba" = "#FF6A00",
  "Amazon" = "#FF9900"
)

# Create the plot
ggplot() +
  # Add step lines for cumulative compute (clusters only)
  geom_step(data = clusters,
            aes(x = date, y = cumulative_compute, color = owner_bucket),
            linewidth = 1, alpha = 0.8, direction = "hv") +
  geom_point(data = clusters,
             aes(x = date, y = cumulative_compute, color = owner_bucket),
             size = 2, alpha = 0.6) +
  # Add vertical dashed lines for model releases
  geom_vline(data = models,
             aes(xintercept = as.numeric(date), color = owner_bucket),
             linetype = "dashed", alpha = 0.4, linewidth = 0.3) +
  # Facet by organization
  facet_wrap(~ owner_bucket, ncol = 2, scales = "free_y") +
  # Apply color palette
  scale_color_manual(values = color_palette, name = "Organization") +
  # Log scale for y-axis
  scale_y_log10(labels = scales::comma) +
  # Labels and theme
  labs(
    title = "Cumulative GPU Cluster Compute Over Time by Organization",
    subtitle = "Lines show compute growth; dashed lines indicate top 10 model releases by training compute",
    x = "Date",
    y = "Cumulative Compute (H100 equivalents, log scale)",
    caption = "Only organizations with GPU clusters; top 10 models by training compute shown"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",  # Hide legend since facets show organizations
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold", size = 10),
    panel.grid.minor = element_blank()
  )
```

Here plot plot notable companies and the increase in their available compute over time. First note that to improve readability of the graph, we free the y-axis. However, it seems like major companies have increased their compute nearly exponentially. As consitent with the previous graphs, cluster opening time do not follow a predictable pattern in terms of model release, though, generally, it seems like different companies take different approaches in either having built infrascture first or release models first. 

### Dataset Size Over Time

Clearly, overall compute available to train model has been increasing at an exponential rate. However have the has this increase in compute been a proactive move to be prepared to train data-intensive models? Or has this been a reactive move in response to models that turned out to be more data intensive than expected?

To answer this question, we first need to look at trends in dataset size and model size.

```{r}
library(ggplot2)
library(dplyr)
library(readr)
library(scales)

# Load the notable AI models dataset
models_data <- read_csv("./Data/ai_models_dataset/notable_ai_models.csv")

# Filter to models with known dataset size and publication date
dataset_plot_data <- models_data %>%
  filter(!is.na(`Training dataset size (gradients)`),
         !is.na(`Publication date`)) %>%
  mutate(`Publication date` = as.Date(`Publication date`),
         `Training dataset size (gradients)` = as.numeric(`Training dataset size (gradients)`))

# Create scatter plot
ggplot(dataset_plot_data, aes(x = `Publication date`, y = `Training dataset size (gradients)`)) +
  geom_point(alpha = 0.6, size = 2, color = "#4285F4") +
  # Log scale for y-axis
  scale_y_log10(labels = comma) +
  # Labels and theme
  labs(
    title = "AI Model Dataset Size Over Time",
    subtitle = "Training dataset size has grown exponentially",
    x = "Publication Date",
    y = "Training Dataset Size (gradients, log scale)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank()
  )
```

This graph simply plots dataset size over time. We adjust the y-axis to be on a log scale, meaning that seemingly linear increases actually correspond to exponential increases in non-log scales. At first it is hard to discern any notable trends other than a clear increase in recent year of dataset size.

### Dataset Size Over Time with Key Models

```{r}
library(ggplot2)
library(dplyr)
library(readr)
library(scales)

# Load the notable AI models dataset
models_data <- read_csv("./Data/ai_models_dataset/notable_ai_models.csv")

# Filter to models with known dataset size and publication date
dataset_plot_data <- models_data %>%
  filter(!is.na(`Training dataset size (gradients)`),
         !is.na(`Publication date`)) %>%
  mutate(`Publication date` = as.Date(`Publication date`),
         `Training dataset size (gradients)` = as.numeric(`Training dataset size (gradients)`))

# Define key model release dates
alexnet_date <- as.Date("2012-09-29")
transformer_date <- as.Date("2017-06-11")

# Create scatter plot with vertical lines for key models
ggplot(dataset_plot_data, aes(x = `Publication date`, y = `Training dataset size (gradients)`)) +
  # Add vertical lines for key models
  geom_vline(xintercept = as.numeric(alexnet_date),
             linetype = "dashed", color = "#EA4335", linewidth = 0.8) +
  geom_vline(xintercept = as.numeric(transformer_date),
             linetype = "dashed", color = "#34A853", linewidth = 0.8) +
  # Add scatter points
  geom_point(alpha = 0.6, size = 2, color = "#4285F4") +
  # Log scale for y-axis
  scale_y_log10(labels = comma) +
  # Labels and theme
  labs(
    title = "AI Model Dataset Size Over Time",
    subtitle = "Training dataset size has grown exponentially. Dashed lines show AlexNet (red) and Transformer (green) releases.",
    x = "Publication Date",
    y = "Training Dataset Size (gradients, log scale)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank()
  )
```

However, much of the demand for more data has been driven by new machine learning approaches. First, the advent of deep learning create demand for larger datasets. After, the advent of the transformer architecture allowed feasible training on even larger datasets (parly why large language models are considered "large"). As such, we add two dashed lines, the first being the landmark [AlexNet](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) paper, which ushered in a new era of deep learning critically, thorough the use of GPU training. The green dashed line represent the release of the landmark [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper, which introduced the transformer architecture that underlies large language models today. Adding these seperators, we begin to more clearly see trends in the data. Notably, after the introduction of the transformer, we see a sharp increase in dataset size.

### Dataset Size Growth Rate Before and After Transformer

```{r}
library(ggplot2)
library(dplyr)
library(readr)
library(scales)

# Load the notable AI models dataset
models_data <- read_csv("./Data/ai_models_dataset/notable_ai_models.csv")

# Filter to models with known dataset size and publication date
dataset_plot_data <- models_data %>%
  filter(!is.na(`Training dataset size (gradients)`),
         !is.na(`Publication date`)) %>%
  mutate(`Publication date` = as.Date(`Publication date`),
         `Training dataset size (gradients)` = as.numeric(`Training dataset size (gradients)`))

# Define key model release dates
alexnet_date <- as.Date("2012-09-29")
transformer_date <- as.Date("2017-06-11")

# Split data into pre-Transformer and post-Transformer periods
pre_transformer <- dataset_plot_data %>% filter(`Publication date` < transformer_date)
post_transformer <- dataset_plot_data %>% filter(`Publication date` >= transformer_date)

# Create scatter plot with two linear regression lines
ggplot(dataset_plot_data, aes(x = `Publication date`, y = `Training dataset size (gradients)`)) +
  # Add vertical lines for key models
  geom_vline(xintercept = as.numeric(alexnet_date),
             linetype = "dashed", color = "#EA4335", linewidth = 0.8, alpha = 0.5) +
  geom_vline(xintercept = as.numeric(transformer_date),
             linetype = "dashed", color = "#34A853", linewidth = 0.8) +
  # Add scatter points
  geom_point(alpha = 0.6, size = 2, color = "#4285F4") +
  # Add linear regression for pre-Transformer period
  geom_smooth(data = pre_transformer,
              method = "lm",
              se = TRUE,
              color = "#FBBC04",
              fill = "#FBBC04",
              alpha = 0.2,
              linewidth = 1) +
  # Add linear regression for post-Transformer period
  geom_smooth(data = post_transformer,
              method = "lm",
              se = TRUE,
              color = "#34A853",
              fill = "#34A853",
              alpha = 0.2,
              linewidth = 1) +
  # Log scale for y-axis
  scale_y_log10(labels = comma) +
  # Labels and theme
  labs(
    title = "Dataset Size Growth Rate Before and After Transformer",
    subtitle = "Yellow line: pre-Transformer growth. Green line: post-Transformer growth. Shaded areas show confidence intervals.",
    x = "Publication Date",
    y = "Training Dataset Size (gradients, log scale)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank()
  )
```

To further quantify the difference between the pre-transformer era and the post-transfomer era of AI, we add regression lines of our data before the transformer and after the transformer. We already see an exponential increase in dataset size before the transformer, as denoted by the yellow line. However, after the transformer, this increase becomes much sharper, as shown by the green line.

By all accounts, dataset used to train ML models is experiencing lightning fast growth.


### Model Parameters Over Time

Dataset size is only one aspect that increases demand of compute, but the size of the model being trained is another factor that influences how much compute is required. We create a similar graph to those above, but this time plotting model size (number of trainable parameters).

```{r}
library(ggplot2)
library(dplyr)
library(readr)
library(scales)

# Load the notable AI models dataset
models_data <- read_csv("./Data/ai_models_dataset/notable_ai_models.csv")

# Filter to models with known parameters and publication date
parameters_plot_data <- models_data %>%
  filter(!is.na(Parameters),
         !is.na(`Publication date`)) %>%
  mutate(`Publication date` = as.Date(`Publication date`),
         Parameters = as.numeric(Parameters))

# Define key model release dates
alexnet_date <- as.Date("2012-09-29")
transformer_date <- as.Date("2017-06-11")

# Split data into pre-Transformer and post-Transformer periods
pre_transformer <- parameters_plot_data %>% filter(`Publication date` < transformer_date)
post_transformer <- parameters_plot_data %>% filter(`Publication date` >= transformer_date)

# Create scatter plot with two linear regression lines
ggplot(parameters_plot_data, aes(x = `Publication date`, y = Parameters)) +
  # Add vertical lines for key models
  geom_vline(xintercept = as.numeric(alexnet_date),
             linetype = "dashed", color = "#EA4335", linewidth = 0.8, alpha = 0.5) +
  geom_vline(xintercept = as.numeric(transformer_date),
             linetype = "dashed", color = "#34A853", linewidth = 0.8) +
  # Add scatter points
  geom_point(alpha = 0.6, size = 2, color = "#4285F4") +
  # Add linear regression for pre-Transformer period
  geom_smooth(data = pre_transformer,
              method = "lm",
              se = TRUE,
              color = "#FBBC04",
              fill = "#FBBC04",
              alpha = 0.2,
              linewidth = 1) +
  # Add linear regression for post-Transformer period
  geom_smooth(data = post_transformer,
              method = "lm",
              se = TRUE,
              color = "#34A853",
              fill = "#34A853",
              alpha = 0.2,
              linewidth = 1) +
  # Log scale for y-axis
  scale_y_log10(labels = comma) +
  # Labels and theme
  labs(
    title = "Model Parameters Growth Rate Before and After Transformer",
    subtitle = "Yellow line: pre-Transformer growth. Green line: post-Transformer growth. Shaded areas show confidence intervals.",
    x = "Publication Date",
    y = "Parameters (log scale)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank()
  )
```

### Training Time Analysis

```{r}
library(ggplot2)
library(dplyr)
library(readr)
library(scales)

# Load the training time results
training_times <- read_csv("./Data/gpu_cluster_training_times.csv")

# Convert date to datetime
training_times <- training_times %>%
  mutate(`First Operational Date` = as.Date(`First Operational Date`))

# Create scatter plot
ggplot(training_times, aes(x = `First Operational Date`, y = `Training time (hours)`)) +
  geom_point(alpha = 0.7, size = 3, color = "#4285F4") +
  # Add a trend line
  geom_smooth(method = "loess", se = TRUE, color = "#D32F2F", alpha = 0.2) +
  # Log scale for y-axis to better show the range
  scale_y_log10(labels = comma) +
  # Format x-axis dates
  scale_x_date(date_breaks = "2 years", date_labels = "%Y") +
  labs(
    title = "Average Training Time for Top 10 Frontier Models on Rank-1 GPU Clusters",
    x = "GPU Cluster First Operational Date",
    y = "Average Training Time (hours, log scale)",
    caption = "Based on top 10 frontier models by training compute released before each cluster's operational date"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank()
  )
```

### GPU Cluster Geographic Distribution

Clearly, AI developers are prioritizing increasing their compute capacity as evident by the exponential increase in compute capacity that every major AI-developing company has experienced. Naturally, the construction and operation of these GPU clusters will have huge effects in the communties they are built, their governance will be influenced by their location, and localities with more compute capacity will have more leverage on growing tech and companies. To explore this further, we map the number of GPU clusters per country.

```{r}
library(ggplot2)
library(dplyr)
library(readr)
library(maps)
library(viridis)
library(RColorBrewer)
library(classInt)

# Load GPU clusters dataset
gpu_clusters <- read_csv("./Data/gpu_clusters_dataset/gpu_clusters.csv")

# Get world map data
world_map <- map_data("world")

# Count existing clusters by country
existing_clusters <- gpu_clusters %>%
  filter(Status == "Existing") %>%
  group_by(Country) %>%
  summarise(count = n()) %>%
  rename(region = Country)

# Count planned clusters by country
planned_clusters <- gpu_clusters %>%
  filter(Status == "Planned") %>%
  group_by(Country) %>%
  summarise(count = n()) %>%
  rename(region = Country)

# Standardize country names to match map_data
standardize_country_names <- function(df) {
  df %>%
    mutate(region = case_when(
      region == "United States of America" ~ "USA",
      region == "United Kingdom of Great Britain and Northern Ireland" ~ "UK",
      region == "Russian Federation" ~ "Russia",
      region == "Republic of Korea" ~ "South Korea",
      region == "Viet Nam" ~ "Vietnam",
      region == "United Arab Emirates" ~ "UAE",
      TRUE ~ region
    ))
}

existing_clusters <- standardize_country_names(existing_clusters)
planned_clusters <- standardize_country_names(planned_clusters)

# Join with world map
world_existing <- world_map %>%
  left_join(existing_clusters, by = "region")

world_planned <- world_map %>%
  left_join(planned_clusters, by = "region")

# Calculate Jenks natural breaks for existing clusters
existing_breaks <- classIntervals(existing_clusters$count, n = 5, style = "jenks")
existing_clusters <- existing_clusters %>%
  mutate(count_binned = cut(count, breaks = existing_breaks$brks, include.lowest = TRUE, dig.lab = 10))

world_existing <- world_map %>%
  left_join(existing_clusters, by = "region")

# Calculate Jenks natural breaks for planned clusters
planned_breaks <- classIntervals(planned_clusters$count, n = 5, style = "jenks")
planned_clusters <- planned_clusters %>%
  mutate(count_binned = cut(count, breaks = planned_breaks$brks, include.lowest = TRUE, dig.lab = 10))

world_planned <- world_map %>%
  left_join(planned_clusters, by = "region")

# Map 1: Existing GPU Clusters
map_existing <- ggplot(world_existing, aes(x = long, y = lat, group = group, fill = count_binned)) +
  geom_polygon(color = "white", linewidth = 0.1) +
  scale_fill_brewer(palette = "Blues", na.value = "gray90", name = "Count") +
  labs(
    title = "Existing GPU Clusters by Country",
    subtitle = "Geographic distribution of operational GPU clusters",
    x = NULL,
    y = NULL
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  ) +
  coord_fixed(1.3)

# Map 2: Planned GPU Clusters
map_planned <- ggplot(world_planned, aes(x = long, y = lat, group = group, fill = count_binned)) +
  geom_polygon(color = "white", linewidth = 0.1) +
  scale_fill_brewer(palette = "Blues", na.value = "gray90", name = "Count") +
  labs(
    title = "Planned GPU Clusters by Country",
    subtitle = "Geographic distribution of planned GPU clusters",
    x = NULL,
    y = NULL
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  ) +
  coord_fixed(1.3)

# Map 3: Combined (Existing + Planned Total)
combined_clusters <- gpu_clusters %>%
  filter(Status %in% c("Existing", "Planned")) %>%
  group_by(Country) %>%
  summarise(count = n(), .groups = "drop") %>%
  rename(region = Country)

combined_clusters <- standardize_country_names(combined_clusters)

# Calculate Jenks natural breaks for combined clusters
combined_breaks <- classIntervals(combined_clusters$count, n = 5, style = "jenks")
combined_clusters <- combined_clusters %>%
  mutate(count_binned = cut(count, breaks = combined_breaks$brks, include.lowest = TRUE, dig.lab = 10))

world_combined <- world_map %>%
  left_join(combined_clusters, by = "region")

map_combined <- ggplot(world_combined, aes(x = long, y = lat, group = group, fill = count_binned)) +
  geom_polygon(color = "white", linewidth = 0.1) +
  scale_fill_brewer(palette = "Blues", na.value = "gray90", name = "Count") +
  labs(
    title = "Total GPU Clusters by Country (Existing + Planned)",
    subtitle = "Combined geographic distribution of all GPU clusters",
    x = NULL,
    y = NULL
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  ) +
  coord_fixed(1.3)

# Display the maps
map_existing
```

First we map the number of GPU clusters already existing globally. We can see that the two major players in compute clusters are the United States and China, with some development in Europe, and even lighter development in Latin America, the Middle East, South Asia, and Oceania. 

```{r}
map_planned
```

To understand the future of GPU cluster allocation, we also map the number of planned GPU clusters in each country. Here we see similar trends to the previous map: the United States and China lead the way in terms of GPU cluster development, while the rest of the world lags behind. Outside of the top two countries, we do see France maintaining their strength in compute within Europe, Brazil maintaining its strength in Latin America, and Saudi Arabia maintaining its strength in the middle east.

```{r}
map_combined
```

Combining existing and planned GPU cluster maps, we see that the countries that have previously benefitted from AI development and GPU cluster development (namely the US and China) will continue to be the countries that benefit from furter development, while most of the rest of the world sees lagging development.