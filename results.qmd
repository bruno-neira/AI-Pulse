# Results

## GPU Clusters

- What is the relationship between GPU clusters and the development of new AI models
    - What is the offset between GPU cluster creation and model releases
- What are trends in GPU cluster development?
    - Where have they been built in the past?
    - Where are they planning on being built?
    - Increase in compute per data center?

### GPU Clusters and Model Development Timeline

```{r}
library(ggplot2)
library(dplyr)
library(readr)

# Load the combined dataset
combined_data <- read_csv("./Data/combined_gpu_models.csv")

# Create owner buckets
bucket_owners <- function(owner) {
  case_when(
    grepl("Google DeepMind", owner, ignore.case = TRUE) ~ "Google DeepMind",
    grepl("OpenAI", owner, ignore.case = TRUE) ~ "OpenAI",
    grepl("Anthropic", owner, ignore.case = TRUE) ~ "Anthropic",
    grepl("xAI", owner, ignore.case = TRUE) ~ "xAI",
    grepl("Meta", owner, ignore.case = TRUE) ~ "Meta",
    grepl("Alibaba", owner, ignore.case = TRUE) ~ "Alibaba",
    grepl("Microsoft", owner, ignore.case = TRUE) ~ "Microsoft",
    grepl("Amazon", owner, ignore.case = TRUE) ~ "Amazon",
    TRUE ~ "Other"
  )
}

# Apply bucketing
combined_data <- combined_data %>%
  mutate(owner_bucket = bucket_owners(owner))

# Separate models and clusters
models <- combined_data %>% filter(type == "model")
clusters <- combined_data %>%
  filter(type == "cluster", owner_bucket != "Other")  # Exclude "Other" clusters for lines

# Find the date of the first GPU cluster
first_cluster_date <- min(combined_data$date[combined_data$type == "cluster"], na.rm = TRUE)

# Filter both models and clusters to only include data after first cluster
models <- models %>% filter(date >= first_cluster_date)
clusters <- clusters %>% filter(date >= first_cluster_date)

# Define color palette for the buckets
color_palette <- c(
  "Google DeepMind" = "#4285F4",  # Google Blue
  "OpenAI" = "#10A37F",            # OpenAI Green
  "Anthropic" = "#D4A574",         # Anthropic Gold
  "xAI" = "#000000",               # Black
  "Meta" = "#0668E1",              # Meta Blue
  "Alibaba" = "#FF6A00",           # Alibaba Orange
  "Microsoft" = "#00A4EF",         # Microsoft Blue
  "Amazon" = "#FF9900",            # Amazon Orange
  "Other" = "#808080"              # Gray
)

# Create the plot
ggplot() +
  # Add vertical lines for GPU cluster operational dates (exclude "Other")
  geom_vline(data = clusters,
             aes(xintercept = as.numeric(date), color = owner_bucket),
             linetype = "dashed", alpha = 0.6, linewidth = 0.5) +
  # Add scatter points for models
  geom_point(data = models,
             aes(x = date, y = training_compute, color = owner_bucket),
             alpha = 0.7, size = 2) +
  # Apply color palette
  scale_color_manual(values = color_palette, name = "Organization") +
  # Log scale for y-axis (training compute)
  scale_y_log10(labels = scales::scientific) +
  # Facet by organization
  facet_wrap(~ owner_bucket, ncol = 3, scales = "free_x") +
  # Labels and theme
  labs(
    title = "AI Model Releases and GPU Cluster Deployment Timeline by Organization",
    subtitle = "Each panel shows models and GPU clusters for a single organization",
    x = "Date",
    y = "Training Compute (FLOPs, log scale)",
    caption = "Vertical dashed lines represent GPU cluster first operational dates"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold", size = 10)
  )
```

### GPU Cluster Geographic Distribution

```{r}
library(ggplot2)
library(dplyr)
library(readr)
library(maps)
library(viridis)
library(RColorBrewer)
library(classInt)

# Load GPU clusters dataset
gpu_clusters <- read_csv("./Data/gpu_clusters_dataset/gpu_clusters.csv")

# Get world map data
world_map <- map_data("world")

# Count existing clusters by country
existing_clusters <- gpu_clusters %>%
  filter(Status == "Existing") %>%
  group_by(Country) %>%
  summarise(count = n()) %>%
  rename(region = Country)

# Count planned clusters by country
planned_clusters <- gpu_clusters %>%
  filter(Status == "Planned") %>%
  group_by(Country) %>%
  summarise(count = n()) %>%
  rename(region = Country)

# Standardize country names to match map_data
standardize_country_names <- function(df) {
  df %>%
    mutate(region = case_when(
      region == "United States of America" ~ "USA",
      region == "United Kingdom of Great Britain and Northern Ireland" ~ "UK",
      region == "Russian Federation" ~ "Russia",
      region == "Republic of Korea" ~ "South Korea",
      region == "Viet Nam" ~ "Vietnam",
      region == "United Arab Emirates" ~ "UAE",
      TRUE ~ region
    ))
}

existing_clusters <- standardize_country_names(existing_clusters)
planned_clusters <- standardize_country_names(planned_clusters)

# Join with world map
world_existing <- world_map %>%
  left_join(existing_clusters, by = "region")

world_planned <- world_map %>%
  left_join(planned_clusters, by = "region")

# Calculate Jenks natural breaks for existing clusters
existing_breaks <- classIntervals(existing_clusters$count, n = 5, style = "jenks")
existing_clusters <- existing_clusters %>%
  mutate(count_binned = cut(count, breaks = existing_breaks$brks, include.lowest = TRUE, dig.lab = 10))

world_existing <- world_map %>%
  left_join(existing_clusters, by = "region")

# Calculate Jenks natural breaks for planned clusters
planned_breaks <- classIntervals(planned_clusters$count, n = 5, style = "jenks")
planned_clusters <- planned_clusters %>%
  mutate(count_binned = cut(count, breaks = planned_breaks$brks, include.lowest = TRUE, dig.lab = 10))

world_planned <- world_map %>%
  left_join(planned_clusters, by = "region")

# Map 1: Existing GPU Clusters
map_existing <- ggplot(world_existing, aes(x = long, y = lat, group = group, fill = count_binned)) +
  geom_polygon(color = "white", linewidth = 0.1) +
  scale_fill_brewer(palette = "Blues", na.value = "gray90", name = "Count") +
  labs(
    title = "Existing GPU Clusters by Country",
    subtitle = "Geographic distribution of operational GPU clusters",
    x = NULL,
    y = NULL
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  ) +
  coord_fixed(1.3)

# Map 2: Planned GPU Clusters
map_planned <- ggplot(world_planned, aes(x = long, y = lat, group = group, fill = count_binned)) +
  geom_polygon(color = "white", linewidth = 0.1) +
  scale_fill_brewer(palette = "Blues", na.value = "gray90", name = "Count") +
  labs(
    title = "Planned GPU Clusters by Country",
    subtitle = "Geographic distribution of planned GPU clusters",
    x = NULL,
    y = NULL
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  ) +
  coord_fixed(1.3)

# Map 3: Combined (Existing + Planned Total)
combined_clusters <- gpu_clusters %>%
  filter(Status %in% c("Existing", "Planned")) %>%
  group_by(Country) %>%
  summarise(count = n(), .groups = "drop") %>%
  rename(region = Country)

combined_clusters <- standardize_country_names(combined_clusters)

# Calculate Jenks natural breaks for combined clusters
combined_breaks <- classIntervals(combined_clusters$count, n = 5, style = "jenks")
combined_clusters <- combined_clusters %>%
  mutate(count_binned = cut(count, breaks = combined_breaks$brks, include.lowest = TRUE, dig.lab = 10))

world_combined <- world_map %>%
  left_join(combined_clusters, by = "region")

map_combined <- ggplot(world_combined, aes(x = long, y = lat, group = group, fill = count_binned)) +
  geom_polygon(color = "white", linewidth = 0.1) +
  scale_fill_brewer(palette = "Blues", na.value = "gray90", name = "Count") +
  labs(
    title = "Total GPU Clusters by Country (Existing + Planned)",
    subtitle = "Combined geographic distribution of all GPU clusters",
    x = NULL,
    y = NULL
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  ) +
  coord_fixed(1.3)

# Display the maps
map_existing
map_planned
map_combined
```

### Training Time Analysis

```{r}
library(ggplot2)
library(dplyr)
library(readr)
library(scales)

# Load the training time results
training_times <- read_csv("./Data/gpu_cluster_training_times.csv")

# Convert date to datetime
training_times <- training_times %>%
  mutate(`First Operational Date` = as.Date(`First Operational Date`))

# Create scatter plot
ggplot(training_times, aes(x = `First Operational Date`, y = `Training time (hours)`)) +
  geom_point(alpha = 0.7, size = 3, color = "#4285F4") +
  # Add a trend line
  geom_smooth(method = "loess", se = TRUE, color = "#D32F2F", alpha = 0.2) +
  # Log scale for y-axis to better show the range
  scale_y_log10(labels = comma) +
  # Format x-axis dates
  scale_x_date(date_breaks = "2 years", date_labels = "%Y") +
  labs(
    title = "Average Training Time for Top 10 Frontier Models on Rank-1 GPU Clusters",
    x = "GPU Cluster First Operational Date",
    y = "Average Training Time (hours, log scale)",
    caption = "Based on top 10 frontier models by training compute released before each cluster's operational date"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank()
  )
```

### Cumulative Compute Over Time by Organization

```{r}
library(ggplot2)
library(dplyr)
library(readr)

# Load the cumulative compute timeline data
cumulative_data <- read_csv("./Data/cumulative_compute_timeline.csv")

# Convert date to datetime and training_compute to numeric
cumulative_data <- cumulative_data %>%
  mutate(date = as.Date(date))

# Filter to only OpenAI
cumulative_data <- cumulative_data %>% filter(owner_bucket == "OpenAI")

# Separate clusters and models
clusters <- cumulative_data %>% filter(type == "cluster")

# For models, only keep those with known training_compute and get top 10 by training_compute
models <- cumulative_data %>%
  filter(type == "model", !is.na(training_compute)) %>%
  arrange(desc(training_compute)) %>%
  head(10)

print(paste("Number of model release lines:", nrow(models)))

# Create the plot
ggplot() +
  # Add step lines for cumulative compute (clusters only)
  geom_step(data = clusters,
            aes(x = date, y = cumulative_compute),
            linewidth = 1, alpha = 0.8, color = "#10A37F", direction = "hv") +
  geom_point(data = clusters,
             aes(x = date, y = cumulative_compute),
             size = 3, alpha = 0.6, color = "#10A37F") +
  # Add vertical dashed lines for model releases
  geom_vline(data = models,
             aes(xintercept = as.numeric(date)),
             linetype = "dashed", alpha = 0.4, linewidth = 0.5, color = "#10A37F") +
  # Log scale for y-axis
  scale_y_log10(labels = scales::comma) +
  # Labels and theme
  labs(
    title = "OpenAI: Cumulative GPU Cluster Compute Over Time",
    subtitle = "Line shows compute growth; dashed lines indicate model releases",
    x = "Date",
    y = "Cumulative Compute (H100 equivalents, log scale)",
    caption = "Only existing clusters with known compute capacity included"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank()
  )
```

### Cumulative Compute Over Time by Organization (Faceted)

```{r}
library(ggplot2)
library(dplyr)
library(readr)

# Load the cumulative compute timeline data
cumulative_data <- read_csv("./Data/cumulative_compute_timeline.csv")

# Convert date to datetime
cumulative_data <- cumulative_data %>%
  mutate(date = as.Date(date))

# Get companies that have clusters
companies_with_clusters <- cumulative_data %>%
  filter(type == "cluster") %>%
  pull(owner_bucket) %>%
  unique()

# Filter to only companies with clusters and exclude "Other"
cumulative_data <- cumulative_data %>%
  filter(owner_bucket %in% companies_with_clusters, owner_bucket != "Other")

# Separate clusters
clusters <- cumulative_data %>% filter(type == "cluster")

# For models, get top 10 by training_compute for each organization
models <- cumulative_data %>%
  filter(type == "model", !is.na(training_compute)) %>%
  group_by(owner_bucket) %>%
  arrange(desc(training_compute)) %>%
  slice_head(n = 10) %>%
  ungroup()

# Define color palette (consistent with previous graphs)
color_palette <- c(
  "Google DeepMind" = "#4285F4",
  "OpenAI" = "#10A37F",
  "Anthropic" = "#D4A574",
  "xAI" = "#000000",
  "Meta" = "#0668E1",
  "Alibaba" = "#FF6A00",
  "Amazon" = "#FF9900"
)

# Create the plot
ggplot() +
  # Add step lines for cumulative compute (clusters only)
  geom_step(data = clusters,
            aes(x = date, y = cumulative_compute, color = owner_bucket),
            linewidth = 1, alpha = 0.8, direction = "hv") +
  geom_point(data = clusters,
             aes(x = date, y = cumulative_compute, color = owner_bucket),
             size = 2, alpha = 0.6) +
  # Add vertical dashed lines for model releases
  geom_vline(data = models,
             aes(xintercept = as.numeric(date), color = owner_bucket),
             linetype = "dashed", alpha = 0.4, linewidth = 0.3) +
  # Facet by organization
  facet_wrap(~ owner_bucket, ncol = 2, scales = "free_y") +
  # Apply color palette
  scale_color_manual(values = color_palette, name = "Organization") +
  # Log scale for y-axis
  scale_y_log10(labels = scales::comma) +
  # Labels and theme
  labs(
    title = "Cumulative GPU Cluster Compute Over Time by Organization",
    subtitle = "Lines show compute growth; dashed lines indicate top 10 model releases by training compute",
    x = "Date",
    y = "Cumulative Compute (H100 equivalents, log scale)",
    caption = "Only organizations with GPU clusters; top 10 models by training compute shown"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",  # Hide legend since facets show organizations
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold", size = 10),
    panel.grid.minor = element_blank()
  )
```

### Dataset Size Over Time

```{r}
library(ggplot2)
library(dplyr)
library(readr)
library(scales)

# Load the notable AI models dataset
models_data <- read_csv("./Data/ai_models_dataset/notable_ai_models.csv")

# Filter to models with known dataset size and publication date
dataset_plot_data <- models_data %>%
  filter(!is.na(`Training dataset size (gradients)`),
         !is.na(`Publication date`)) %>%
  mutate(`Publication date` = as.Date(`Publication date`),
         `Training dataset size (gradients)` = as.numeric(`Training dataset size (gradients)`))

# Create scatter plot
ggplot(dataset_plot_data, aes(x = `Publication date`, y = `Training dataset size (gradients)`)) +
  geom_point(alpha = 0.6, size = 2, color = "#4285F4") +
  # Log scale for y-axis
  scale_y_log10(labels = comma) +
  # Labels and theme
  labs(
    title = "AI Model Dataset Size Over Time",
    subtitle = "Training dataset size has grown exponentially",
    x = "Publication Date",
    y = "Training Dataset Size (gradients, log scale)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank()
  )
```

### Dataset Size Over Time with Key Models

```{r}
library(ggplot2)
library(dplyr)
library(readr)
library(scales)

# Load the notable AI models dataset
models_data <- read_csv("./Data/ai_models_dataset/notable_ai_models.csv")

# Filter to models with known dataset size and publication date
dataset_plot_data <- models_data %>%
  filter(!is.na(`Training dataset size (gradients)`),
         !is.na(`Publication date`)) %>%
  mutate(`Publication date` = as.Date(`Publication date`),
         `Training dataset size (gradients)` = as.numeric(`Training dataset size (gradients)`))

# Define key model release dates
alexnet_date <- as.Date("2012-09-29")
transformer_date <- as.Date("2017-06-11")

# Create scatter plot with vertical lines for key models
ggplot(dataset_plot_data, aes(x = `Publication date`, y = `Training dataset size (gradients)`)) +
  # Add vertical lines for key models
  geom_vline(xintercept = as.numeric(alexnet_date),
             linetype = "dashed", color = "#EA4335", linewidth = 0.8) +
  geom_vline(xintercept = as.numeric(transformer_date),
             linetype = "dashed", color = "#34A853", linewidth = 0.8) +
  # Add scatter points
  geom_point(alpha = 0.6, size = 2, color = "#4285F4") +
  # Log scale for y-axis
  scale_y_log10(labels = comma) +
  # Labels and theme
  labs(
    title = "AI Model Dataset Size Over Time",
    subtitle = "Training dataset size has grown exponentially. Dashed lines show AlexNet (red) and Transformer (green) releases.",
    x = "Publication Date",
    y = "Training Dataset Size (gradients, log scale)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank()
  )
```

### Dataset Size Growth Rate Before and After Transformer

```{r}
library(ggplot2)
library(dplyr)
library(readr)
library(scales)

# Load the notable AI models dataset
models_data <- read_csv("./Data/ai_models_dataset/notable_ai_models.csv")

# Filter to models with known dataset size and publication date
dataset_plot_data <- models_data %>%
  filter(!is.na(`Training dataset size (gradients)`),
         !is.na(`Publication date`)) %>%
  mutate(`Publication date` = as.Date(`Publication date`),
         `Training dataset size (gradients)` = as.numeric(`Training dataset size (gradients)`))

# Define key model release dates
alexnet_date <- as.Date("2012-09-29")
transformer_date <- as.Date("2017-06-11")

# Split data into pre-Transformer and post-Transformer periods
pre_transformer <- dataset_plot_data %>% filter(`Publication date` < transformer_date)
post_transformer <- dataset_plot_data %>% filter(`Publication date` >= transformer_date)

# Create scatter plot with two linear regression lines
ggplot(dataset_plot_data, aes(x = `Publication date`, y = `Training dataset size (gradients)`)) +
  # Add vertical lines for key models
  geom_vline(xintercept = as.numeric(alexnet_date),
             linetype = "dashed", color = "#EA4335", linewidth = 0.8, alpha = 0.5) +
  geom_vline(xintercept = as.numeric(transformer_date),
             linetype = "dashed", color = "#34A853", linewidth = 0.8) +
  # Add scatter points
  geom_point(alpha = 0.6, size = 2, color = "#4285F4") +
  # Add linear regression for pre-Transformer period
  geom_smooth(data = pre_transformer,
              method = "lm",
              se = TRUE,
              color = "#FBBC04",
              fill = "#FBBC04",
              alpha = 0.2,
              linewidth = 1) +
  # Add linear regression for post-Transformer period
  geom_smooth(data = post_transformer,
              method = "lm",
              se = TRUE,
              color = "#34A853",
              fill = "#34A853",
              alpha = 0.2,
              linewidth = 1) +
  # Log scale for y-axis
  scale_y_log10(labels = comma) +
  # Labels and theme
  labs(
    title = "Dataset Size Growth Rate Before and After Transformer",
    subtitle = "Yellow line: pre-Transformer growth. Green line: post-Transformer growth. Shaded areas show confidence intervals.",
    x = "Publication Date",
    y = "Training Dataset Size (gradients, log scale)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank()
  )
```

### Model Parameters Over Time

```{r}
library(ggplot2)
library(dplyr)
library(readr)
library(scales)

# Load the notable AI models dataset
models_data <- read_csv("./Data/ai_models_dataset/notable_ai_models.csv")

# Filter to models with known parameters and publication date
parameters_plot_data <- models_data %>%
  filter(!is.na(Parameters),
         !is.na(`Publication date`)) %>%
  mutate(`Publication date` = as.Date(`Publication date`),
         Parameters = as.numeric(Parameters))

# Create scatter plot
ggplot(parameters_plot_data, aes(x = `Publication date`, y = Parameters)) +
  geom_point(alpha = 0.6, size = 2, color = "#4285F4") +
  # Log scale for y-axis
  scale_y_log10(labels = comma) +
  # Labels and theme
  labs(
    title = "AI Model Parameters Over Time",
    subtitle = "Model size has grown exponentially",
    x = "Publication Date",
    y = "Parameters (log scale)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank()
  )
```

### Model Parameters Over Time with Key Models

```{r}
library(ggplot2)
library(dplyr)
library(readr)
library(scales)

# Load the notable AI models dataset
models_data <- read_csv("./Data/ai_models_dataset/notable_ai_models.csv")

# Filter to models with known parameters and publication date
parameters_plot_data <- models_data %>%
  filter(!is.na(Parameters),
         !is.na(`Publication date`)) %>%
  mutate(`Publication date` = as.Date(`Publication date`),
         Parameters = as.numeric(Parameters))

# Define key model release dates
alexnet_date <- as.Date("2012-09-29")
transformer_date <- as.Date("2017-06-11")

# Create scatter plot with vertical lines for key models
ggplot(parameters_plot_data, aes(x = `Publication date`, y = Parameters)) +
  # Add vertical lines for key models
  geom_vline(xintercept = as.numeric(alexnet_date),
             linetype = "dashed", color = "#EA4335", linewidth = 0.8) +
  geom_vline(xintercept = as.numeric(transformer_date),
             linetype = "dashed", color = "#34A853", linewidth = 0.8) +
  # Add scatter points
  geom_point(alpha = 0.6, size = 2, color = "#4285F4") +
  # Log scale for y-axis
  scale_y_log10(labels = comma) +
  # Labels and theme
  labs(
    title = "AI Model Parameters Over Time",
    subtitle = "Model size has grown exponentially. Dashed lines show AlexNet (red) and Transformer (green) releases.",
    x = "Publication Date",
    y = "Parameters (log scale)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank()
  )
```

### Model Parameters Growth Rate Before and After Transformer

```{r}
library(ggplot2)
library(dplyr)
library(readr)
library(scales)

# Load the notable AI models dataset
models_data <- read_csv("./Data/ai_models_dataset/notable_ai_models.csv")

# Filter to models with known parameters and publication date
parameters_plot_data <- models_data %>%
  filter(!is.na(Parameters),
         !is.na(`Publication date`)) %>%
  mutate(`Publication date` = as.Date(`Publication date`),
         Parameters = as.numeric(Parameters))

# Define key model release dates
alexnet_date <- as.Date("2012-09-29")
transformer_date <- as.Date("2017-06-11")

# Split data into pre-Transformer and post-Transformer periods
pre_transformer <- parameters_plot_data %>% filter(`Publication date` < transformer_date)
post_transformer <- parameters_plot_data %>% filter(`Publication date` >= transformer_date)

# Create scatter plot with two linear regression lines
ggplot(parameters_plot_data, aes(x = `Publication date`, y = Parameters)) +
  # Add vertical lines for key models
  geom_vline(xintercept = as.numeric(alexnet_date),
             linetype = "dashed", color = "#EA4335", linewidth = 0.8, alpha = 0.5) +
  geom_vline(xintercept = as.numeric(transformer_date),
             linetype = "dashed", color = "#34A853", linewidth = 0.8) +
  # Add scatter points
  geom_point(alpha = 0.6, size = 2, color = "#4285F4") +
  # Add linear regression for pre-Transformer period
  geom_smooth(data = pre_transformer,
              method = "lm",
              se = TRUE,
              color = "#FBBC04",
              fill = "#FBBC04",
              alpha = 0.2,
              linewidth = 1) +
  # Add linear regression for post-Transformer period
  geom_smooth(data = post_transformer,
              method = "lm",
              se = TRUE,
              color = "#34A853",
              fill = "#34A853",
              alpha = 0.2,
              linewidth = 1) +
  # Log scale for y-axis
  scale_y_log10(labels = comma) +
  # Labels and theme
  labs(
    title = "Model Parameters Growth Rate Before and After Transformer",
    subtitle = "Yellow line: pre-Transformer growth. Green line: post-Transformer growth. Shaded areas show confidence intervals.",
    x = "Publication Date",
    y = "Parameters (log scale)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank()
  )
```
